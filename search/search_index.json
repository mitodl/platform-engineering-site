{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#handy-links","title":"Handy Links","text":""},{"location":"#for-engineering","title":"For Engineering","text":"<ul> <li>Oncall Emergency Break Glass</li> <li>Celery Monitoring Production</li> <li>Concourse Pipelines Production</li> <li>Directory of MIT OL's Applications</li> <li>MIT OL Engineering Github Discussions</li> <li>MIT Online Learning Intranet</li> <li>MIT Atlas Status</li> </ul>"},{"location":"#for-platform-engineering","title":"For Platform Engineering","text":"<ul> <li>Grafana Production</li> </ul>"},{"location":"application_specific_guides/heroku_legacy/config_vars/","title":"Config Vars","text":""},{"location":"application_specific_guides/heroku_legacy/config_vars/#managing-heroku-config-vars-with-pulumi","title":"Managing Heroku Config Vars with Pulumi","text":""},{"location":"application_specific_guides/heroku_legacy/config_vars/#herokuappconfigassociation","title":"heroku.app.ConfigAssociation","text":"<p>The resource/mechanism we are using to manage config vars in Heroku is called a 'ConfigAssociation' which is documented (here)[https://www.pulumi.com/registry/packages/heroku/api-docs/app/configassociation/]. A <code>ConfigAssociation</code> takes in an application ID and two sets of variable maps: <code>sensitive_vars</code> and <code>vars</code>. The only difference being that <code>sensitive_vars</code> will not be output during <code>up</code> operations.</p>"},{"location":"application_specific_guides/heroku_legacy/config_vars/#required-config-for-the-pulumi-provider","title":"Required config for the pulumi provider","text":"<p>The <code>pulumiverse_heroku</code> provider requires a configuration item in the pulumi stacks named <code>heroku:apiKey</code>. We don't want to have to encrypt that api key in two dozen different stacks so we wrapped our provider config with a <code>setup_heroku_provider()</code> function much the same way we do vault. This can be seen (here)[https://github.com/mitodl/ol-infrastructure/blob/main/src/ol_infrastructure/lib/heroku.py]. In lieu of setting <code>heroku:apiKey</code> in ever stack, we can set <code>heroku:user</code> which this function will then do a lookup in the backgroun out of sops config to get the apporpriate apiKey value.</p>"},{"location":"application_specific_guides/heroku_legacy/config_vars/#four-flavors-of-vars","title":"Four Flavors of Vars","text":"<p>While we don't yet have a component resource or abstraction available for setting up Config Vars in a simpler fashion, we do have a basic blueprint available with the MITOpen application.</p>"},{"location":"application_specific_guides/heroku_legacy/config_vars/#unchanging-values","title":"Unchanging Values","text":"<p>These are not really variables because they represent Key:Value mappings that are unchanging between environments. That is, Production and QA will have the same value set for the same environment. These values are specified directly in the python code under <code>heroku_vars</code></p>"},{"location":"application_specific_guides/heroku_legacy/config_vars/#simple-environment-specific-vars","title":"Simple Environment Specific Vars","text":"<p>These are simple 1-to-1 mappings from a value stored in the Pulumi configuration under <code>heroku_app:vars:</code>. This map contains the variable names, in their final forms using all-caps, and the static values that are applicable to the environment.</p>"},{"location":"application_specific_guides/heroku_legacy/config_vars/#interpolated-environment-specific-vars","title":"Interpolated Environment Specific Vars","text":"<p>These are key:value mappings that are used in more complicated manners than a simple 1-to-1 mapping as with the simple settings. These values are stored in the Pulumi configuration under <code>heroku_app:interpolated_vars:</code> in lower-case, signifying that they do not directly become Config Vars in Heroku. These more involved interpolations take place during the construction of the <code>heroku_interpolated_vars</code> dictionary.</p>"},{"location":"application_specific_guides/heroku_legacy/config_vars/#secrets","title":"Secrets","text":"<p>Many Config Vars that we use represent values that can be considered secret or otherwise sensitive and should not be publicly disclosed. Nothing from these vars is derived from values stored in the Pulumi configuration, rather they are obtained either from SOPS config or directly from vault at stack application time. Secrets are complicated to work with and it is best to reach out to DevOps for assistence in getting your new secret configuration var setup.</p>"},{"location":"application_specific_guides/heroku_legacy/dynosaur_management/","title":"Heroku Dynosaur Management","text":""},{"location":"application_specific_guides/heroku_legacy/dynosaur_management/#pipeline","title":"Pipeline","text":"<p>The pipeline can be found here. And the pipeline definition can be found here.</p> <p>There are two dicts in the pipeline definition which define the various production and QA applications. The definition of each includes the name of the application as the key to the dict, and a substructure that lists the application owner and a list of dyno name/qty/size combinations. The owner is needed to perform an apiKey lookup from pre-existing SOPS data within the repo.</p> <p>At the moment we are not resetting the web node counts / configurations because HireFire has a hand in managing those.</p>"},{"location":"application_specific_guides/heroku_legacy/log_drains/","title":"Log Drains","text":""},{"location":"application_specific_guides/heroku_legacy/log_drains/#configure-a-log-drain","title":"Configure a Log Drain","text":"<p>To get logs from heroku applications -&gt; Grafana we need to configure a 'log drain' in heroku for each application. This is pretty straight forward but does require collecting a few pieces of information first:</p> <ul> <li>The address of the 'vector-log-proxy'</li> <li>log-proxy-ci.odl.mit.edu</li> <li>log-proxy-qa.odl.mit.edu</li> <li>log-proxy.odl.mit.edu</li> <li>The basic auth password for the 'vector-log-proxy' server.</li> <li><code>sops -d src/bridge/secrets/vector/vector_log_proxy.ci.yaml</code>  heroku section, username: vector-log-proxy</li> <li><code>sops -d src/bridge/secrets/vector/vector_log_proxy.qa.yaml</code>  heroku section, username: vector-log-proxy</li> <li><code>sops -d src/bridge/secrets/vector/vector_log_proxy.production.yaml</code>  heroku section, username: vector-log-proxy</li> </ul> <p>Next we configure the log drain using the heroku-cli. There is no web interface for configuring this.</p> <pre><code>heroku drains:add -a &lt;HEROKU_APP_NAME&gt; 'https://vector-log-proxy:&lt;PASSWORD_FROM_SOPS&gt;@&lt;URL_FROM_ABOVE&gt;:9000/events?app_name=&lt;APP_NAME_WO_ENV_INFO&gt;&amp;environment=&lt;ENV&gt;&amp;service=heroku'\n</code></pre> <p>And you'll get a response like:</p> <pre><code>Successfully added drain https://vector-log-proxy:&lt;The rest of the URL you just used&gt;\n</code></pre> <p><code>app_name</code>, <code>environment</code>, and <code>service</code> all refer directly to the organization fields used to categorize the logs in Grafana.</p>"},{"location":"application_specific_guides/heroku_legacy/log_drains/#references","title":"References","text":"<p>https://devcenter.heroku.com/articles/logplex https://devcenter.heroku.com/articles/log-drains#https-drains</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/","title":"Moving a Heroku Managed Postgres DB to Pulumi Managed AWS RDS","text":""},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#preparation","title":"Preparation","text":"<p>You will need to gather some information about the Heroku managed application and its database before you start. You'll need the Heroku CLI with auehenticated access to the application in question to continue.</p> <p>You'll also need the postgresql tools, specifically the <code>pg_dump</code> and <code>psql</code> tools.</p> <p>Record the following information somewhere persistent that you'll be able to refer back to through this process. I like using my notes.</p> <ul> <li>The Heroku application's name. Our convention is generally - so for the CI environment of the micromasters application, you'd use <code>micromasters-ci</code>. <li>The applications's <code>DATABASE_URL</code>. You can obtain this with the following invocation: <code>heroku config:get DATABASE_URL -a &lt;your app&gt;</code></li> <li>The currently attached Heroku database name as well as the alias they have assigned to it by default. You can get this with: <code>heroku addons -a &lt;application&gt; | grep -i postgres</code> for example: <pre><code>\u2570 \u27a4 heroku addons -a micromasters-ci | grep -i postgres\nheroku-postgresql (postgresql-rigid-71273)  mini   $5/month   created\n \u2514\u2500 as HEROKU_POSTGRES_YELLOW\n</code></pre> so in this case we see that postgresql-rigid-71273 is attached as HEROKU_POSTGRES_YELLOW</li> <li>A set of URLs to browse when the transition is done to ensure everything is working properly. You should also browse them before the transition to note how everything looks.</li>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#building-the-infrastructure-with-pulumi","title":"Building the Infrastructure with Pulumi","text":"<p>Describing in detail how to code the necessary resources to build the AWS RDS Postgres instance and associated S3 bucket, IAM rules, VPC peerings etc. is beyond the scope of this ducument. You can see what I did in my Github branch.</p> <p>Once the infrastructure is properly built, you'll need to record the new AWS RDS Postgres instance's endpoint. You can do this from within the directory for the application you're working on. For my current project, that's <code>ol-infrastructure/src/ol_infrastructure/applications/micromasters</code> with this: 'poetry run pulumi stack export -s applications.micromasters.CI | grep -i endpoint' but obviously sub your app in for micromasters.</p> <p>You'll also need to retrieve the database password from Vault using Pulumi. You can do that with the following invocation: <code>poetry run pulumi config get  \"micromasters:db_password\"</code></p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#construct-a-new-database_url","title":"Construct A New DATABASE_URL","text":"<p>I suggest doing this in a text file you can source easily since you'll be working with this database a bit for this project. I keep such things in an 'envsnips' folder in my home directory.</p> <p>The file should look something like: <pre><code>export DATABASE_URL=postgresql://oldevops:&lt;password you pulled from Pulumi config&gt;@micromasters-ci-app-db.cbnm7ajau6mi.us-east-1.rds.amazonaws.com:5432/micromasters\n</code></pre></p> <p>Make sure the URL has the following components:</p> <ul> <li><code>postgresql://</code> is the protocol identifier followed by a :.</li> <li>'oldevops' is the database user, then another :.</li> <li>the database password we pulled from Pulumi above, followed by an @ sign.</li> <li>The endpoint hostname we retrieved from Pulumi earlier, followed by a :.</li> <li>The port number. We usually use 5432. Then a /.</li> <li>The database name.</li> </ul> <p>If your URL is missing any of these it will not work. Once you've finished write out your file and source it in your shell.</p> <p>Now, test that you can connect using the URL you just built with:</p> <p><code>psql $DATABASE_URL</code></p> <p>If you get an access denied message, make sure you got the correct password for the app and environment (e.g. CI, QA or production) and check the other components.</p> <p>We'll assume $DATABASE_URL is set to to the new RDS database we've created for the rest of the runbook.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#put-the-heroku-app-into-maintenance-mode","title":"Put the Heroku App Into Maintenance Mode","text":"<p>In order to ensure database consistency during the transition we need to put the Heroku application into maintenance mode.</p> <pre><code>heroku maintenance:on -a &lt;app&gt;\n</code></pre> <p>CAUTION customers will see a notice about ongoing maintenance until you take the app out of maintenance mode, so be mindful of the wall clock time!</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#dump-heroku-managed-db","title":"Dump Heroku Managed DB","text":"<p>Use something like the following invocation to dump the contents of the current application database.</p> <p><code>pg_dump -x -O $(heroku config:get DATABASE_URL -a micromasters-rc) &gt; micromasters_qa_db_dump.sql</code></p> <p>Obviously, substitute your app for micromasters and your environment for rc/qa.</p> <p>(Aside: We use rc and qa interchangably here).</p> <p>Examine the dump in your editor (read-only to be safe) and ensure that all the necessary components are present: Schema, data, foreign keys, and the like.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#restore-dump-into-aws-rds-db","title":"Restore Dump Into AWS RDS DB","text":"<p>Using the DATABASE_URL we just created and tested, we can now restore the data we dumped in the prior step into the new DB:</p> <p><code>psql $DATABASE_URL &lt; micromasters_qa_db_dump.sql</code></p> <p>You will see a lot of output representing each statement as it's processed by the DB. You shouldn't see any errors here.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#take-heroku-application-out-of-maintenance-mode","title":"Take Heroku application out of maintenance mode","text":"<pre><code>heroku maintenance:off -a &lt;app&gt;\n</code></pre> <p>At this point, the application will resume normal operation and be available to customers.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#coordinate-transition","title":"Coordinate Transition","text":"<p>In the process of changing the database out from under a running application, there will be some small period of down time, so it's important to coordinate with all the appropriate stakeholders and leadership before you do.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#perform-the-final-transition","title":"Perform The Final Transition","text":"<p>At the time, it's important that you perform the following steps quickly in succession, because once you detach the current DB, the application will be down. Keep this as brief as possible.</p> <p>You may wish to cue up the commands you want to run in a text file somewhere you can easily review them, and then cut and paste them into your shell when the time comes.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#create-an-additional-db-attachment","title":"Create An Additional DB Attachment","text":"<p>You'll need to create an additional attachment for the current DB: <code>heroku addons:attach postgresql-amorphous-36035 --as HEROKU_POSTGRES_DB</code></p> <p>Substitute your db instance you gathered above. HEROKU_POSTGRES_DB is just an alias we can use if we should need to roll back.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#detach-the-current-database","title":"Detach The Current Database","text":"<p>This is where you'll need to use the Heroku managed database instance above, along with the Heroku application name we collected. Substitute accordingly into the following invocation:</p> <p><code>heroku addons:detach postgresql-amorphous-36035 -a micromasters-rc</code></p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#change-the-database_url-to-the-new-rds-instance","title":"Change the DATABASE_URL to the New RDS Instance","text":"<p>Ensuring that your DATABASE_URL environment variable is properly set to your new RDS from the above steps, use it to set DATABASE_URL in the heroku app:</p> <p><code>heroku config:set -a micromasters-rc DATABASE_URL=$DATABASE_URL</code></p> <p>Now immediately print out the value you just set to ensure that all looks good: <code>heroku config:get -a micromasters-rc DATABASE_URL</code></p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#test-your-work","title":"Test Your Work","text":"<p>You should carefully test the application you just transitioned to ensure everything works using the set of URLs you gathered at the beginning. - Do the pages have all the elements they should? - Are images loading?</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#how-to-roll-back","title":"How To Roll Back","text":"<p>If something goes wrong and you need to roll back, don't panic!</p> <p>All you need to do is promote the old Heroku managed DB back into use:</p> <p><code>heroku pg:promote --app micromasters-ci postgresql-rigid-71273</code></p> <p>Obviously substitute your db and application for the ones above.</p> <p>Re-run your tests as defined above to make sure everything's working right post-rollback.</p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#s3-buckets","title":"S3 Buckets","text":"<p>Our applications use S3 buckets for CMS asset storage and backup among other things.</p> <p>You will need to either continue using the existing buckets by using <code>pulumi import</code> or creating new onnes. You should create new ones if the old ones don't conform to naming conventions. You'll also need to ensure that IAM permissions are properly set in your Pulumi code.</p> <p>To sync the bucket contents, use the AWS CLI <code>aws s3 sync</code> command.</p> <p>For example, the command we used to sync the old micromasters S3 bucket to the new one which conforms to our desired naming conventions is: <pre><code>aws s3 sync s3://odl-micromasters-production s3://ol-micromasters-app-production\n</code></pre></p>"},{"location":"application_specific_guides/heroku_legacy/transition_postgres_db_to_rds/#saltstack-cloudfront","title":"Saltstack &amp; Cloudfront","text":"<p>Currently, we are using Saltstack to configure some aspects of our Heroku applications, including which S3 bucket to use and which CloudFront distribution we're fronting the app with.</p> <p>TODO: This section needs to be way less of a hand wave and include actual detail as to how to operate the Saltstack side or whatever we replace it with.</p>"},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/","title":"Architecture","text":""},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#image-building","title":"Image Building","text":"<p>https://github.mit.edu/ol-notebooks is an org containing a template repository and all course repositories. Each course repository contains a Dockerfile and requirements.txt file alongside any Jupyter notebooks and data.</p> <p>Concourse runs a parameterized job which pulls from the course repositories, constructs a docker image to bundle everything together and push to a private ECR repository we maintain. The dockerfiles use the official Jupyter pytorch docker images as a base and install tensorflow as well. The code to provision the pipelines is here and the Pulumi code to set up the ECR repository is here.</p>"},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#jupyterhub","title":"Jupyterhub","text":"<p>Jupyterhub will use the KubeSpawner library (with some very slight modifications to enable image selection via query param) to allow users to start up and interact with a set of images we maintain. For courses, this will involve Jupyterhub starting up a notebook server for the user by pulling the corresponding course image from ECR.</p> <p>The backing database is a Postgres RDS instance.</p> <p>Images are pre-pulled via continuous-pre-puller daemonsets. This is currently configured to pull all 4 existing images we build and maintain via specification of an extraImages block.</p> <p>Jupyterhub domains are currently gated by an SSO login to the olapps Keycloak realm. Once authenticated with Keycloak, Jupyterhub is set up to use the TmpAuthenticator class. When accessing the /tmplogin endpoint provided by the authenticator, Jupyterhub will unconditionally authenticate users as a random UUID. This ensures that users can spin up ephemeral notebooks provided they are able to authenticate with MIT Learn.</p> <p>Culling is performed via the jupyterhub-idle-culler configured via Helm chart. It currently culls both running, inactive servers as well as users. Culling users is important as we will accumulate UUID-keyed users and sessions in the database over time.</p>"},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#monitoring","title":"Monitoring","text":"<p>We maintain a Grafana dashboard which shows some basic metrics about the Jupyterhub deployment. This includes:</p> <ul> <li><code>kube_pod_container_info</code>: shows which images notebook containers are currently running.</li> <li><code>kube_pod_container_status_restarts_total</code>: shows how many times notebook containers have restarted. This being non-zero may be indicative of OOMKills.</li> </ul>"},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#authoring-workflow","title":"Authoring Workflow","text":""},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#adding-a-new-course","title":"Adding a New Course","text":"<p>At the moment, adding a new course involves a few manual steps:</p> <ol> <li>Create a new repository in the ol-notebooks org using the template repo.</li> <li>Author your changes. If you started from the template repo, the only required changes you'll need to make are adding your notebooks and data, adding any dependencies to either requirements.txt or the Dockerfile, and ensuring that the Dockerfile copies your notebooks and data into the ${NB_USER} home directory. Sometimes this will be done for you by the course authors.<ul> <li>Here's an example of a PR which adds a notebook and data files to a fresh course repo: https://github.mit.edu/ol-notebooks/UAI_SOURCE-UAI.ST.1-1T2026/pull/1</li> <li>Note that there's a 100mb limit on files checked into the repo. If the course requires larger data files, they need to either be hosted externally or compressed and unzipped at runtime.</li> </ul> </li> <li>Make changes to ol-infrastructure to add a new image build job in Concourse. This involves adding a new job to the jupyter_courses.py file and provisioning the new pipeline via fly command. Additionally, add the new image to the QueryStringKubeSpawner's <code>KNOWN_IMAGES</code> in dynamicImageConfig.py and to <code>COURSE_NAMES</code> in JupyterHub's Pulumi code.<ul> <li>If the notebook requires GPU resources, also add the new course to the <code>GPU_ENABLED_COURSES</code> in <code>dynamicImageConfig.py</code></li> <li>Here's an example PR which added a new course called <code>uai_source-uai.intro</code>: https://github.com/mitodl/ol-infrastructure/pull/3630</li> </ul> </li> <li>Apply your ol-infrastructure changes to the dev/stage/prod environments and unpause your new Concourse job to start building the image.</li> <li>Once the image is built and pushed to ECR, you should be able to start a new notebook server using the new image. You can construct the URL using the following format: <code>https://nb.learn.mit.edu/tmplogin?course=&lt;IMAGE_TAG&gt;&amp;notebook=&lt;URL_ENCODED_PATH_TO_NOTEBOOK_FILE&gt;</code>.<ul> <li>For example, if your image URI is <code>610119931565.dkr.ecr.us-east-1.amazonaws.com/ol-course-notebooks:uai_source-uai.intro</code> and your notebook is at <code>lectures/lecture1/mod5_lec1.ipynb</code>, the URL would be <code>https://nb.learn.mit.edu/tmplogin?course=uai_source-uai.intro&amp;notebook=lectures%2Flecture1%2Fmod5_lec1.ipynb</code>.</li> </ul> </li> <li>Lastly, open the link and run the entire notebook. This will help you catch any dependency issues or let you know if the notebook uses too much memory to execute. See the Troubleshooting section for more details on common failure modes you might encounter.</li> </ol>"},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#updating-an-existing-course","title":"Updating an Existing Course","text":"<p>The steps for updating a new course are a subset of the steps for adding a new course:</p> <ol> <li>Author your changes in the course repo. If it is an update to an existing course, it should already have a Concourse build pipeline which will automatically attempt to build an updated image.</li> <li>Once the image is built, you should be able to start a notebook server with the updated image automatically. If you adjusted the notebook directory structure, you may need to construct a new URL, but you will not need to make any additional infrastructure changes.</li> <li>Test the updated image by running through the notebook as you would for a new one.</li> </ol>"},{"location":"application_specific_guides/jupyterhub/jupyterhub_admin_guide/#troubleshooting","title":"Troubleshooting","text":"<p>If you run into issues attempting to run a course image, here are some common things to check:</p> <ul> <li>If you get the wrong image when you log in, verify that you've specified the right course parameter and that the value is in <code>KNOWN_IMAGES</code> in <code>dynamicImageConfig.py</code>.</li> <li>If the image starts but you get a 404, verify that the notebook path is correct and URL encoded.</li> <li>You may need to change the <code>COPY</code> command in the dockerfile depending on the file structure of the repo. By default it expects all files to be in the root and handles copying it correctly, but by convention existing courses use file structures like <code>assignments/assignment1/mod1_assign1.ipynb</code>, which requires a Dockerfile change.</li> <li>If a notebook starts misbehaving while you're running code, it may be an OOMKill. This can manifest in a few ways such as being unable to run more code, or a popup mentioning that the file on disk has changed unexpectedly.</li> <li>Validating this directly is tricky and the most direct way to validate it is by watching <code>top</code> in a shell on the pod running your notebook. However, for cases where the OOMKill results in the pod restarting, it should appear on this graph</li> <li>The code itself may have issues. The specific way it manifests varies widely, but a few common ones are:</li> <li>Missing dependencies, resulting in import errors. If an inline <code>!pip install</code> resolves the issue, add the pinned dependency to requirements.txt.</li> <li>Missing data files, resulting in a <code>FileNotFoundError</code>. Typically this involves asking the course authors to provide you with the data, or ensuring that the file is where the code expects it to be.</li> <li>GPU-related issues when using PyTorch or Tensorflow.<ul> <li>Notebooks that use Tensorflow features may need to use a Tensorflow-specific base image. Modify the <code>FROM</code> directive in the course's Dockerfile as seen here</li> <li>Ensure that if a course needs a GPU-enabled node that the course it is added to the <code>GPU_ENABLED_COURSES</code> set in dynamicImageConfig.py</li> </ul> </li> <li>Code output seems suspicious or incorrect when compared to the provided notebook content. Sometimes this is obvious (e.g. a plot that looks nonsensical), other times it may be more subtle (e.g. a model training to suspiciously low accuracy). In these cases, it's best to reach out to the course authors for help if you're not an expert in the course material.</li> </ul>"},{"location":"application_specific_guides/ocw-studio/Update_OCW_Studio_Pipeline/","title":"Updating the OCW Studio Pipeline","text":""},{"location":"application_specific_guides/ocw-studio/Update_OCW_Studio_Pipeline/#make-your-change","title":"Make Your Change","text":"<p>In this case, we needed to pin the aws-cli container to a previous version because Amazon upgraded it from AL2 to AL2023 out from underneath us. The PR is here.</p> <p>Get your PR approved.</p>"},{"location":"application_specific_guides/ocw-studio/Update_OCW_Studio_Pipeline/#allow-doof-to-enact-his-evil-plan","title":"Allow Doof To Enact His Evil Plan","text":"<p>Send the message <code>@doof release notes</code> to this slack channel.</p> <p>Doof will prompt you to click your checkboxes for this release. It takes a bit for him to notice your change.</p> <p>Click the check box, and wait another bit while he 'sees' that. Then, when prompted, tell Doof to merge your change and deploy the Heroku app.</p> <p>When Doof is done the output should look something like:</p> <pre><code>Dr. Heinz Doofenshmirtz\nAPP  10:53 AM\nMy evil scheme v0.153.2 for ocw-studio has been released to production at https://ocw-studio.odl.mit.edu. And by 'released', I mean completely...um...leased.\n</code></pre>"},{"location":"application_specific_guides/ocw-studio/Update_OCW_Studio_Pipeline/#update-the-pipeline","title":"Update The Pipeline","text":"<p>Access a shell on Heroku RC by running:</p> <pre><code>heroku run --app ocw-studio-rc bash\n</code></pre> <p>Once you get a shell prompt, run the following command:</p> <p><code>./manage.py backpopulate_pipelines -f ocw-www</code></p> <p>You may omit the -f ocw-www to update all pipelines.</p> <p>The output should look something like:</p> <pre><code>~ $ ./manage.py backpopulate_pipelines\nCreating website pipelines\nStarted celery task ad65adf1-7f8b-4f80-8c44-9ddebe329bca to upsert pipelines for 2859 sites\nWaiting on task...\nPipeline upserts finished, took 422.49245 seconds\n~ $\n</code></pre>"},{"location":"application_specific_guides/ocw-studio/Update_OCW_Studio_Pipeline/#re-trigger-the-failed-pipeline","title":"Re-trigger The Failed Pipeline","text":"<p>Surf to The OCW Studio Pulumi Pipeline and trigger a new build.</p> <p>That should be it! Assuming your changes worked, the pipeline should now succeed. If it doesn't but you're sure your changes are correct, ensure that Doof actually finished deploying.</p>"},{"location":"application_specific_guides/ocw-studio/Update_OCW_Studio_Pipeline/#for-more-details","title":"For More Details","text":"<p>See The OCW Studio README</p>"},{"location":"application_specific_guides/odlbot/Resetting_Doof_Github_Token/","title":"Resetting odlbot / Doof's Github Access Token","text":""},{"location":"application_specific_guides/odlbot/Resetting_Doof_Github_Token/#note-this-doc-makes-lots-of-assumptions-about-prior-knowledge-todo","title":"Note: This doc makes lots of assumptions about prior knowledge. TODO :)","text":"<p>From time to time we need to go through the fire drill of rotating all our github access tokens.</p> <p>If you need to reset  Doof's you'll need to login to gtihub with the mitx-devops@mit.edu account.</p> <p>You can get the 2FA code from Vault in the platform-secrets github secret.</p> <p>Find the personal access token once logged in by clicking the account icon in the top right, then choosing Settings -&gt; Developer Settings -&gt; Personal Access Tokens (Classic)</p> <p>Doof actually gets deployed to heroku. log in to heroku.com with the mitx-devops account.</p> <p>You can find user, pass and 2FA in vault under platform-secrets/heroku.</p> <p>Once logged in, click the odl-release-bot application. Then click Settings and navigate to \"Reveal Config Vars\".</p> <p>Now use the web form to edit the value of the GITHUB_ACCESS_TOKEN variable. Replace its current contents with the new token you just generated on the Github site.</p> <p>That's it! Doof should be back in no time :)</p>"},{"location":"application_specific_guides/openedx/access_django_manage/","title":"How To Access An OpenEdX Django Admin manage.py","text":""},{"location":"application_specific_guides/openedx/access_django_manage/#pre-requisites","title":"Pre-requisites","text":"<p>You will need the following pieces of information to get started: - The product you want to access e.g. mitxonline, ocw-studio, xpro, mitx - The environment you want - one of ci, qa, or production. - Valid credentials to login to the MIT Open Learning AWS Account   - You should most likely have been given these as a part of your onboarding. - The MIT oldevops AWS key file - oldevops.pem which at the time of this document's   writing can be accessed from (TBD: I forget and have asaked my team to remind me.)</p>"},{"location":"application_specific_guides/openedx/access_django_manage/#finding-the-right-ec2-instance","title":"Finding The Right EC2 Instance","text":"<ol> <li>Log into the AWS console / web UI with your MIT issued credentials.</li> <li>Click the service selector (the tightly grouped bunch of white square boxes in the upper left) and choose EC2.</li> <li>Now click \"Instances (running)\"</li> <li>You will now see a text box with the prompt \"Find instances by attribute...\"</li> <li>In this box, type 'edxapp-worker--' - for instance, for mitxonline production you would type    'edxapp-worker-mitxonline-production' and hit enter/return. <li>You should now see a list of instances named edxapp-worker-mitxonline-production</li> <li>You'll need to temporarily add ssh access to the security group for your instance. Right click on the first instance    in the list and pick Security, then Change Security Groups. Type 'ssh' into the 'add security groups' text box. You    should see a group appropriate to your product, in the case of mitxonline-production I see mitxonline-production-public-ssh.    Select that group and click \"Add security group\" then click the orange Save button at the bottom of the page.</li> <li>Now, left click on the first instance in the list which should expand into instance detail. Click the little square within a    square Copy icon next to the \"Public IPV4 Address\".</li>"},{"location":"application_specific_guides/openedx/access_django_manage/#making-the-connection","title":"Making The Connection","text":"<p>From your laptop, use the oldevops.pem key to ssh to the ubuntu user on the machine whose IP you copied from the previous step.</p> <p>So for instance:</p> <p><code>ssh -i oldevops.pem ubuntu@34.204.173.109</code></p> <p>At this point you should be good to go and should see a prompt that looks something like:</p> <p><code>ubuntu@ip-10-22-3-162:~$</code></p>"},{"location":"application_specific_guides/openedx/enable_rapid_response_edxapp/","title":"Enable Rapid Response in edxapp","text":""},{"location":"application_specific_guides/openedx/enable_rapid_response_edxapp/#reference","title":"Reference","text":"<p>https://github.com/mitodl/rapid-response-xblock/blob/master/README.md</p>"},{"location":"application_specific_guides/openedx/enable_rapid_response_edxapp/#how-to","title":"How-To","text":"<ol> <li>Common env.yaml file must have <code>FEATURES:ALLOW_ALL_ADVANCED_COMPONENTS: True</code></li> <li>Common env.yaml file must have <code>ENABLE_RAPID_RESPONSE_AUTHOR_VIEW: True</code></li> <li>Rapid-response package pip packages must be installed in the image <code>rapid-response-xblock</code> and <code>ol-openedx-rapid-response-reports</code>. Refer to existing package listings in the openedx docker configs.</li> <li>In the LMS Admin UI -&gt; <code>/admin/lms_xblock/xblockasidesconfig/</code> create an <code>enabled</code> record.</li> <li>In the CMS Admin UI -&gt; <code>/admin/xblock_config/studioconfig/</code> create an 'enabled' record.</li> <li>Verify via studio on a test/demo course, find an existing multiple choice problem or create a new one. After creation in the 'unit view', you should now have a checkbox at the bottom of a multiple choice problem that will say 'Enable problem for rapid-response'.</li> </ol>"},{"location":"application_specific_guides/openedx/mysql_access/","title":"Querying An MIT OL Open EdX Mysql Database","text":"<p>This is a quick and dirty guide to connecting to the OpenEdX MySQL database for one of our products.</p>"},{"location":"application_specific_guides/openedx/mysql_access/#connect-to-an-ec2-server-that-hosts-containers-for-the-product-in-question","title":"Connect to an EC2 server that hosts containers for the product in question","text":"<p>For instance, in the case of XPro CI, connect to a server in instance group edxapp-[web/worker]-mitxpro-ci.</p> <p>You can find detailed instructions on how to do this here.</p>"},{"location":"application_specific_guides/openedx/mysql_access/#install-a-mysql-or-mariadb-client-if-ones-not-there-already","title":"Install a mysql or mariadb client if one's not there already","text":"<p>(If you do this in production environments, and you probably shouldn't, be sure to remove it when done.)</p> <p><pre><code>sudo apt install mariadb-client\n</code></pre> OR</p> <pre><code>sudo apt install mysql-client\n</code></pre>"},{"location":"application_specific_guides/openedx/mysql_access/#get-the-database-credentials","title":"Get the Database Credentials","text":"<p>Use docker compose to connect to a container like the LMS or CMS:</p> <pre><code>docker compose exec -it lms bash\n</code></pre> <p>and then, once inside the container, you can find the database connection information in /openedx/edx-platform/lms/envs/lms.yaml in the DATABASES section.</p> <p>Be sure to choose the correct database for your environment.</p>"},{"location":"application_specific_guides/openedx/mysql_access/#to-the-prompt","title":"To The Prompt!","text":"<p>Now, Ctrl-D out of the container bash you were logged into and get back to the host you sshed into.</p> <p>You can now use the mysql client to connect to the database.</p> <p>e.g.</p> <pre><code>mysql -h &lt;hostname&gt; -u &lt;username&gt; -p\n</code></pre> <p>You'll need to enter or paste in the password you found in the previous section when prompted.</p> <p>That should be all you need to get a mysql prompt to run queries against! Be careful, there are no guardrails here!</p>"},{"location":"application_specific_guides/openedx/prove_build_was_deployed/","title":"Proving That A Particular OpenEdX Build Was Deployed To a Product Environment","text":"<p>This doc is NOT exhaustive. It assumes prior knowledge of our products and EC2. I can fill in the exhaustive details later if there's ever time :)</p> <p>There are two methods for determining this.</p>"},{"location":"application_specific_guides/openedx/prove_build_was_deployed/#ami","title":"AMI","text":"<ul> <li>Find the instance in the AWS console. For instance if looking for an XPro CI instance, use <code>edxapp-web-xpro-ci</code> to find an XPro CI webserver. Select it.</li> <li>Click \"AMI\" an select the instance's AMI image.</li> <li>Under the \"Tags\" tab one of the tags is <code>edx_sha</code>. That's the Git repository hash this container is built from.</li> </ul>"},{"location":"application_specific_guides/openedx/prove_build_was_deployed/#container-contents","title":"Container Contents","text":"<ul> <li>Log onto the EC2 instance you want to check.</li> <li>Get a shell inside an LMS/CMS container e.g. <pre><code>cd /etc/docker/compose\nsudo -s\ndocker compose exec -it cms bash\n</code></pre></li> <li>Once inside you can change directory to /openedx/edx-platform and run <code>git log</code>. That will show you the commit that built the container.</li> </ul>"},{"location":"application_specific_guides/openedx/release_upgrades/","title":"How To Update The Release Version Of An Open edX Deployment","text":""},{"location":"application_specific_guides/openedx/release_upgrades/#open-edx-releases","title":"Open edX Releases","text":"<p>Open edX cuts new named releases approximately every 6 months. Each of our deployments needs to be able to independently upgrade the respective version on their own schedule. MITx Residential upgrades twice per year, roughly aligned with the release timing of the upstream Open edX platform, whereas xPro has typically upgraded on an annual basis in the December timeframe.</p> <p>Our MITx Online deployment of Open edX is a special case in that it deploys directly from the master branches of the different applications and has a weekly release schedule.</p>"},{"location":"application_specific_guides/openedx/release_upgrades/#open-learnings-open-edx-deployments","title":"Open Learning's Open edX Deployments","text":"<p>At Open Learning we have four distinct installations of Open edX, each with their own release cadence and configuration requirements. For each of those deployments we have multiple environment stages (e.g. CI, QA, Production). Each of those deployment * stages combinations needs to be able to have their versions managed independently, with newer versions being progressively promoted to higher environments.</p> <p>This is further complicated by the long testing cycles for new releases in a given deployment. We need to be able to deploy a newer release to a lower environment stage, while maintaining the ability to deploy patches of the current release in a given time period to subsequent environments. For example, as we start testing the Palm release for the MITx Residential deployment we need to get it installed in the CI environment. Meanwhile, we may have a bug-fix or security patch that needs to be tested in the QA environment and propagated to Production.</p> <p>In addition to the cartesian product of (versions * deployments * env stages) we also have to manage these values across the different components of the Open edX ecosystem. For example, the core of the platform is called edx-platform, which in turn relies on: - a forum service - the edx-notes API service - a codejail REST API service - and myriad MFEs (Micro-FrontEnds).</p> <p>For any of these different components we may need to be able to override the repository and/or branch that we are building/deploying from.</p>"},{"location":"application_specific_guides/openedx/release_upgrades/#versioning-of-build-and-deploy","title":"Versioning of Build and Deploy","text":"<p>In order to support this sea of complexity we have a module of <code>bridge.settings.openedx</code> that has a combination of data structures and helper functions to control what applications and versions get deployed where and when. These values are then used in the build stages of the different components, as well as driving the deployment pipelines in Concourse that are responsible for orchestrating all of this.</p>"},{"location":"application_specific_guides/openedx/release_upgrades/#managing-supported-releases","title":"Managing Supported Releases","text":"<p>We only want to support the build and deployment of a minimal subset of Open edX releases. This is controlled by the Enum <code>OpenEdxSupportedRelease</code> in the <code>bridge.settings.openedx.types</code> module. When there is a new release created it needs to be added to this Enum.  For example, to add support for the Palm release a new line is added of the format <code>palm = (\"palm\", \"open-release/palm.master\")</code></p> <p>When we have upgraded all deployments past a given release we remove it from that Enum so that we no longer need to worry about maintaining configuration/code/etc. for that release.</p>"},{"location":"application_specific_guides/openedx/release_upgrades/#performing-an-upgrade","title":"Performing An Upgrade","text":"<p>There are two data structures that control the applications and versions that get included in a given deployment and which version to use in the respective environment stage. The <code>OpenLearningOpenEdxDeployment</code> Enum is the top level that sets the release name for the environment stage of a given deployment.</p> <p>There are situations where we need to customize a component that is being deployed. In those cases we typically create a fork of the upstream repository where we manage the patches that we require. The <code>ReleaseMap</code> dictionary is used to manage any overrides of repository and branch for a given component of the platform, as well as which components are included in that deployment. The <code>OpenEdxApplicationVersion</code> structure will map to the default repositories and branches for a given component, but supplies a <code>branch_override</code> and <code>origin_override</code> parameter to manage these customizations.</p> <p>For example, to upgrade our MITx Residential deployments to start testing the Palm release we change the <code>CI</code> stages of the <code>mitx</code> and <code>mitx-staging</code> deployments to use the <code>palm</code> value for the <code>OpenEdxSupportedRelease</code></p> <pre><code>@@ -13,7 +13,7 @@ class OpenLearningOpenEdxDeployment(Enum):\n     mitx = DeploymentEnvRelease(\n         deployment_name=\"mitx\",\n         env_release_map=[\n-            EnvRelease(\"CI\", OpenEdxSupportedRelease[\"olive\"]),\n+            EnvRelease(\"CI\", OpenEdxSupportedRelease[\"palm\"]),\n             EnvRelease(\"QA\", OpenEdxSupportedRelease[\"olive\"]),\n             EnvRelease(\"Production\", OpenEdxSupportedRelease[\"olive\"]),\n         ],\n@@ -21,7 +21,7 @@ class OpenLearningOpenEdxDeployment(Enum):\n     mitx_staging = DeploymentEnvRelease(\n         deployment_name=\"mitx-staging\",\n         env_release_map=[\n-            EnvRelease(\"CI\", OpenEdxSupportedRelease[\"olive\"]),\n+            EnvRelease(\"CI\", OpenEdxSupportedRelease[\"palm\"]),\n             EnvRelease(\"QA\", OpenEdxSupportedRelease[\"olive\"]),\n             EnvRelease(\"Production\", OpenEdxSupportedRelease[\"olive\"]),\n         ],\n</code></pre> <p>Because Palm is a new release for these deployments we also need to add a <code>palm</code> key to the <code>ReleaseMap</code> dictionary that contains the applications that are associated with those deployments and the appropriate <code>OpenEdxApplicationVersion</code> records for that deployment.</p> <pre><code>@@ -61,6 +61,122 @@ ReleaseMap: dict[\n     OpenEdxSupportedRelease,\n     dict[OpenEdxDeploymentName, list[OpenEdxApplicationVersion]],\n ] = {\n+    \"palm\": {\n+        \"mitx\": [\n+            OpenEdxApplicationVersion(\n+                application=\"edx-platform\",  # type: ignore\n+                application_type=\"IDA\",\n+                release=\"palm\",\n+                branch_override=\"mitx/palm\",\n+                origin_override=\"https://github.com/mitodl/edx-platform\",\n+            ),\n             ...\n+        ],\n+        \"mitx-staging\": [\n+            OpenEdxApplicationVersion(\n+                application=\"edx-platform\",  # type: ignore\n+                application_type=\"IDA\",\n+                release=\"palm\",\n+                branch_override=\"mitx/palm\",\n+                origin_override=\"https://github.com/mitodl/edx-platform\",\n+            ),\n             ...\n+        ],\n+    },\n     \"olive\": {\n         \"mitx\": [\n             OpenEdxApplicationVersion(\n</code></pre> <p>All of the deployment pipelines for these application components are managed by a corresponding <code>meta</code> pipeline that will automatically update the build and pipeline configuration based on the changed version information as soon as it is merged into the <code>master</code> branch of <code>ol-infrastructure</code>.</p> <p>Note - TMM 2023-04-14</p> <ul> <li>The current configuration of our meta pipelines means that before the updates to the   <code>bridge.settings.openedx.version_matrix</code> module can be picked up by the <code>meta</code>   pipelines the <code>ol-infrastructure</code> docker image needs to be built and pushed to our   registry so that it can be loaded. This means that once the ol-infrastructure image   pipeline   completes it might be necessary to manually trigger the meta pipelines again.</li> <li>Once a new release is deployed to a given environment stage for the first time it may   be necessary to manually ensure that all database migrations are run properly. It will   be attempted automatically on deployment, but there are often conflicts between the   existing database state and the migration logic that require intervention.</li> </ul>"},{"location":"application_specific_guides/openedx/release_upgrades/#updating-an-mitol-openedx-branch-with-the-latest","title":"Updating An MITOL OpenEdX Branch With The Latest","text":"<p>This involves updating our edx-platform fork and rebasing the correct branch.</p> <p>For instance, to update our forked OpenEdX Sumac branch for MITxOnline, you would:</p> <ul> <li>Check out our fork of the edx-platform repo - <code>gh repo clone mitodl/edx-platform</code></li> <li>Change to the right branch - <code>git checkout mitx/sumac</code></li> <li>Update all the things - <code>git fetch --all</code></li> <li>Rebase from upstream - <code>git pull --rebase upstream open-release/sumac.master</code></li> <li>Check the results, and if correct push the branch - <code>git push origin mitx/sumac --force</code></li> </ul>"},{"location":"application_specific_guides/openedx/release_upgrades/#troubleshooting","title":"Troubleshooting","text":""},{"location":"application_specific_guides/openedx/release_upgrades/#openedx-redwood","title":"OpenEdX Redwood","text":"<p>With each new OpenEdX release, inevitably there will be problems. For instance, in the Redwood release, we noticed that login for MITX CI was failing with a 500 response code.</p> <p>So we ran the following Grafana query using the query builder:</p> <pre><code>{environment=\"mitx-ci\", application=\"edxapp\"} |= `` | json | line_format `{{.message}}\n</code></pre> <p>That yielded a problem with JWT signing keys as the exception cited a signing failure.</p> <p>So we had to log onto an EC2 instance appropriate to MITX CI and run the following command from the lms container shell:</p> <pre><code>manage.py generate_jwt_signing_key\n</code></pre> <p>That produced public and private signing key JSON blobs, which we could then add to SOPS and then through the pipeline into Vault where they'll be picked up as Redwood deploys to the various apps and environments.</p>"},{"location":"application_specific_guides/openedx/retire_an_openedx_user/","title":"How To Retire An MIT Online Learning OpenEdX User","text":""},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>You'll need the email address the user registered under. e.g. cpatti@mit.edu</li> <li>You'll need the pre-requisites defined in How To Access An MIT OL OpenEdX Django Admin manage.py.</li> </ul>"},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#look-up-this-users-username","title":"Look Up This User's username","text":"<p>You'll need to have Django admin and superuser access to the product you're looking to retire users for.</p> <p>Anyone who has the requisite access already can help. pdpinch@ and the Devops team are good folks to ask.</p> <p>Next, you'll need to login to the admin web UI for the application we'll be working with. You can find that URL from the Application Links Page.</p> <p>Find the product in question (e.g. mitxonline) and choose the LMS url for the environment you want (e.g. production) as a base. Then add /admin on to the end. Ensure there's only one slash before /admin or things will go awry.</p> <p>In our case, since we are looking to retire users from mitxonline, we'll use https://courses.mitxonline.mit.edu/admin</p> <p>Find the Users link on that page, click it. Now type the email address into the search box and click Search.</p> <p>This should yield the user's username. Copy that off into a safe place as we'll need it for the next section.</p>"},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#get-yourself-connected","title":"Get Yourself Connected","text":"<p>First, follow the step by step instructions defined in How To Access An MIT OL OpenEdX Django Admin manage.py.</p> <p>This will land you at a shell prompt of the OpenEdX worker for the product in question.</p> <p>It should look something like:</p> <p><code>ubuntu@ip-10-22-3-162:~$</code></p>"},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#prepare-your-environment","title":"Prepare Your Environment","text":"<p>Next you'll need to prepare your UNIX shell's environment to be able to run the manage.py command.</p> <p>Type: <code>sudo su - edxapp -s /bin/bash</code></p> <p>Then type <code>source edxapp_env</code>.</p> <p>At this point you should be ready to run the user retirement command.</p>"},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#get-to-the-retiring-already","title":"Get To The Retiring Already!","text":""},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#the-invocation","title":"The Invocation","text":"<p>Here's the command you'll use to retire a user.</p> <p><code>python edx-platform/manage.py lms retire_user --user_email &lt;user email&gt; --username '&lt;username&gt;'</code></p> <p>The single quotes around username are important in case there are any spaces in there. Otherwise the shell will mis-parse the command and throw an error.</p> <p>So for example if I wanted to retire myself from mitxonline production, I'd use:</p> <p><code>python edx-platform/manage.py lms retire_user --user_email cpatti@mit.edu --username 'ChrisPatti'</code></p> <p>You should see a bunch of very voluminous output. Most of it is honestly garbage for our purposes. We'll focus on the bits we care about at the end:</p> <p><code>2023-06-21 16:07:20,714 INFO 186970 [openedx.core.djangoapps.user_api.management.commands.retire_user] [user None] [ip None] retire_user.py:173 - User successfully moved to the retirement pipeline</code></p> <p>The None here isn't anything to worry about. It's the system trying to prevent us from leaking PII (personally identifiable information) into the logs.</p> <p>At this point if all went well, we're done with our edx worker shell prompt for now so we can log out. Always be super careful to not leave production shells open unnecessarily. You'd be surprised how many systems have been brought down by someone not realizing they're in the wrong terminal :)</p>"},{"location":"application_specific_guides/openedx/retire_an_openedx_user/#priming-the-pump-well-pipeline-in-this-case","title":"Priming The Pump (Well, Pipeline In This Case)","text":"<p>Now that we've successfully staged our user for retirement, we need to tell the retirement pipeline to actually retire the user.</p> <ul> <li>Surf to the appropriate concourse URL for the entironment you're working with:</li> <li>CI</li> <li>QA</li> <li>Production</li> </ul> <p>and search for 'tubular'. You'll want the tubular pipeline in the group associated with whichever product you're working with. In our case, it'd be   the misc-cloud-tubular pipeline in the mitxonline group. - Click the green + icon with a circle around it in the upper right of your screen. This will trigger a run of this pipeline. - If all goes well, you should see each stage go green one by one. You can click on any stage to see more detail around what that stage is doing.   You can see an example of a successful pipeline run here</p> <p>At this point, if the pipeline is green, congratulations are in order! This user had been retired!</p>"},{"location":"application_specific_guides/openedx/retirement_pipeline/","title":"Setting Up The Retirement Pipeline For A New MIT OL OpenEdX Application","text":""},{"location":"application_specific_guides/openedx/retirement_pipeline/#getting-set-up","title":"Getting Set Up","text":"<p>First, follow the instructions in the Setting Up User Retirement In The LMS document.</p> <p>You'll need to ssh into an edx-worker for the appropriate environment you're working in.</p> <p>Run <code>sudo su - edxapp -s /bin/bash</code> and then <code>source edxapp_env</code> to get to where you need to be to run the Django manage application.</p> <p>You can follow the doc verbatim with the exception that: * The manage.py executable is located in the edx-platform folder, so you'd start your invocations with <code>python edx-platform/manage.py</code> * Ignore the <code>--settings &lt;your settings&gt;</code> option as we already have our settings defined in the LMS configuration.</p> <p>Be sure that the retirement tates, user and application creation invocations exit without throwing any fatal errors. ALL these scripts unfortunately print a number of warnings, which can be ignored.</p> <p>Here's what the output of each section should look approxiately like:</p>"},{"location":"application_specific_guides/openedx/retirement_pipeline/#retirement-states-creation","title":"Retirement States Creation","text":"<pre><code>States have been synchronized. Differences:\n   Added: {'COMPLETE', 'FORUMS_COMPLETE', 'ENROLLMENTS_COMPLETE', 'RETIRING_LMS_MISC', 'RETIRING_LMS', 'NOTES_COMPLETE', 'RETIRING_ENROLLMENTS', 'PENDING', 'RETIRING_NOTES', 'PROCTORING_COMPLETE', 'RETIRING_FORUMS', 'ABORTED', 'ERRORED', 'LMS_MISC_COMPLETE', 'RETIRING_PROCTORING', 'LMS_COMPLETE'}\n   Removed: set()\n   Remaining: set()\nStates updated successfully. Current states:\nPENDING (step 1)\nRETIRING_FORUMS (step 11)\nFORUMS_COMPLETE (step 21)\nRETIRING_ENROLLMENTS (step 31)\nENROLLMENTS_COMPLETE (step 41)\nRETIRING_NOTES (step 51)\nNOTES_COMPLETE (step 61)\nRETIRING_PROCTORING (step 71)\nPROCTORING_COMPLETE (step 81)\nRETIRING_LMS_MISC (step 91)\nLMS_MISC_COMPLETE (step 101)\nRETIRING_LMS (step 111)\nLMS_COMPLETE (step 121)\nERRORED (step 131)\nABORTED (step 141)\nCOMPLETE (step 151)\nedxapp@ip-10-22-2-49:~$\n</code></pre>"},{"location":"application_specific_guides/openedx/retirement_pipeline/#retirement-user-creation","title":"Retirement User Creation","text":"<pre><code>Created new user: \"retirement_service_worker\"\nSetting is_staff for user \"retirement_service_worker\" to \"True\"\nSetting is_superuser for user \"retirement_service_worker\" to \"True\"\nAdding user \"retirement_service_worker\" to groups []\nRemoving user \"retirement_service_worker\" from groups []\n2023-04-28 21:10:00,691 INFO 69935 [common.djangoapps.student.models.user] [user None] [ip None] user.py:782 - Created new profile for user: retirement_service_worker\n</code></pre>"},{"location":"application_specific_guides/openedx/retirement_pipeline/#retirement-dot-application-creation","title":"Retirement DOT Application Creation","text":"<pre><code>None] [ip None] create_dot_application.py:82 - Created retirement application with id: 10, client_id: XXXXXXXXXXXXXXXXXXXXXXXX, and client_secret: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n</code></pre> <p>You'll need to save this client_id and secret somewhere because you'll need to use it in the next step!</p>"},{"location":"application_specific_guides/openedx/retirement_pipeline/#we-keep-secrets-in-the-vault-duh","title":"We Keep Secrets In The Vault, Duh :)","text":"<p>Next, we'll need to make these secrets accessible to the pipeline so they can be consumed by the various retirement worker scripts.</p> <p>In order to do that, we'll use a utility called sops to encrypt the secrets so we can safely store them in Github and conveyed to Vault.</p> <p>Unfortunately setting yourself up to use sops is a bit of a process in and of itself and it outside the scope of this document.</p> <p>Once you've got sops squared away, use it on the appropriate operational configuration file. Here's the one for QA.</p> <p>So you'd invoke: <code>sops operations.qa.yml</code>. At that point, sops will decrypt the file and open your editor of choice with its contents.</p> <p>Now, we want to add the necessary secrets we copied in the previous step, as well as the LMS host for this environment. You can find the LMS hosts for the various environments listed in the App Links Wiki page.</p> <p>If you're unsure, ask an old hand for help. Disambiguating the various environments can be tricky, and better safe than sorry!</p> <p>Here's an example of what I added for mitxonline qa:</p> <pre><code>  mitxonline/tubular_oauth_client:\n    id: CLIENTIDUNENCRYPTEDSECRETSAREFUNYESTHEYARE\n    secret: EVENMORESLIGHTLYLONGERGIBBERISHTHATISYOURUNENCRYPTEDSECRET\n    host: https://courses-qa.mitxonline.mit.edu\n</code></pre> <p>Once you've made your additions, save the file and quit your editor. sops will now do its magic and re-encrypt those values.</p> <p>BE SURE NOT TO COMMIT UNENDCRYPTED SECRETS TO GITHUB.</p> <p>Now create a pull request and get these changes merged. If this is for production, you'll also need to kick off this pipeline manually.</p> <p>At this point, your secrets should be in vault and available to the pipeline we're about to build.</p>"},{"location":"application_specific_guides/openedx/retirement_pipeline/#start-the-rube-goldberg-device-building-the-pipeline-itself","title":"START THE RUBE GOLDBERG DEVICE! Building The Pipeline Itself","text":"<p>In order to get this done, you'll need to have the Concourse <code>fly</code> CLI command installed and an appropriate target for your Concourse server and team defined.</p> <p>I like to keep my target names reasonably short, so for the target I created for mitxonline QA, I ran:</p> <p><code>fly login --target mo-qa --team-name=mitxonline --concourse-url https://cicd-qa.odl.mit.edu</code></p> <p>Once we have a suitable target defined, we can use it to actually create our pipeline for real:</p> <ul> <li>From a checked out mitodl/ol-infrastructure repository, change directory to the <code>src/ol_concourse/pipelines/open_edx/tubular</code> folder.</li> <li>Run <code>poetry run python tubular.py</code> - This should print a reasonable looking blob of JSON to the screen with a fly command to run at the bottom.</li> <li>Go ahead and run that command! For example, given the mo-qa target I defined above, I ran: <code>fly -t mo-qa sp -p misc-cloud-tubular -c definition.json</code>.</li> <li>At this point fly should show you the pipeline definition and will ask you if you want to actually make the change. Answer yes.</li> <li>Assuming there were no errors, your pipeline definition should be in place and we're ready to get the ball rolling!</li> <li>You can either use the fly unpause-pipeline or else, in the Concourse web UI, click the little Play icon at the very top right of the screen. </li> <li>The build is set to run once a week, so once you unpause the pipeline, you may noeed to click the + icon in order to actually kick off a build.</li> </ul> <p>That's it! Obviously keep an eye out on the pipeline for any failures.</p>"},{"location":"application_specific_guides/renovate/updating_renovate_config/","title":"Validating Renovate Configuration Changes","text":"<p>You can find our Renovate configuration in the .github repository at the root of our mitodl organization here.</p> <p>You can validate any changes you make by running the following <code>npx</code> command. (This requires a working node.js installation): <pre><code>npx --package renovate renovate-config-validator\n</code></pre></p> <p>This will surface any errors you may have made, and if all is well you'll see a message like:</p> <p><code>\u2500cpatti at rocinante in ~/src/mit/.github on main\u2714 25-09-24 - 16:46:22 \u2570\u2500\u2820\u2835 npx --package renovate renovate-config-validator                                                      &lt;region:us-east-1&gt;  INFO: Validating renovate.json  INFO: Config validated successfully</code></p>"},{"location":"cookbooks_patterns/devops_patterns_cookbook/","title":"MIT OL Devops Patterns Cookbook","text":""},{"location":"cookbooks_patterns/devops_patterns_cookbook/#summary","title":"Summary","text":"<p>This document will serve as a place to put patterns and best practices.</p> <p>The goal is to ease the path for both new builders and experienced builders by helping narrow down the bevvy of choices and present a workable best practice solution to a given problem.</p> <p>There are many potential ways to organize a document like this, but for now I intend to start with the recipes broken down by infrastructure components.</p> <p>Example of what I mean are Docker, Traefik, Pyinfra and the like.</p> <p>Please write your recipes in the following form with What I Want and How To Build It as bold subsections.</p>"},{"location":"cookbooks_patterns/devops_patterns_cookbook/#recipes","title":"Recipes","text":""},{"location":"cookbooks_patterns/devops_patterns_cookbook/#traefik","title":"Traefik","text":""},{"location":"cookbooks_patterns/devops_patterns_cookbook/#token-based-authentication","title":"Token Based Authentication","text":"<p>What I Want</p> <p>I want Traefik to allow requests only from clients that pass a particular token in the HTTP headers of the request. Here's an example <code>curl</code> from Tika with the actual token:</p> <pre><code>curl  --header 'X-Access-Token: &lt;crazy hex digits&gt;' https://tika-qa.odl.mit.edu\n</code></pre> <p>How To Build It</p> <p>Traefik does not contain this functionality by default, so we must leverage the checkheaders Traefik middleware plugin.</p> <p>You will need to add a blob to your Traefik static configuration like this:</p> <pre><code>experimental:\n  plugins:\n    checkheadersplugin:\n      moduleName: \"github.com/dkijkuit/checkheadersplugin\"\n      version: \"v0.2.6\"\n</code></pre> <p>Since we use pyinfra to automate our image builds, you'll need to add code like this to your deploy.py and a line like this to your docker-compose.yaml file.</p>"},{"location":"getting_started/developer_eks_access/","title":"Developer EKS Access","text":""},{"location":"getting_started/developer_eks_access/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>an environment variable <code>GITHUB_TOKEN</code> set to a classic GitHub token with <code>read:org</code> permissions.</li> <li>Latest (as of 02-19-2025) <code>aws-cli</code> is installed and available on your <code>$PATH</code>.</li> <li><code>kubectl</code> &gt;= 1.30 is installed and available on your <code>$PATH</code>. (Newer is better, usually)</li> <li>Cloned copy of ol-infrastructure</li> <li>Should run with a standard python install with <code>hvac</code> installed. Alternatively, follow the instructions in ol-infrastructure/README.md.</li> </ul>"},{"location":"getting_started/developer_eks_access/#overview","title":"Overview","text":"<p>This document will guide you through the process of setting up your local environment to access the EKS cluster. This will allow you to interact with applications deployed into EKS, including tailing logs and opening a shell into the running containers.</p>"},{"location":"getting_started/developer_eks_access/#extra-reading","title":"Extra Reading","text":"<p>For more information about <code>kubeconfig</code> files, refer to the Kubernetes documentation here and here.</p>"},{"location":"getting_started/developer_eks_access/#steps","title":"Steps","text":"<ol> <li>Within your cloned copy of <code>ol-infrastructure</code>, navigate to the <code>eks</code> directory at <code>src/ol_infrastructure/infrastructure/aws/eks</code> (or prefix the script with that path).</li> <li> <p>There is a script in this directory called <code>login_helper.py</code> that will help you set up your local environment to access the EKS cluster. Run this script with the following command:</p> <pre><code>uv run login_helper.py aws_creds\n</code></pre> </li> <li> <p>This will return several <code>export AWS_</code> statements on <code>stdout</code> that you can then run in your current shell. You need ALL of them. Additionally, it will include the timestamp of when these credentials will expire. By default they expire in one hour, but you can change that to 8 hours with <code>-d 480</code> argument. 8 hours is the maximum allowed.</p> </li> <li> <p>After running the <code>export</code> commands on your shell, run the following command to generate a <code>kubeconfig</code> file:</p> <pre><code>uv run login_helper.py kubeconfig\n</code></pre> </li> <li> <p>This will generate a <code>kubeconfig</code> file on <code>stdout</code> that will define contexts to all active clusters. Each context is named for the cluster so for example <code>operations-ci</code>, <code>applications-qa</code>, and so on.</p> </li> <li> <p>You can save this <code>kubeconfig</code> file to your local machine for kubectl to use with the following:</p> <pre><code>uv run login_helper.py kubeconfig &gt; ~/.kube/config\n</code></pre> </li> <li> <p>Additionally, you can specify a default current context with <code>--set-current-context &lt;context&gt;</code> argument.</p> <pre><code>uv run login_helper.py kubeconfig --set-current-context applications-qa &gt; ~/.kube/config\n</code></pre> </li> <li> <p>Or you can set it by hand once you've saved your <code>kubeconfig</code> file:</p> <pre><code>kubectl config use-context applications-qa\n</code></pre> </li> <li> <p>You can now interact with the EKS cluster using <code>kubectl</code>. For example, to list all pods in the <code>learn-ai</code> namespace of the <code>applications-qa</code> cluster:</p> <pre><code>kubectl get pods -n learn-ai\n</code></pre> </li> </ol> <p>Tip</p> <p><code>login_helper</code> sends progress logs to stderr; only relevant output goes to stdout. The <code>aws_creds</code> and <code>kubeconfig</code> steps can easily be combined in a private script (example).</p>"},{"location":"getting_started/developer_eks_access/#other-interesting-kubectl-commands","title":"Other Interesting <code>kubectl</code> Commands","text":"<ul> <li>Get all the pods for the learn-ai namespace:</li> </ul> <pre><code>kubectl get pods -n learn-ai\n</code></pre> <ul> <li>Get all the pods in the learn-ai namespace with more information:</li> </ul> <pre><code>kubectl get pods -n learn-ai -o wide\n</code></pre> <ul> <li>Describe a pod, which can tell you interesting things like the pod's IP address, the node it's running on, and the events that have happened to it, as well as the containers that make up the pod:</li> </ul> <pre><code>kubectl describe pod &lt;pod-name&gt; -n learn-ai\n</code></pre> <ul> <li>Output the logs of a pod in the learn-ai namespace. <code>kubectl</code> will makes its best guess at which container to output logs from:</li> </ul> <pre><code>kubectl logs &lt;pod-name&gt; -n learn-ai\n</code></pre> <ul> <li>Be specific and output the logs from the <code>nginx</code> container.</li> </ul> <pre><code>kubectl logs &lt;pod-name&gt; -n learn-ai -c nginx\n</code></pre> <ul> <li><code>tail</code> or follow the logs of the nginx container in a pod in the learn-ai namespace:</li> </ul> <pre><code>kubectl logs -f &lt;pod-name&gt; -n learn-ai -c nginx\n</code></pre> <ul> <li>Open a shell into the nginx container of a pod in the learn-ai namespace:</li> </ul> <pre><code>kubectl exec -it &lt;pod-name&gt; -n learn-ai -c nginx -- /bin/bash\n</code></pre>"},{"location":"getting_started/emergency-break-glass/","title":"Emergency Break Glass Procedure","text":"<p>In Slack, simply type:</p> <pre><code>/rootly page\n</code></pre> <p>At this point you'll be presented with a dialog that looks something like this:</p> <p></p> <p>Under \"What's the Problem?\" give a very short description of the issue.</p> <p>Under \"Who Do You Want To Notify?\" choose the Platform Engineering Team in the dropdown.</p> <p>Don't bother with the detailed Description, you'll want those details in the Slack channel or Github issue.</p> <p>Click \"Start Paging\" and that's it. You've successfully signalled an emergency and broken glass.</p>"},{"location":"getting_started/pe-product-infra-map/","title":"This Document Will Help Map Formal Product Names To Infrastructure","text":""},{"location":"getting_started/pe-product-infra-map/#projects-that-live-in-kubernetes","title":"Projects That Live In Kubernetes","text":"Formal Product Name Kubernetes Cluster Environments (e.g. CI, QA, Production) MIT Learn Applications CI, QA, Production MIT Learn AI Applications CI, QA, Production MITx Online Applications CI, QA, Production"},{"location":"getting_started/pe-product-infra-map/#pre-k8s-projects","title":"Pre K8s Projects","text":"<p>Nota Bene: For infra like Consul or Vault, we're not enumerating each combination because you get the idea :)</p> Formal Product Name EC2 Instance Greoup Name Deployment Technology MIT x Residential edxapp-worker-mitx-production, docker compose edxapp-web-mitx-production, docker compose ODL Video Service odl-video-service-production docker compose OpenEdX Forum V2 open-edx-forum-server-production Consul consul-mitx-production OpenEdX Notes edx-notes-server-mitx-production docker compose Hashicorp Vault vault-server-operations-production"},{"location":"getting_started/update_this_site/","title":"Updating This Site","text":"<ol> <li>Run <code>uv sync</code> to install dependencies.</li> <li>Make your changes to the Markdown files or add new ones.</li> <li>Run <code>uv run mkdocs build</code> to build the site.</li> <li>Run <code>uv run mkdocs serve</code> to preview the site locally.</li> <li>When you're happy, run <code>uv run mkdocs gh-deploy</code> to push the changes to the GitHub Pages site.</li> </ol>"},{"location":"getting_started/vault_access_for_developers/","title":"OL Secrets Management","text":"<p>The Engineering group in Open Learning hosts its own Hashicorp Vault clusters to handle secrets management. Part of our work is to provide OL developers access to Vault QA to perform the following tasks: 1. Securely share secrets 2. Generate local .env file for app development</p>"},{"location":"getting_started/vault_access_for_developers/#requirements","title":"Requirements","text":"<ul> <li>A Keycloak account in the <code>ol-platform-engineering</code> realm. If you currently do not have one, please ping someone in Platform Engineering to set you up.</li> </ul>"},{"location":"getting_started/vault_access_for_developers/#1-securely-share-secrets","title":"1. Securely share secrets","text":"<ul> <li>Log in to Vault QA<ul> <li>Method: <code>OIDC</code></li> <li>Role: <code>local-dev</code></li> <li>Note: Make sure to enable popups</li> </ul> </li> </ul> <p>You should see a popup asking for your Keycloak username and password. Once successfully authenticated, you should see the Vault UI where we have configured a separate Vault mount <code>secret-sandbox</code> that should be used for securely sharing sensitive data with the group.</p> <p><code>WARNING - DO NOT store anything permanent in that mount as there are no guarantees that they will not be deleted or overwritten. Any values stored in this mount are visible and accessible by all users of the `ol-platform-engineering` Keycloak realm which as of this writing is and should be restricted to OL Engineering staff.</code></p>"},{"location":"getting_started/vault_access_for_developers/#2-generate-local-env-file-for-app-development","title":"2. Generate local .env file for app development","text":"<ul> <li>Install Hashicorp Vault</li> <li>Download the relevant app vars shell script from the repo (Ex. app_vars.sh)</li> <li>In your terminal do the following:<ul> <li>Set Vault's url as an environment variable<ul> <li><code>export VAULT_ADDR=https://vault-qa.odl.mit.edu</code></li> </ul> </li> <li>Login to Vault from the CLI:<ul> <li><code>vault login -method=oidc role=\"local-dev\"</code></li> </ul> </li> <li>Run the following to generate vault client config:     <pre><code>vault agent generate-config -type=\"env-template\" \\\n-exec=\"./app_vars.sh\" \\\n-path=\"secret-dev/*\" \\\n-path=\"secret-operations/mailgun\" \\\n-path=\"secret-operations/global/embedly\" \\\n-path=\"secret-operations/global/odlbot-github-access-token\" \\\n-path=\"secret-operations/global/mit-smtp\" \\\n-path=\"secret-operations/global/update-search-data-webhook-key\" \\\n-path=\"secret-operations/sso/mitlearn\" \\\n-path=\"secret-operations/tika/access-token\" \\\nagent-config.hcl\n</code></pre></li> <li>Start the vault agent:<ul> <li><code>vault agent -config=agent-config.hcl -log-level=error</code></li> </ul> </li> </ul> </li> </ul> <p>You should now have a .env file containing the secrets for the relevant app from vault.</p>"},{"location":"getting_started/vault_access_for_developers/#references","title":"References","text":"<ul> <li>https://developer.hashicorp.com/vault/docs/agent-and-proxy/agent/generate-config</li> </ul>"},{"location":"getting_started/vault_access_for_developers/#bonus-note-generating-iam-creds-for-mit-learn-local-development","title":"Bonus Note : Generating IAM creds for MIT Learn local development","text":"<p>Apparently Devs sometimes need valid IAM credentials for local development with MIT Learn. Previously people were just lifting these creds out of Heroku settings but that isn't an option now. To replace that, use the following link:</p> <p>https://vault-qa.odl.mit.edu/ui/vault/secrets/aws-mitx/credentials/ol-mitopen-application</p> <p>You will need to login with a classic github token that has <code>read:org</code> permissions. This link will allow you to generate IAM creds that last 32 days and are personal to just you rather than shared ones from the app itself.</p>"},{"location":"monitoring_observability/bump_sentry_credit/","title":"How To Bump Our Sentry Credits When We Run Out","text":"<p>Sometimes despite our best efforts, one of our projects spirals out of control and generates a HUGE number of events which can overrun our Sentry quota and cause errors to get dropped.</p> <p>The way to both review our status, check dropped events AND boost our quota is to surf to the Sentry URL for MIT OL and navigate to Settings -&gt; Subscription.</p> <p>Once there you can see all our current stats including current usage, quota percentage and dropped events/errors if any.</p> <p>Use the \"Edit\" button to increase our quota if necessary to get us through the month AFTER addressing whichever product is causing the overage.</p>"},{"location":"monitoring_observability/inventory/","title":"Tools","text":"<ul> <li>Sentry</li> <li>Purpose: Detailed application monitoring and error logging.</li> <li>Healthchecks.io</li> <li>Purpose: Absence of alerting detection. For determining when other alerting tools may have failed or become unresponsive.</li> <li>GrafanaCloud</li> <li>Purpose: Collecting and storing metric and log data from applications and infrastructure.</li> <li>Subcomponents:<ul> <li>Grafana: Visualization and alerting on metric and log data.</li> <li>Cortex: Backend for storing metric data.</li> <li>Loki: Backend for storing log data</li> </ul> </li> <li>Pingdom</li> <li>Purpose: External synthetic and HTTP monitoring.</li> </ul>"},{"location":"monitoring_observability/inventory/#usage-matrix","title":"Usage Matrix","text":"ODL App / Component / Process Sentry HealthChecks.io Grafana Metrics Grafana Logs OCW Studio (webapp) Yes Yes Yes Yes OCW Site (static content) N/A N/A No No OCW Site Backup (process) N/A Yes N/A No MITx (webapp) Yes No No Yes MITx (openEdx) Yes No No Yes MITx (infrastructure) N/A N/A Yes Yes xPro (webapp) Yes No No Yes xPro (openEdx) Yes No No Yes xPro (infrastructure) N/A N/A Yes Yes Residential (openEdx) Yes No No Yes Residential (infrastructure) N/A N/A Yes Yes MIT Open (webapp) Yes No No Yes MIT Open Discussions (webapp) Yes No No Yes MIT Open Reddit (webapp) Yes No No Yes odl-video (webapp) Yes No No Yes Bootcamps (webapp) Yes No No Yes MicroMasters (webapp) Yes No No Yes"},{"location":"monitoring_observability/inventory/#synthetic-monitoring","title":"Synthetic Monitoring","text":"ODL App / Component / Process URL Pingdom Grafana Bootcamp production bootcamp.odl.mit.edu yes no MITx CAS cas.mitx.mit.edu yes no MITx Online Production Application nitxonline.mit.edu yes no MITx Online Production edX courses.mitxonline.mit.edu yes no MITx Online QA edX Application courses-qa.mitxonline.mit.edu yes no MITx Online RC Application rc.mitxonline.mit.edu yes no MITx QA CMS studio-mitx-qa.mitx.mit.edu yes no MITx QA LMS mitx-qa.mitx.mit.edu yes no MITx current QA preview preview-mitx-qa.mitx.mit.edu yes no MITx production CMS studio.mitx.mit.edu yes no MITx production CMS draft studio-staging.mitx.mit.edu yes no MITx production LMS lms.mitx.mit.edu yes no MITx production LMS draft staging.mitx.mit.edu yes no MITx production preview preview.mitx.mit.edu yes no MITx production preview draft preview.mitx.mit.edu yes no Micromasters CI micromasters-ci.odl.mit.edu yes no Micromasters RC micromasters-rc.odl.mit.edu yes no Micromasters production micromasters.mit.edu yes no OCW Production (Fastly) ocw.mit.edu yes no OCW production CMS 1 ocwcms.mit.edu yes no OCW production CMS 2 ocw-production-cms-2.odl.mit.edu yes no OCW production origin server ocw-origin.odl.mit.edu yes no ODL Video RC video-rc.odl.mit.edu yes no ODL Video production video.odl.mit.edu yes no Open Discussions production open.mit.edu yes no xPro CMS RC studio-rc.xpro.mit.edu/heartbeat yes no xPro CMS production studio.xpro.mit.edu/heartbeat yes no xPro LMS RC courses-rc.xpro.mit.edu/heartbeat yes no xPro LMS production courses.xpro.mit.edu/heartbeat yes no xPro RC xpro-rc.odl.mit.edu yes no xPro preview RC preview-rc.xpro.mit.edu/heartbeat yes no xPro preview production preview.xpro.mit.edu/heartbeat yes no xPro production xpro.mit.edu yes no"},{"location":"monitoring_observability/key_labels/","title":"Key Labels used by ODL","text":"<ol> <li>environment : The 'tier'. For example, dev, QA, Production.</li> <li>service : The product provided. For example, 'MITx', 'Residential', 'OCW'</li> <li>application : The application within the service, for example, edxapp, django, consul</li> </ol> <p>There may be multiple instances (environments) of a service. A service is comprised of applications.</p>"},{"location":"monitoring_observability/key_labels/#environments","title":"Environments","text":"<ul> <li>ci</li> <li>production</li> <li>qa</li> </ul>"},{"location":"monitoring_observability/key_labels/#services","title":"Services","text":"<ul> <li>bootcamps</li> <li>micromasters</li> <li>mitx-online</li> <li>mitx-residential</li> <li>ocw</li> <li>open</li> <li>xpro</li> </ul>"},{"location":"monitoring_observability/key_labels/#applications","title":"Applications","text":"<ul> <li>concourse</li> <li>consul</li> <li>django</li> <li>edxapp</li> <li>elasticsearch</li> <li>heroku-app</li> <li>nginx</li> <li>redis</li> <li>vault</li> </ul>"},{"location":"monitoring_observability/overview/","title":"Monitoring Overview","text":"<p>We primarily use Vector for collecting both metrics and logs from our applications. Vector is pretty flexible and offers <code>sources</code> and <code>sinks</code> to/from many different systems.</p>"},{"location":"monitoring_observability/overview/#metrics","title":"Metrics","text":"<p>Ultimately we utilize Prometheus, but our stack is a little non-traditional.</p> <p>For our applications in EC2, we use Vector as a middleman to perform the 'prometheus-scrape' action, rather than performing the scrape with an actual prometheus instance that we would have to run. Right at scrape time, on the node performing the scrape, we can perform <code>transforms</code> on the data to drop timeseries that we are not interested in or add labels + metadata that would be helpful. This is good, because we are charged by the timeseries in GrafanaCloud and many prometheus endpoints produce lots of not-interesting data. The more we can drop early, the better. Once it is in GrafanaCloud, we pay for it.</p> <p>After the transforms, Vector ships the data straight to GrafanaCloud where it is stored in Cortex, which is actually not prometheus but it is API compatible and the distinction isn't important to us.</p>"},{"location":"monitoring_observability/overview/#logs","title":"Logs","text":"<p>For log collection and aggregation, we again are using Vector and GrafanaCloud. Vector is also very good at collecting and parsing logfiles and again we can utilize its ability to <code>transform</code> the data to drop not-interesting data and save a little money.</p> <p>After any transforms on the log data are completed, Vector ships the data straight to GrafanaCloud again and it is stored in Loki. Loki is like a hybrid of ElasticSearch and Prometheus and it has its own query language.</p>"},{"location":"monitoring_observability/overview/#vector-patterns","title":"Vector Patterns","text":"<p>There are a couple different patterns at play in our environment.</p>"},{"location":"monitoring_observability/overview/#applications-deployed-into-ec2","title":"Applications deployed into EC2","text":"<p>The statements below do not necessarily apply to EC2 instances managed with salt-stack</p> <p>For applications deployed in EC2, we install and configure Vector at AMI build time. We pull in global secrets (values that are the same regardless of where the AMI is running) at build time. Other secrets are interpolated runtime secrets using environment variables populated via <code>/etc/default/</code> files and vault-template.</p> <p>Examples: - Config files laid down at AMI build time here - <code>/etc/default</code> secrets from runtime here</p>"},{"location":"monitoring_observability/overview/#applications-deployed-in-heroku","title":"Applications deployed in Heroku","text":"<p>Specifics of collecting logs out of Heroku are covered here</p>"},{"location":"monitoring_observability/overview/#applications-deployed-into-ecs","title":"Applications deployed into ECS","text":"<p>Coming soon!</p>"},{"location":"platform_services/ci_cd/concourse_github_issues_user_guide/","title":"Concourse github Issues Workflow User Guide","text":""},{"location":"platform_services/ci_cd/concourse_github_issues_user_guide/#the-problem","title":"The Problem","text":"<p>Here at MIT OL we use Concourse CI. It's an incredibly powerful package for managing complex continuous integration workflows, and we leverage its power in all kinds of interesting ways.</p> <p>What that means for you, the developer however is that our pipelines can be difficult to get one's head around and understand. As an example, this is our edx platform meta-pipeline.</p>"},{"location":"platform_services/ci_cd/concourse_github_issues_user_guide/#the-ask","title":"The Ask","text":"<p>Most of the time, there are exactly two questions software developers wants answered when it comes to deploying their software:</p> <ol> <li>How can I tell when my code has been deployed to $X?</li> <li>How can I trigger my code to be deployed to $X?</li> </ol>"},{"location":"platform_services/ci_cd/concourse_github_issues_user_guide/#the-solution","title":"The Solution","text":"<p>Thankfully, our director Tobias came up with an excellent and novel solution. What's one of the most common non code mechanism developers use to govern their work?</p> <p>Github Issues!</p>"},{"location":"platform_services/ci_cd/concourse_github_issues_user_guide/#a-traffic-light-for-deploys","title":"A Traffic Light For Deploys","text":"<p>Now that you've indulged me with a full page of setup. Let's get down to brass tacks and answer those two most common questions devs have about their deploys:</p> <ol> <li>How can I tell when my code has been deployed to $X?</li> </ol> <p>We're currently keeping the Github Issues that govern our pipelines on MIT's internal Github, because the public one has throttles that were shooting us in the foot :)</p> <p>So, take a look at the issues for the concourse-workflow repo.</p> <p>Right now, that page looks something like this (simplified view):</p> <pre><code>[bot] Pulumi ol-infrastructure-vault-encryption_mounts substructure.vault.encryption_mounts.operations.Production deployed. DevOps finalized-deployment pipeline-workflow product:infrastructure\n#1427 opened yesterday by tmacey\n[bot] Pulumi ol-infrastructure-forum-server applications.forum.xpro.Production deployed. DevOps finalized-deployment pipeline-workflow product:infrastructure\n#1426 opened yesterday by tmacey\n[bot] Pulumi ol-infrastructure-dagster-server applications.dagster.QA deployed. DevOps pipeline-workflow product:infrastructure promotion-to-production\n#1425 opened yesterday by tmacey\n</code></pre> <p>Let's say I'm someone on the data platform team and I'm wondering whether or not my changes have been deployed in the Dagster project. Aha! I look down the list and see that the Dagster project has been deployed to QA, but not to production.</p> <p>Let's click on that issue and take a look.</p> <p>At the time of this writing, this issue is Open, which means concourse Github issues is waiting for us to tell it that we're ready to deploy these changes to production. We can see everything this deployment would contain because for each Concourse build that's happened since this issue was created, a comment was added along with the build log.</p> <p>If we're ready to move these changes to production, we just Close this issue. That's all there is to it!</p> <p>If you want to watch your change's progress, just click one of those build log links which will bring you to the pipeline where that's happening. You should see a new build in progress. Just click that tab to see the details.</p> <p>And that's all there is to know! It's a simple system for busy people.</p> <p>If you have any questions, don't hesitate to reach out to @sre on slack or send us E-mail at oldevops@mit.edu</p> <p>Thanks for taking the time to read this doc! Obviously feel free to suggest any improvements or let me know if anything's unclear.</p>"},{"location":"platform_services/ci_cd/deploying_concourse/","title":"Deploying Concourse","text":""},{"location":"platform_services/ci_cd/deploying_concourse/#summary","title":"Summary","text":"<p>This document will detail the best practice we use to develop and deploy changes to our Concourse pipeline web and worker servers.</p>"},{"location":"platform_services/ci_cd/deploying_concourse/#developing","title":"Developing","text":"<p>As with most projects here at MIT OL's Devops team, you'll want to start by checking out the ol-infrastructure project to your local workspace.</p> <p>As usual, be sure to run <code>poetry install</code> so all the right dependencies will be cached and ready to run your newly changed pyinfra code.</p> <p>Now change directory to the src/bilder/images/concourse folder. Here you'll see a number of files. The most important for our purposes is deploy.py.</p> <p>Very likely, any chances you might want to make will be in this file.</p>"},{"location":"platform_services/ci_cd/deploying_concourse/#local-testing","title":"Local Testing","text":"<p>Apparently I'm alone on this bus, but I personally enjoy testing changes on my machine before I commit them to Git. The testing doesn't have to be deep, something like syntax and being end to end runnable are good enough for me.</p> <p>In any case, in order to locally build the docker container, run the following command:</p> <p><code>pyinfra @docker/debian:bookworm deploy.py</code></p> <p>NOTA BENE: I am not entirely sure about the Debian Bookworm base container I'm using here, it worked well enough for my very shallow testing purposes.</p> <p>TODO: Figure out what we actually use for a base container and cite that in this doc.</p> <p>If you have a syntax error or an error in your pyinfra code that prevents the container from building, you will see that on your screen and can debug it accordingly.  Otherwise, you'll see a message about the container building successfully.</p>"},{"location":"platform_services/ci_cd/deploying_concourse/#build-the-image","title":"Build The Image","text":"<p>Now that we've very shallowly 'smoke tested' our code, let's run the actual official script to build the AMI with packer and stage it for later deployment.</p> <p>From the same directory, run the following command:</p> <p><code>pr packer build -on-error=ask -var node_type=web -var app_name=concourse -only amazon-ebs.third-party src/bilder/images</code></p> <p>You will see a LOT of output and the process could take up to 20 minutes.</p>"},{"location":"platform_services/ci_cd/deploying_concourse/#initial-testing-in-ci","title":"Initial Testing in CI","text":"<p>For most of the rest of the journey to production, you'll be using a pipeline rather than doing it by hand with Pulumi, but for your first time, you'll probably want to do it by hand and then minutely monitor the resultant deployed image in CI to ensure everything went smoothly.</p> <p>TODO: Add curl smoke test for web node</p>"},{"location":"platform_services/ci_cd/deploying_concourse/#deployment-to-qa-and-beyond","title":"Deployment to QA and Beyond","text":"<p>For the remainder of your change's journey through QA and ultimately to production, you'll be using the Concourse pipeline.</p> <p>TODO: Add further testing details</p>"},{"location":"platform_services/ci_cd/getting_started_with_concourse/","title":"Getting Started Building Concourse Pipelines in Python","text":""},{"location":"platform_services/ci_cd/getting_started_with_concourse/#getting-set-up","title":"Getting Set Up","text":"<ul> <li>Use the Tutorial on the Concourse website under Docs-&gt;Getting Started to get set up.</li> <li>Do NOT use any other tutorials you find linked elsewhere. Some are out of date and the failure modes can be incredibly hard to debug!</li> <li>If you're using an Arm64 Mac (e.g. M1, M2, etc.) just substitute this platform specific image for the one cited in the tutorial. So the first line of your docker-compose.yml should look something like this: <code>image: rdclda/concourse:7.7.1</code></li> <li>Nota Bene: You should probably use the latest version of this. At the time of this writing the latest has a bug but that will likely be fixed by the time anyone reads this.</li> <li>You'll also need to install Docker Desktop for Mac.</li> <li>Be sure that at the end of this process you have a Concourse instance running locally in your Docker install. If you     didn't end up running <code>docker-compose up -d</code> or similar then you may have missed something. Watch for errors and check that your newly launched Concourse is healthy and that you can browse to the localhost URL cited in the tutorial which should get you the Concourse main page with the giant spinning turbine blade icon.</li> </ul>"},{"location":"platform_services/ci_cd/getting_started_with_concourse/#now-were-cooking-with-gas-learning-how-concourse-works-by-building-pipelines","title":"Now We're Cooking with Gas! Learning How Concourse Works By Building Pipelines","text":"<ul> <li>Work through the entire tutorial including building and actually creating pipelines from all the examples. This is critical so you'll have an understanding of what all the component parts are and how they fit together as you code your Python in later steps.</li> <li>Actually make a point of working with some of the later examples involving Git repos. Commit some chnges to your test repo and watch them flow through the pipeline. Pretty neat eh?</li> <li>You may notice that some of the later examples get pretty unwieldy and become difficult to get right. It can be tricky figuring out what level of indentation is correct just by eyeballing it. You might find the yamllint utility helpful for this as it will tell you when there are syntax errors. Ignore its whining about improper indent levels and focus on the errors :)</li> </ul>"},{"location":"platform_services/ci_cd/getting_started_with_concourse/#it-gets-easier-building-pipelines-in-python","title":"It Gets Easier - Building Pipelines in Python","text":"<p>Thankfully, we have been spared the pain of coding pipelines in YAML by virtue of a Python wrapper that Tobias Macey wrote.</p> <p>Each YAML section is wrapped in a Python object. It's a 1 to 1 mapping because the Python models are auto-generated from the schema defined by Concourse.</p> <p>Tahe a look at the simplest hello-world tutorial example converted into Python here. I've put the explanatory comments inline to make it easier to understand what's going on.</p>"},{"location":"platform_services/ci_cd/getting_started_with_concourse/#actually-building-a-pipeline-from-your-python","title":"Actually Building a Pipeline From Your Python","text":"<p>Our Concourse pipeline Python build scripts, like everything else we maintain, is managed by Poetry.</p> <p>So, to actually invoke our hello world script and get ready to actually create the pipeline, we run: <code>poetry run python3 ./hello.py</code></p> <p>This will emit the JSON our Python produces, along with an actual Concourse <code>fly</code> invocation at the end. This assumes you're already properly logged into your Concourse instance. For our purposes use the same one you set up previously to run through the tutorials.</p> <p>The output should look something like this: <pre><code>{\n  \"jobs\": [\n    {\n      \"build_logs_to_retain\": null,\n      \"max_in_flight\": 1.0,\n      \"serial\": null,\n      \"old_name\": null,\n      \"on_success\": null,\n      \"ensure\": null,\n      \"on_error\": null,\n      \"disable_manual_trigger\": null,\n      \"serial_groups\": null,\n      \"build_log_retention\": null,\n      \"name\": \"deploy-hello-world\",\n      \"plan\": [\n        {\n          \"config\": {\n            \"image_resource\": {\n              \"source\": {\n                \"repository\": \"busybox\",\n                \"tag\": \"latest\"\n              },\n              \"params\": null,\n              \"version\": null,\n              \"type\": \"registry-image\"\n            },\n            \"caches\": null,\n            \"run\": {\n              \"args\": [\n                \"Hello, World!\"\n              ],\n              \"user\": null,\n              \"path\": \"echo\",\n              \"dir\": null\n            },\n            \"inputs\": null,\n            \"platform\": \"linux\",\n            \"params\": null,\n            \"container_limits\": null,\n            \"outputs\": null,\n            \"rootfs_uri\": null\n          },\n          \"file\": null,\n          \"params\": null,\n          \"task\": \"hello-task\",\n          \"privileged\": null,\n          \"vars\": null,\n          \"output_mapping\": null,\n          \"image\": null,\n          \"input_mapping\": null,\n          \"container_limits\": null\n        }\n      ],\n      \"interruptible\": null,\n      \"public\": null,\n      \"on_failure\": null,\n      \"on_abort\": null\n    }\n  ],\n  \"groups\": null,\n  \"resource_types\": null,\n  \"var_sources\": null,\n  \"display\": null,\n  \"resources\": null\n}\n</code></pre> <pre><code>fly -t pr-inf sp -p misc-cloud-hello -c definition.json\n</code></pre></p> <p>Since we'll be using fly to create our pipeline in the tutorial Concourse instance, use -t tutorial rather than pr-inf and make any other necessary substitutions according to your environment.</p> <p>That's it! You should now have the basic Hello World pipeline created from your Python source and operating properly, printing that famous phrase as a result.</p> <p>TODO Add a more meaty example like the the-artifact example so we get to show inputs and outputs.</p>"},{"location":"platform_services/cloud_infrastructure/import_existing_resources_with_pulumi/","title":"Importing Resources That Already Exist Into Your Pulumi Stack","text":""},{"location":"platform_services/cloud_infrastructure/import_existing_resources_with_pulumi/#summary","title":"Summary","text":"<p>There are several ways to skin this particular cat but Tobias has shown me one that works really well so that's what we'll outline here.</p> <p>There's another approach using the pulumi import CLI command but in my experience that brings in a bunch of extra attributes we don't want. (There may be ways to tune this, I just don't know them.)</p>"},{"location":"platform_services/cloud_infrastructure/import_existing_resources_with_pulumi/#whole-cloth","title":"Whole Cloth","text":"<p>Before you start importing, code your resources the same way you always would.</p> <p>For example, if you need an S3 bucket, use all the usual Pulumi code - s3.Bucket etc.</p> <p>You want to code such that in a disaster recovery scenario, if we were staring from scratch, the resources would get build 100% correctly and the applications they support would pass all monitoring checks and smoke tests and function normally.</p>"},{"location":"platform_services/cloud_infrastructure/import_existing_resources_with_pulumi/#bring-on-the-special-import-sauce","title":"Bring On The Special (Import) Sauce","text":"<p>If you know you want to keep existing resources while having Pulumi create the rest, you should tell it to import these resources by passing in a ResourceOptions object at resource creation time. Here's an example of our S3 bucket:</p> <pre><code>bootcamps_storage_bucket_name = f\"ol-bootcamps-app-{stack_info.env_suffix}\"\nbootcamps_storage_bucket = s3.Bucket(\n    f\"ol-bootcamps-app-{stack_info.env_suffix}\",\n    ### ****THIS LINE IS THE IMPORT CLAUSE****\n    opts=ResourceOptions(import_=bootcamps_storage_bucket_name,ignore_changes=[]),\n    bucket=bootcamps_storage_bucket_name,\n    # ...\n</code></pre> <p>Note that obviously the object you're creating will differ if it's not an S3 bucket, for example a Vault mount point.</p>"},{"location":"platform_services/cloud_infrastructure/import_existing_resources_with_pulumi/#tuning","title":"Tuning","text":"<p>After having added the above code, go ahead and run pulumi up on your stack. At this stage DO NOT SAY YES to finalizing these changes if there are any diagnostic warnigns displayed.</p> <p>Here's an example of something like what we'd expect:</p> <pre><code># cpatti @ rocinante in ~/src/mit/ol-infrastructure/src/ol_infrastructure/applications/bootcamps on git:cpatti_pulumi_bootcamp x ol-infrastructure-yqmQEgvq-py3.11 [14:31:28]\n$ pulumi up  -s applications.bootcamps_ecommerce.Production\nPreviewing update (applications.bootcamps_ecommerce.Production):\n     Type                                                      Name                                                                                           Plan       Info\n +   pulumi:pulumi:Stack                                       ol-infrastructure-bootcamps-ecommerce-application-applications.bootcamps_ecommerce.Production  create\n +   \u251c\u2500 ol:infrastructure:aws:database:OLAmazonDB              bootcamps-db-applications-production                                                           create\n +   \u2502  \u251c\u2500 aws:rds:ParameterGroup                              bootcamps-db-applications-production-postgres-parameter-group                                  create\n +   \u2502  \u251c\u2500 aws:rds:Instance                                    bootcamps-db-applications-production-postgres-instance                                         create\n +   \u2502  \u2514\u2500 aws:rds:Instance                                    bootcamps-db-applications-production-postgres-replica                                          create\n +   \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS  bootcamps-db-applications-production-CPUUtilization-OLCloudWatchAlarmSimpleRDSConfig           create\n +   \u2502  \u2514\u2500 aws:cloudwatch:MetricAlarm                          bootcamps-db-applications-production-CPUUtilization-simple-rds-alarm                           create\n +   \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS  bootcamps-db-applications-production-WriteLatency-OLCloudWatchAlarmSimpleRDSConfig             create\n +   \u2502  \u2514\u2500 aws:cloudwatch:MetricAlarm                          bootcamps-db-applications-production-WriteLatency-simple-rds-alarm                             create\n +   \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS  bootcamps-db-applications-production-FreeStorageSpace-OLCloudWatchAlarmSimpleRDSConfig         create\n +   \u2502  \u2514\u2500 aws:cloudwatch:MetricAlarm                          bootcamps-db-applications-production-FreeStorageSpace-simple-rds-alarm                         create\n +   \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS  bootcamps-db-applications-production-EBSIOBlance-OLCloudWatchAlarmSimpleRDSConfig              create\n +   \u2502  \u2514\u2500 aws:cloudwatch:MetricAlarm                          bootcamps-db-applications-production-EBSIOBalance%-simple-rds-alarm                            create\n +   \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS  bootcamps-db-applications-production-DiskQueueDepth-OLCloudWatchAlarmSimpleRDSConfig           create\n +   \u2502  \u2514\u2500 aws:cloudwatch:MetricAlarm                          bootcamps-db-applications-production-DiskQueueDepth-simple-rds-alarm                           create\n +   \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS  bootcamps-db-applications-production-ReadLatency-OLCloudWatchAlarmSimpleRDSConfig              create\n +   \u2502  \u2514\u2500 aws:cloudwatch:MetricAlarm                          bootcamps-db-applications-production-ReadLatency-simple-rds-alarm                              create\n +   \u251c\u2500 ol:services:Vault:DatabaseBackend:postgresql           bootcamps                                                                                      create\n +   \u2502  \u2514\u2500 vault:index:Mount                                   bootcamps-mount-point                                                                          create\n +   \u2502     \u2514\u2500 vault:database:SecretBackendConnection           bootcamps-database-connection                                                                  create\n +   \u2502        \u251c\u2500 vault:database:SecretBackendRole              bootcamps-database-role-approle                                                                create\n +   \u2502        \u251c\u2500 vault:database:SecretBackendRole              bootcamps-database-role-admin                                                                  create\n +   \u2502        \u251c\u2500 vault:database:SecretBackendRole              bootcamps-database-role-readonly                                                               create\n +   \u2502        \u2514\u2500 vault:database:SecretBackendRole              bootcamps-database-role-app                                                                    create\n +   \u251c\u2500 pulumi:providers:vault                                 vault-provider                                                                                 create\n +   \u251c\u2500 aws:iam:Policy                                         bootcamps-production-policy                                                                    create\n =   \u251c\u2500 aws:s3:Bucket                                          ol-bootcamps-app-production                                                                    import     [diff: -tagsAll~tags]; 1 warning\n =   \u251c\u2500 vault:index:Mount                                      bootcamps-vault-secrets-storage                                                                import\n +   \u251c\u2500 aws:ec2:SecurityGroup                                  bootcamps-db-access-production                                                                 create\n +   \u2514\u2500 vault:aws:SecretBackendRole                            bootcamps-app-production                                                                       create\n\n\nDiagnostics:\n  aws:s3:Bucket (ol-bootcamps-app-production):\n    warning: inputs to import do not match the existing resource; importing this resource will fail\n\nOutputs:\n    bootcamps_app: {\n        rds_host: output&lt;string&gt;\n    }\n</code></pre> <p>The two key bits of output to focus on here are the diagnostic warning towards the end:</p> <pre><code>Diagnostics:\n  aws:s3:Bucket (ol-bootcamps-app-production):\n    warning: inputs to import do not match the existing resource; importing this resource will fail\n</code></pre> <p>This tells us that Pulumi has detected a critical difference between the resource's state in the real world and Pulumi's model of what's there and what needs to change to arrive at the desired state.</p> <p>The next most important bit is nestled amongst Pulumi telling us what changes it plans to make:</p> <p><code>=   \u251c\u2500 aws:s3:Bucket                                          ol-bootcamps-app-production                                                                    import     [diff: -tagsAll~tags]; 1 warning</code></p> <p>This tells us that an attribute, in this case the tags that we're specifying the S3 bucket should have in our Pulumi source disagrees with what's actually sitting out there on EC2.</p> <p>In order to fix this discrepancy, we go back to our ResourceOptions line we added, adding the tags attribute into it to tell Pulumi to leave the current tags alone and not complain that they differ:</p> <p><code>opts=ResourceOptions(import_=bootcamps_storage_bucket_name,ignore_changes=[\"policy\",\"tags\"]),</code></p> <p>You'll also note that \"policy\" is in that attributes list. That's needed because Pulumi signalled a mismatch on a previous run.</p> <p>After these additions, your pulumi up should succeed and all the resources should be created with no further warnings.</p>"},{"location":"platform_services/cloud_infrastructure/import_existing_resources_with_pulumi/#cleaning-up","title":"Cleaning Up","text":"<p>After you've successfully built your environment with Pulumi and imported the existing resources, you'll want to remove all those ResourceOptions lines from your Pulumi model source as the import should only be done once.</p>"},{"location":"platform_services/cloud_infrastructure/moira_certificates/","title":"Moira Certificates","text":"<p>There are two services in our porfolio that interact with Moira.</p> <ol> <li>MIT Open / OpenDiscussions</li> <li>ODL-Video-Service</li> </ol> <p>Both of these services authenticate against Moira with certificates issued by ca.mit.edu which is also the provider of MIT Personal Certificates. There is no web interface for requesting an application certificate from ca.mit.edu, so you need to email mitcert@mit.edu with the CSR in the body of the email and a clear request that you're asking for a certificate issued from ca.mit.edu and NOT InCommon/Internet2 which is where most MIT certificates now come from.</p> <p>Both of these applications utilize the same two environment variables for storing and accessing this key/cert pair.</p> <ul> <li>MIT_WS_CERTIFICATE</li> <li>MIT_WS_PRIVATE_KEY</li> </ul>"},{"location":"platform_services/cloud_infrastructure/moira_certificates/#generating-a-csr-from-the-mitws-private-key","title":"Generating a CSR From the MITWS Private Key","text":"<p>This is the OpenSSL invocation and details I used to generate the CSR for the OVS cert:</p> <pre><code>\u256d\u2500feoh at prometheus in ~/Packages/openssl 25-04-07 - 15:22:04\n\u2570\u2500\u25cb openssl req -out app.csr -key mitws.key -new -sha256                                                                                                                                          &lt;region:us-east-1&gt;\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:US\nState or Province Name (full name) [Some-State]:Massachusetts\nLocality Name (eg, city) []:Cambridge\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:MIT\nOrganizational Unit Name (eg, section) []:odl-video\nCommon Name (e.g. server FQDN or YOUR name) []:video.odl.mit.edu\nEmail Address []:odl-devops@mit.edu\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:\nAn optional company name []:\n</code></pre>"},{"location":"platform_services/cloud_infrastructure/moira_certificates/#mitopen-vault-locations","title":"MITOpen Vault Locations","text":"<ul> <li>In all vault environments: <code>secret-mit-open/global/mit-application-certificate</code></li> <li>Maintained by hand.</li> </ul>"},{"location":"platform_services/cloud_infrastructure/moira_certificates/#odl-video-service-vault-location","title":"ODL Video Service Vault Location","text":"<ul> <li>In all vault environments: <code>secret-odl-video-service/ovs/secrets</code></li> <li>Inside a single JSON structure at <code>misc.mit_ws_certificate</code> and <code>misc.mit_ws_private_key</code></li> <li>Maintained automatically by pulumi.</li> <li><code>sr/bridge/secrets/odl_video_service</code> or here</li> </ul>"},{"location":"platform_services/cloud_infrastructure/moira_certificates/#certificate-usage-and-expiration-tracking","title":"Certificate Usage and Expiration Tracking","text":"Action Date Application Description Who 20230625 Open Replaced MIT Open certificate, expires 20240625 MD 20230928 OVS OVS Cert expired, replaced with Open certificate above, expires 20240625 MD 20240201 Both No action. Verified certificates currently in use. Updated reminder in team calendar. MD 20240613 Both Replaced both certificates with 2024-2025 versions. Reminder sent to team calendar. MD 20250417 OVS Replaced OVS certificate. Expires 20260415. IS&amp;T INC: INC1537323 CP"},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/","title":"How to Publish an MIT OL Package to NPM","text":"<p>Note: This is a VERY rough document. We hate the current process, but I'm a big believer in not allowing the perfect to be the enemy of the good :)</p>"},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>You will need Vault access for production.</li> <li>You'll need to know which Github repository needs publhsing</li> </ul>"},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#setup","title":"Setup","text":""},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#git","title":"Git","text":"<p>Check out the Git repository that equates to the NPM module that needs publishing. Usually the dev asking for help can give you this if it's not obvious.</p>"},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#npm-login","title":"npm login","text":""},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#fun-with-vaults-web-ui","title":"Fun with Vault's Web UI","text":"<p>Now login to npm from the command line. You'll need to have npm and node installed in order for this to work. If you're on a Mac, you can use homebrew <code>brew install npm</code> but YMMV.</p> <p>You'll need the username and TOTP code we use for this. Get from the production Vault instance.</p> <p>Navigate to \"platform-secrets\" and look for the 'npmjs' entry.</p> <p>This will get you the username and password. Now you'll need the TOTP code.</p> <p>Click the eye icon to reveal the contents of the 'totp-path-mitx-devops' entry.</p> <p>Now click the icon to the left of the eye to copy the command you'll need to run to your local clipboard.</p> <p>Open the CLI by clicking on the little black box with the '&gt;' in it in the very upper right of the screen. Now paste the contents if your clipboard into the bottom section of the screen where you can enter commands. This will get you the TOTP code you'll need.</p>"},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#to-the-cli","title":"To the CLI!","text":"<p>To login to npm on the command line and complete setup, run: <code>npm login mitx-devops</code> and hit return.</p> <p>This will prompt to open a web page. You'll need to supply the password and TOTP code you got in the previous step. If it's been too long since you gathered the TOTP code, you may need to hit up-arrow in the Vault web UI CLI and get a fresh TOTP code.</p> <p>If you're successful you should see something like: <code>Logged in on https://registry.npmjs.org/.</code></p>"},{"location":"platform_services/cloud_infrastructure/publishing_to_npm/#doing-the-actual-publishing","title":"Doing The Actual Publishing","text":"<p>Now change directory to the Git repo you checked our earlier and type:</p> <p><code>npm publish</code>.</p> <p>If it blows up, check the <code>packages.json</code> file and ensure the the organization is set correctly. It should be 'mitodl'. See this commit for an example fix.</p>"},{"location":"platform_services/cloud_infrastructure/pulumi_aws_classic_5_to_6_upgrade/","title":"Pulumi AWS Classic 4 To 5 Upgrade","text":"<p>So, this is roughly what I believe is happening. So you guys can know too.</p> <ol> <li>I updated / tested a bunch of stacks, mostly CI or sometimes QA, yesterday with the <code>pulumi-aws 6.5.0</code> upgrade. That is a two part thing. Python package but also provider.</li> <li>Part of that upgrade, pulumi changed some of the state/stack structures and they are no longer backwards compatible, in particular those around RDS instances.</li> <li>So I, by hand, upgraded a bunch of stacks to the new provider, they migrate nicely everything is good.</li> <li>Concourse comes around and something triggers it and it applies the stacks again but with the old provider. It works okay the first time, but when the second time comes around it decides it has lost track of the RDS instance and it tries to recreate it.</li> <li>If it DOES try to recreate it, thankfully it fails, because the RDS instance is still there, just pulumi has lost it, and a duplicate error gets thrown and the apply/up fails.</li> <li>But, now the stack is broken and requires manual intervention to revive it.<ul> <li>So what I do is I go into the history/checkpoint area in S3 and find the most recent checkpoint that references the 6.5.0 provider (the one I upgraded to). I pull that down.</li> <li>Checkpoints can\u2019t be imported directly they need to be fixed. Fix it with this: https://gist.github.com/clstokes/977b7bd00b37e0a564f707f0ebe36e08</li> <li><code>pr pulumi stack import -s &lt;stackname&gt; --file &lt;fixed stack checkpoint file&gt;</code></li> <li><code>poetry install</code> using an environment pre-6.5.0 upgrade if necessary (I keep multiple ol-inf envs for exactly this kind of thing so I just switch between my 5.4 and 6.5 envs).</li> <li><code>pulumi plugin rm resource aws 6.5.0</code> uninstall the PROVIDER</li> <li><code>pr pulumi up --refresh -s &lt;stackname&gt;</code> x2 \u2014 Shouldn\u2019t be trying to create a database anymore.</li> </ul> </li> <li>Many of the stacks were fine because they either:<ul> <li>didn\u2019t have RDS resources</li> <li>didn\u2019t get up\u2019d by concourse inbetween.</li> </ul> </li> <li>BUT! BUT! When the PR making this upgrade was merged to <code>main</code> , it trigged nearly everything. Very sad. :disappointed:<ul> <li>But the code has been merged so that is fine right? Wrong.</li> <li>The resources that concourse uses to do the needful are not upgraded yet. Specifically these:<ul> <li>https://hub.docker.com/r/mitodl/concourse-pulumi-resource-provisioner</li> <li>https://hub.docker.com/r/mitodl/concourse-pulumi-resource</li> <li>https://hub.docker.com/r/mitodl/ol-infrastructure</li> </ul> </li> <li>These actually have a complicated silent depedency between them, but basically <code>mitodl/ol-infrastructure</code> is a base layer for the other two.</li> <li>These builds also kicked off (maybe? I did run them by hand too\u2026) when the PR was merged but they didn\u2019t publish to dockerhub before concourse started trying to update CI/QA stacks automatically.<ul> <li>EVEN IF they had published to dockerhub before, it wouldn\u2019t matter because servers-be-cachin\u2019.</li> <li>The concourse workers were holding on to their old versions and efficiently re-using them, ignorant of the new versions available out on dockerhub.</li> <li>Solve this by doing and instance refresh on concourse workers. New servers do new <code>docker pull</code> on the resources needed.</li> </ul> </li> </ul> </li> <li>Addendum: Made a neat script to pause concourse pipelines en-mass. Should have run this before #8 :disappointed: <pre><code>for pl in $(fly -t pr-inf ps --json | jq -r '.[].name'); do\n  fly -t pr-inf pp -p $pl\ndone\n</code></pre></li> </ol>"},{"location":"platform_services/cloud_infrastructure/recaptcha/","title":"reCAPTCHA Errors","text":"<pre><code>ERROR for site owner: Invalid domain for site key.\n</code></pre> <ol> <li>Need to login to google with the mitx devops username and password (vault-production -&gt; <code>platform-secrets/google</code>). It will need to do an email verification code to one of our lists if you don't have an active session.</li> <li>Make your way to the reCAPTCHA console located here.</li> <li>There is a dropdown on the top left that lets you see which site/application configuration that you're working with.</li> <li>Once you're on the site that you care about, there is a gear icon on the top right. Click that for the settings.</li> <li>Three things to verify:<ol> <li>Does the site key match what is listed in app configuration + vault?</li> <li>Does the secret key match what is listed in app configuration + vault?</li> <li>Is the list of domains correct?</li> <li>NOTE: When checking keys, look at the end of the string rather than the start. They all seem to start the same.</li> </ol> </li> </ol>"},{"location":"platform_services/cloud_infrastructure/tagging_amis_with_installed_software_metadata/","title":"Tagging AMIs with Installed Software Metadata","text":""},{"location":"platform_services/cloud_infrastructure/tagging_amis_with_installed_software_metadata/#background","title":"Background","text":"<p>We are looking to start a new pattern where each AMI we produce is tagged with metadata about the software installed on it. This included 3rd party software such as Hashicorp products as well as information our applications. Recently we've been asked 'What version of X is running in production right now?\" and it was a surprisingly difficult question to answer. The idea behind this new pattern is to change that, by providing the required information simply by inspecting the AMI behind any running EC2 instance.</p>"},{"location":"platform_services/cloud_infrastructure/tagging_amis_with_installed_software_metadata/#implementation","title":"Implementation","text":"<p>We use pyinfra to do most of the operations involved with creating a new AMI and this is no exception.</p> <p>Firstly, during the new build we will create a file on the build instance at <code>/etc/ami_tags.json</code> which contains our tag keys and values.</p> <p><pre><code>from bilder.lib.ami_helpers import build_tags_document\ntags_json = json.dumps(\n    build_tags_document(\n        source_tags={\n            \"consul_version\": VERSIONS[\"consul\"],\n            \"consul_template_version\": VERSIONS[\"consul-template\"],\n            \"vault_version\": VERSIONS[\"vault\"],\n            \"docker_repo\": DOCKER_REPO_NAME,\n            \"docker_digest\": DOCKER_IMAGE_DIGEST,\n            \"edxapp_repo\": edx_platform.git_origin,\n            \"edxapp_branch\": edx_platform.release_branch,\n            \"edxapp_sha\": edx_platform_sha,\n            \"theme_repo\": theme.git_origin,\n            \"theme_branch\": theme.release_branch,\n            \"theme_sha\": theme_sha,\n        }\n    )\n)\nfiles.put(\n    name=\"Place the tags document at /etc/ami_tags.json\",\n    src=io.StringIO(tags_json),\n    dest=\"/etc/ami_tags.json\",\n    mode=\"0644\",\n    user=\"root\",\n)\n</code></pre> This file persists as part of the AMI and will exist on any instances spawned from the image.</p> <p>Next, we need to add three steps to our packer <code>build</code> stanza.</p> <p>First, we need to retrieve the file we just created remotely in the pyinfra code. We use the same SSH information that we utilized when we ran <code>pyinfra</code>. This needs to be a <code>provisioner</code> step because the build instance still needs to be running in order to copy a file from it.</p> <p><pre><code>provisioner \"shell-local\" {\n  inline = [\"scp -o StrictHostKeyChecking=no -i /tmp/packer-${build.ID}.pem ${build.User}@${build.Host}:/etc/ami_tags.json /tmp/ami_tags-${build.ID}.json\"]\n}\n</code></pre> Second, we create a <code>post-processor</code> that generates a <code>packer manifest</code> for the build. This is just a json file local to the machine running the packer build (not the remote ec2 build instance as before). Because this is the first <code>post-processor</code> and follows the last <code>provisioner</code> step, the remote EC2 instance has been terminated and an AMI has been generated. The manifest will contain the AMI ID which is needed for the next step. <pre><code>post-processor \"manifest\" {\n  output = \"/tmp/packer-build-manifest-${build.ID}.json\"\n}\n</code></pre></p> <p>Finally, we will take the AMI ID out of the <code>packer manifest</code> and combined with the <code>ami_tags.json</code> file we will make a <code>create-tags</code> call on the newly created AMI to add our metadata to it.</p> <pre><code>post-processor \"shell-local\" {\n  inline = [\"AMI_ID=$(jq -r '.builds[-1].artifact_id' /tmp/packer-build-manifest-${build.ID}.json | cut -d \\\":\\\" -f2)\",\n            \"export AWS_DEFAULT_REGION=us-east-1\",\n            \"aws ec2 create-tags --resource $AMI_ID --cli-input-json \\\"$(cat /tmp/ami_tags-${build.ID}.json)\\\"\",\n            \"aws --no-cli-pager ec2 describe-images --image-ids $AMI_ID\"]\n}\n</code></pre>"},{"location":"platform_services/cloud_infrastructure/test_and_deploy_pulumi_packer_projects/","title":"How To Test And Deploy MIT OL Pulumi/Packer Projects","text":"<p>The way we build images here at MIT OL is complicated and involves a number of components with various moving parts, so it can be difficult to understand where to start, what to change, and moreover how to safely test your changes without breaking production. This document will address these issues.</p>"},{"location":"platform_services/cloud_infrastructure/test_and_deploy_pulumi_packer_projects/#the-pipeline","title":"The Pipeline","text":"<p>Each MIT OL project that uses this technique has an accompanying Concourse pipeline that builds the Packer image, then deploys that image to the appropriate stage's AWS EC2 launch profile where new instances will be launched by instance refresh to deploy the new code / configuration into the wild.</p> <p>One such project is Tika. It's a data transformation service used in our data platform. Here is its pipeline.</p>"},{"location":"platform_services/cloud_infrastructure/test_and_deploy_pulumi_packer_projects/#packer-stage","title":"Packer Stage","text":"<p>The first couple stages are reasonable self explanatory:</p> <ul> <li>Validate the packer template for any syntax errors and the like</li> <li>Actually run packer build on the template, invoking pyinfra and packer, producing an AMI.</li> </ul> <p>You can find the pyinfra sources in ol-infrastructure src/bilder/images/. Here is the pyinfra folder for Tika."},{"location":"platform_services/cloud_infrastructure/test_and_deploy_pulumi_packer_projects/#testing","title":"Testing","text":"<p>If you want to test the pyinfra portion locally, you can use the following invocation:</p> <pre><code># cpatti @ rocinante in ~/src/mit/ol-infrastructure/src/bilder/images/tika on git:main o [17:11:31] C:1\n$ pr pyinfra @docker/debian:latest deploy.py\n</code></pre> <p>You should see output similar to the following:</p> <p>TODO: Is there a base image we can use that won't whine about curl/wget and say no hosts remaining?</p> <pre><code>--&gt; Loading config...\n\n--&gt; Loading inventory...\n\n--&gt; Connecting to hosts...\n    [@docker/debian:latest] Connected\n\n--&gt; Preparing Operations...\n    Loading: deploy.py\n    [@docker/debian:latest] Ready: deploy.py\n\n--&gt; Proposed changes:\n    Groups: @docker\n    [@docker/debian:latest]   Operations: 32   Change: 32   No change: 0\n\n\n--&gt; Beginning operation run...\n--&gt; Starting operation: Install Hashicorp Products | Ensure unzip is installed\n    [@docker/debian:latest] Success\n\n--&gt; Starting operation: Install Hashicorp Products | Create system user for vault\n    [@docker/debian:latest] Success\n\n--&gt; Starting operation: Install Hashicorp Products | Download vault archive\n    [@docker/debian:latest] sh: 1: curl: not found\n    [@docker/debian:latest] sh: 1: wget: not found\n    [@docker/debian:latest] Error: executed 0/3 commands\n    [@docker/debian:latest] docker build complete, image ID: cc0b00b971bd\n--&gt; pyinfra error: No hosts remaining!\n</code></pre> <p>When you're satisfied that your pyinfra build will at least build correctly, you can trigger a local packer image build that the CI deployment stage can consume to get your changes into CI. Note that this can take a while, so be prepared for it to chug for 15-20 minutes. Great time to go get a beverage :)</p> <p>You can kick this off with an invocation like the following:</p> <pre><code>poetry run packer build  src/bilder/images/tika/tika.pkr.hcl\n</code></pre> <p>Note that obviously your path will change if the project you're building is different.</p> <p>Note also that you may require a slightly different invocation for different projects. In Tika's case we require a custom Packer template as specified above, but in the case of other projects which use the default Packer template, you might use an invocation like the one we use to build Concourse's image:</p> <pre><code>poetry run packer build -on-error=ask -var node_type=web -var app_name=concourse -only amazon-ebs.third-party src/bilder/images\n</code></pre> <p>Note the node_type and app_name variable declarations above. For Concourse, we can build either a web image or a worker image, so it's important we include node_type in our invocation.</p>"},{"location":"platform_services/cloud_infrastructure/test_and_deploy_pulumi_packer_projects/#deploy-stage","title":"Deploy Stage","text":"<p>The remaining boxes in the pipeline are deployment stages. One per environment stage e.g. CI, QA and Production.</p> <p>Each of these deployment stages basically runs the equivalent of a <code>pulumi up</code> on the Pulumi stack associated with the application in question. Here's the one for Tika.</p> <p>Let's take a look at the output from a build of this stage:</p> <pre><code>INFO:root:@ updating....\n\n8&lt; snip for brevity 8&lt;\n\nINFO:root:    aws:ec2:LaunchTemplate tika-server-tika-ci-launch-template  [diff: ~blockDeviceMappings]\n\nINFO:root:    aws:autoscaling:Group tika-server-tika-ci-auto-scale-group  [diff: ~instanceRefresh,tags,vpcZoneIdentifiers]\n\nINFO:root:@ updating....\n\nINFO:root:    pulumi:pulumi:Stack ol-infrastructure-tika-server-applications.tika.CI\n\nINFO:root:    ol:infrastructure:aws:auto_scale_group:OLAutoScaleGroup tika-server-tika-ci\n\nINFO:root:\n\nINFO:root:Resources:\n\nINFO:root:    17 unchanged\n</code></pre> <p>From this rather verbose blurb we can see that Pulumi updated the ASG and all associated resources like the Launch template. This is the mechanism which seeds the updated image into our EC2 environment where instance refresh safely cycles the old instances out and the ones with our updated code in.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/","title":"Deploying an Application at MIT Open Learning with Kubernetes","text":""},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#assumptions","title":"Assumptions","text":"<ul> <li>You'll be deploying an app that includes a helm chart.</li> <li>You've installed helm and kubectl.</li> <li>You plan to manage your Kubernetes app's infra and deployment with Pulumi.</li> </ul>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#questions","title":"Questions","text":"<ul> <li>Will you need to create a new Kubernetes cluster along with this new application you're deploying? Generally we'd like the answer to be \"no\".</li> </ul>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#structure","title":"Structure","text":""},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#application","title":"Application","text":"<p>All the files and configuration directly pertaining to the app itself. These usually live here.</p> <p>For example our Open Metadata application is here.</p> <p>We'll use Open Metadata QA's example to help guide us on a tour of how the architecture hangs together.</p> <p>This will generally be your home base for this project.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#infrastructure","title":"Infrastructure","text":"<p>All the component infrastructure required to support your application.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#network","title":"Network","text":"<p>General Rules:</p> <ul> <li>There will only be one EKS cluster in any given VPC</li> <li>Pods and EKS nodes share the same address spaces<ul> <li>Pod + node address spaces reside in different availability zones</li> <li>Pod + node address spaces should be positioned \"in the middle of the VPC\"</li> <li>Pod + node address spaces are at least  <code>/21</code>  -&gt; ~2048 addresses per space</li> <li>There are at least 4 pod and node address spaces -&gt; at least ~8192 addresses per cluster</li> </ul> </li> <li>Service Address spaces are arbitrary but CANNOT be a subnet of the VPC.<ul> <li>Service address spaces should not overlap from cluster to cluster</li> <li>Service address spaces jump +60 on the third octet for CI-&gt;QA-&gt;Production</li> <li>Service address spaces are at least <code>/23</code> -&gt; ~512 addresses per space</li> </ul> </li> </ul> VPC CI QA Production Applications Services:10.110.24.0/23Pods:172.18.128.0/21172.18.136.0/21172.18.144.0/21172.18.152.0/21 Services:10.110.84.0/23Pods:10.12.128.0/2110.12.136.0/2110.12.144.0/2110.12.152.0/21 Services:10.110.144.0/23Pods:10.13.128.0/2110.13.136.0/2110.13.144.0/2110.13.152.0/21 Data Services:10.110.22.0/23Pods:172.23.128.0/21172.23.136.0/21172.23.144.0/21172.23.152.0/21 Services:10.110.82.0/23Pods:10.2.128.0/2110.2.136.0/2110.2.144.0/2110.2.152.0/21 Services:10.110.142.0/23Pods:10.3.128.0/2110.3.136.0/2110.3.144.0/2110.3.152.0/21 Operations Services:10.110.20.0/23Pods:172.16.128.0/21172.16.136.0/21172.16.144.0/21172.16.152.0/21 Services:10.110.80.0/23Pods:10.1.128.0/2110.1.136.0/2110.1.144.0/2110.1.152.0/21 Services:10.110.140.0/23Pods:10.0.128.0/2110.0.136.0/2110.0.144.0/2110.0.152.0/21 ... ... ... ... <p>Networking Configuration YAML</p> <ul> <li>src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.CI.yaml</li> <li>src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.QA.yaml</li> <li>src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.Production.yaml</li> </ul>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#eks-cluster","title":"EKS Cluster","text":"<p>This is the beating heart of where your kubernetes cluster is defined in Pulumi. It contains a multitude of configuration optioons including namespaces defined in this cluster, what operating environment (e.g. CI, QA, or Production) the cluster will operate in.</p> <p>It also contains such details as how Vault ties in, and the instance type (size, etc) for its workers.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#substructure","title":"Substructure","text":"<p>There's not much to configure here, but the code that this section embodies is critical. It builds critical path components like SSL certs and the components (like Traefik and Vault!) that manage them as well as authentication and other secrets.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#building-the-foundation-eks-cluster","title":"Building the Foundation - EKS Cluster","text":"<p>If you'll need to build a new EKS cluster as we did the data cluster used for the Open metadata application, you'll need to create the necessary configuration in the infrastructure and substructure sections.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#networking","title":"Networking","text":"<p>You'll need to choose pod and service subnets for your cluster.</p> <p>For now, use the example of the data-qa cluster's definitions and pay attention to the relationship between the subnets we chose and the VPC subnets they live in.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#namespace","title":"Namespace","text":"<p>You'll likely need to add a new namespace for your application whether you're creating a new cluster or not to the cluster's configuration.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#application_1","title":"Application","text":"<p>When we deploy our Kubernetes applications with Pulumi we use the Pulumi Kubernetes Provider.</p> <p>Helm charts are deployed by Pulumi by translating the helm chart into a kubernetes.helm.v3.release object. Click the link about for an example of how we translated Open Metadata's helm chart. Pay particular attention to the Values dictionary.</p>"},{"location":"platform_services/container_orchestration/deploying_apps_with_kubernetes/#make-it-so","title":"Make It So","text":"<p>We use CI to prove things out. So you'll want to start with that environment.</p> <p>If your application will require its own EKS cluster, you'll want to build that first.</p> <p>So change directories to ol-infrastructure/src/ol_infrastructure/infrastructure/aws/eks and run pulumi up.</p> <p>For instance, were you building the data CI cluster, you'd run: <pre><code>pulumi up -s infrastructure.aws.eks.data.CI\n</code></pre></p> <p>You'll almost certainly need to fix issues as you go. There's lots of complex configuration here that can't be covered in a simple doc.</p> <p>Then you'll want to build the resources in substructure for your project, so once again for the data CI cluster we'd want to change directory to ol-infrastucture/src/ol_infrastructure/substructure/aws/eks and run pulumi up.</p> <pre><code>pulumi up -s substructure.aws.eks.data.CI\n</code></pre> <p>Then you'll want to deploy your application. So for OMD as an example, cd to ol-infrastucture/src/ol_infrastructure/applications/open_metadata and run pulumi up.</p> <pre><code>pulumi up -s applications.open_metadata.CI\n</code></pre>"},{"location":"platform_services/container_orchestration/kubernetes_cookbook/","title":"MIT OL Kubernetes Cookbook","text":""},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#kubectl-recipes","title":"kubectl Recipes","text":""},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#port-forward-to-a-pod","title":"Port-forward to a pod","text":"<p>https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/</p> <pre><code>kubectl port-forward &lt;podname&gt; &lt;local port&gt;:&lt;remote port&gt; -n &lt;namespace&gt;\n\nkubectl port-forward grafana-alloy-dnqj2 12345:12345 -n operations\n</code></pre>"},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#get-a-pgsql-prompt","title":"Get a pgsql Prompt","text":"<pre><code>kubectl run -i --tty postgres --image=postgres --restart=Never -n airbyte -- sh\n</code></pre>"},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#deleting-k8s-namespaces-with-stuck-vault-finalizers","title":"Deleting k8s Namespaces With Stuck Vault Finalizers","text":"<pre><code>kubectl patch -n airbyte vaultauth airbyte-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n airbyte vaultconnection airbyte-vault-connection -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n airbyte vaultstaticsecret airbyte-basic-auth-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n airbyte vaultstaticsecret airbyte-forward-auth-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n airbyte vaultdynamicsecret airbyte-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultdynamicsecret open-metadata-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultdynamicsecret openmetadata-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultdynamicsecret openmetadata-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultstaticsecret openmetadata-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultstaticsecret openmetadata-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultauth open-metadata-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultauth open-metadata-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n open-metadata vaultconnection open-metadata-vault-connection -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\nkubectl patch -n operations vaultstaticsecret vault-kv-global-odl-wildcard -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\n</code></pre>"},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#get-overview-of-a-namespace","title":"Get Overview Of a Namespace","text":"<p>Shows things like open ports, pod status and the like. <pre><code>kubectl get all -n open-metadata\n</code></pre></p>"},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#get-information-status-on-a-particular-resource","title":"Get Information / Status On A Particular Resource","text":"<p><pre><code>kubectl describe &lt;resource&gt; &lt;optional-resource-name&gt; -n &lt;namespace&gt;\n</code></pre> e.g. <pre><code>kubectl describe pod -n open-metadata openmetadata-5f78b769d4-4wgs9                                                                                                                 feoh@prometheus\n</code></pre></p>"},{"location":"platform_services/container_orchestration/kubernetes_cookbook/#pulumi-server-side-complaints","title":"Pulumi Server Side Complaints","text":"<p>Sometimes pulumi will complain about being unable to manage a field or something on k8s resources. Something like this:</p> <pre><code>Diagnostics:\n  pulumi:pulumi:Stack (ol-infrastructure-open_metadata-application-applications.open_metadata.CI):\n    error: preview failed\n\n  kubernetes:core/v1:ServiceAccount (open-metadata-vault-service-account):\n    error: Preview failed: 1 error occurred:\n        * the Kubernetes API server reported that \"open-metadata/open-metadata-vault\" failed to fully initialize or become live: Server-Side Apply field conflict detected. See https://www.pulumi.com/registry/packages/kubernetes/how-to-guides/managing-resources-with-server-side-apply/#handle-field-conflicts-on-existing-resources for troubleshooting help.\n    The resource managed by field manager \"pulumi-kubernetes-51b738f0\" had an apply conflict: Apply failed with 1 conflict: conflict with \"pulumi-kubernetes-cef7f602\": .metadata.labels.pulumi_stack\n\n  kubernetes:rbac.authorization.k8s.io/v1:ClusterRoleBinding (open-metadata-vault-cluster-role-binding):\n    error: Preview failed: 1 error occurred:\n        * the Kubernetes API server reported that \"open-metadata-vault:cluster-auth\" failed to fully initialize or become live: Server-Side Apply field conflict detected. See https://www.pulumi.com/registry/packages/kubernetes/how-to-guides/managing-resources-with-server-side-apply/#handle-field-conflicts-on-existing-resources for troubleshooting help.\n    The resource managed by field manager \"pulumi-kubernetes-0e168a03\" had an apply conflict: Apply failed with 2 conflicts: conflicts with \"pulumi-kubernetes-0754bbed\":\n    - .metadata.labels.pulumi_stack\n    conflicts with \"pulumi-kubernetes-f4f83ba0\":\n    - .metadata.labels.pulumi_stack\n</code></pre> <p>Easiest thing to do is set an env var on execution which will bring the questionable fields back into pulumi management and keep you moving. There is still probably a bigger issue at play, though.</p> <pre><code>PULUMI_K8S_ENABLE_PATCH_FORCE=\"true\" pr pulumi up -s applications.open_metadata.CI\n</code></pre>"},{"location":"platform_services/secrets_management/faking_it_with_consul_and_vault/","title":"Faking it with Consul and Vault","text":""},{"location":"platform_services/secrets_management/faking_it_with_consul_and_vault/#why","title":"Why?","text":"<p>Sometimes you want to test an AMI build process without all all the supporting stuff that allows an EC2 instance to talk to vault and consul like a good little box. All that supporting stuff typically happens are instance start time and rides in on <code>user_data</code> which you don't have if you fired up an EC2 by hand just to see what your AMI looks like so far.</p> <p>So, for development purposes here is an abbreviated guide of faking it until you make it.</p>"},{"location":"platform_services/secrets_management/faking_it_with_consul_and_vault/#consul","title":"Consul","text":"<ol> <li>From a like-minded instance, find the proper IAM instance profile ARN. In my case I'm making a new type of EC2 box for edxapp / mite-staging / ci, so I went to one of the existing boxes for that environment, copied the Instance Profile ARN and associated it to my new, one-off, manually created EC2 box. This is essential.</li> <li>Second, from the same like-minded instance, nab <code>/etc/consul.d/99-autojoin.json</code>. <pre><code>{\"retry_join\": [\"provider=aws tag_key=consul_env tag_value=mitx-staging-ci\"], \"datacenter\": \"mitx-staging-ci\"}\n</code></pre></li> <li>Copy that config to your one-off box and restart consul and confirm you are now joined to the cluster with <code>consul catalog nodes</code> or whatever you fancy.</li> </ol>"},{"location":"platform_services/secrets_management/faking_it_with_consul_and_vault/#vault","title":"Vault","text":"<p>This requires using vault &gt;= 1.13.x which introduced the <code>token_file</code> auto_auth method.</p> <ol> <li> <p>On your one-off EC2 box, backup the existing <code>/etc/vault/vault.json</code> file and change the auto_auth block to be like the following: <pre><code>  \"auto_auth\": {\n    \"method\": {\n      \"type\": \"token_file\",\n      \"config\": {\n        \"token_file_path\": \"/etc/vault/vault_token\"\n      }\n    },\n    \"sink\": [\n      {\n        \"type\": \"file\",\n        \"derive_key\": false,\n        \"config\": [\n          {\n            \"path\": \"/etc/vault/vault_agent_token\"\n          }\n        ]\n      }\n    ]\n  },\n</code></pre></p> </li> <li> <p>In the vault UI, top right side little man icon, click 'Copy Token' and put that into <code>/etc/vault/vault_token</code> <pre><code>echo -n \"&lt;token&gt;\" &gt; /etc/vault/vault_token\n</code></pre></p> </li> <li>Restart vault and verify that it is happy and healthy.</li> </ol>"},{"location":"platform_services/secrets_management/faking_it_with_consul_and_vault/#disclaimer","title":"Disclaimer","text":"<p>You basically just gave this node a root token to vault ... so, you know ... don't do this anywhere besides CI and don't leave it hanging around. This is just for debugging.</p>"},{"location":"platform_services/secrets_management/restore_vault_backups/","title":"So you broke vault?","text":""},{"location":"platform_services/secrets_management/restore_vault_backups/#symptoms","title":"Symptoms","text":"<ul> <li>The vault UI or CLI reports that the cluster is sealed and there is no amount of coaxing to get it out of that state.</li> <li>There may be other symptoms. Add to this list as needed.</li> </ul> <pre><code>root@ip-172-16-0-82:/etc/vault/# VAULT_SKIP_VERIFY=true vault status\nKey                      Value\n---                      -----\nRecovery Seal Type       awskms\nInitialized              false                 &lt;&lt;&lt;&lt;&lt; Ahh! Bad!\nSealed                   true                  &lt;&lt;&lt;&lt;&lt; Sealed!\nTotal Recovery Shares    0\nThreshold                0\nUnseal Progress          0/0                   &lt;&lt;&lt;&lt;&lt; This node knowns nothing about any other nodes\nUnseal Nonce             n/a\nVersion                  1.14.1\nBuild Date               2023-07-21T10:15:14Z\nStorage Type             raft\nHA Enabled               true                  &lt;&lt;&lt;&lt;&lt; And it is supposed too!\n</code></pre>"},{"location":"platform_services/secrets_management/restore_vault_backups/#what-to-do","title":"What to do!?!","text":"<ol> <li>Go to the apporpriate S3 bucket and find the most recent backup. Production backs up every six hours. QA + CI backup once a day. Bucket names are: ol-infra-ci-vault-backups, ol-infra-qa-vault-backups, ol-infra-production-vault-backups.</li> <li>Download the most recent backup locally to your machine and stage it to the vault node you're going to run the restore on. You only run this procedure ONCE! You do not need to run this on every vault node. <pre><code>scp -i &lt;path to your oldevops.pem ssh private key&gt; &lt;path to downloaded .snapshot file&gt; admin@&lt;IP address of the vault node you're restoring to&gt;:/tmp\n</code></pre></li> <li>On the node that you've copied the <code>.snapshot</code> file to, verify that the vault status outputs as above.</li> <li>Export a vault setting to make life less annoying <code>export VAULT_SKIP_VERIFY=true</code></li> <li>Initialize the vault cluster: <code>vault operator init</code></li> <li>Output will look like this: <pre><code>Recovery Key 1: bX5A***********************************ExhBC\nRecovery Key 2: XHo3***********************************LZxmZ\nRecovery Key 3: EEOE***********************************XWC8p\nRecovery Key 4: FyTq***********************************EY0ij\nRecovery Key 5: oXaW***********************************Wr74k\n\nInitial Root Token: hvs.**********************wf\n\nSuccess! Vault is initialized\n\nRecovery key initialized with 5 key shares and a key threshold of 3. Please\nsecurely distribute the key shares printed above.\n</code></pre></li> <li>The message says those recovery keys are important but they aren't in this case. You don't need to save them. What you do need is the initial root token. Export that into env var: <code>export VAULT_TOKEN=&lt;token value including hvs. from prev command output&gt;</code></li> <li>Do the restore: <code>vault operator raft snapshot restore &lt;path to .snapshot file&gt;</code>. It doesn't actually output anything.</li> <li>Unset VAULT_TOKEN with <code>unset VAULT_TOKEN</code> (because that token was tied the shortlived cluster that existed before running the restore). Then do a vault status: <code>vault status</code> and it should look something like this: <pre><code>Key                      Value\n---                      -----\nRecovery Seal Type       shamir\nInitialized              true\nSealed                   false                                       &lt;&lt;&lt;&lt; Victory!\nTotal Recovery Shares    2\nThreshold                2\nVersion                  1.14.1\nBuild Date               2023-07-21T10:15:14Z\nStorage Type             raft\nCluster Name             vault-cluster-ab3e7a1b\nCluster ID               ef172e45-fake-uuid-here-aeb8da8a7179\nHA Enabled               true\nHA Cluster               https://256.256.256.256:8201\nHA Mode                  standby\nActive Node Address      https://active.vault.service.consul:8200\nRaft Committed Index     815551\nRaft Applied Index       815551\n</code></pre></li> <li>Loop through the other nodes in the cluster and verify they have a similar vault unseal status. If they don't, try <code>systemctl restart vault</code>. They should join backup and restore the raft on their own provided they were broken to begin with.</li> <li>Verify that the vault UI works as excpected again and your secrets are there.</li> <li>Some, possibly serious, complications after doing this: it is possible that there are credential secrets out in the wild being used that are no longer tracked via leases in vault. For instance if they were issued between when the backup took place and when the cluster stopped being viable. This could be the case for PKI secrets as well and of course if any static secrets in vaults were changed between the backup and beginning of the outage.</li> </ol>"},{"location":"platform_services/service_discovery/consul_incantations/","title":"Misbehaving consul cluster incident from 20231004-20231905","text":""},{"location":"platform_services/service_discovery/consul_incantations/#overview","title":"Overview","text":"<p>At some point <code>operations-production</code> consul cluster fell over and stopped being useful. The root cause was not immediately know but was almost definitely related to an upgrade to <code>1.16.2</code> from <code>???</code> (no good record indication what the cluster was on before all this went down...)</p> <p>Key takeaway from the logs was this cryptic message about being unable to restore snapshot:</p> <pre><code>{\"@level\":\"error\",\"@message\":\"failed to restore snapshot\",\"@module\":\"agent.server.raft\",\"@timestamp\":\"2023-10-04T14:17:40.200037Z\",\"error\":\"failed to restore snapshot 1156-86945697-1696429059987: failed inserting acl token: missing value for index 'accessor'\"}\n</code></pre> <p>This is something that happens anytime a consul server restarts, it gets a copy of the raft from the other servers currently running and restores it. But, it is failing to do that and crashing.</p>"},{"location":"platform_services/service_discovery/consul_incantations/#initial-response","title":"Initial Response","text":"<p>Tobias was able to revive the cluster by downgrading it to 1.14.10 and it was then able to restore the snapshot it received from other nodes / on the filesystem (unclear how broken the cluster was at this time).</p>"},{"location":"platform_services/service_discovery/consul_incantations/#research","title":"Research","text":"<p>Looking up that message returned one very-not-promising result from the hashicorp forums.</p>"},{"location":"platform_services/service_discovery/consul_incantations/#resolution","title":"Resolution","text":"<p>Ultimitely spent a lot of time reading and pursuing dead ends but what I believe ulimitely resolved the issue was the following:</p> <ol> <li>Step through each consul server in the cluster and ensure:   a. it is on version 1.14.10   b. It has this acl stanza in <code>00-default.json</code>: <code>\"acl\": {\"enabled\":  true, \"default_policy\": \"allow\", \"enable_token_persistence\": false}</code>   c. Restart the servers one at a time to ensure the quorum never drops below 3 (or 2 if you're in non-prod).</li> <li>Now you should be able to issue <code>acl</code> commands using the consul cli.   a. They won't work though because you don't have a token and in this particular case the cluster says it is not elgible for bootstrapping.   b. At some point in history this very special cluster had ACLs enabled and then disabled? Possibly?</li> <li>We need to reset/recover the ACL master token. Follow the procedure here.</li> <li>The output of that command should be \"Bootstrap Token (Global Management)\" an <code>AccessorID</code> and <code>SecretID</code>. Export the secret ID in your terminal as <code>CONSUL_HTTP_TOKEN</code>.</li> <li>Then you can do <code>consul acl token list</code> and one of them should be labeled \"Master Token\".   a. export the <code>SecretID</code> from the Master Token as <code>CONSUL_HTTP_TOKEN</code> or just save it off somewhere.</li> <li>Repeat step 1, loop through all the servers and remove the <code>acl</code> stanza, restarting one at a time to ensure quorum.</li> <li>Once all nodes are running again and ACL is disabled, start upgrading the nodes. one at a time, to 1.16.2. <pre><code>wget https://releases.hashicorp.com/consul/1.16.2/consul_1.16.2_linux_amd64.zip\nunzip consul_1.16.2_linux_amd64.zip\nmv consul consul_1_16_2\nsystemctl stop consul\ncp consul_1_16_2 /usr/local/bin/consul\ncd /var/lib\ncp -r consul consul_bak\ncd consul\nrm -rf serf/ raft/ server_metadata.json checkpoint-signature\nsystemctl start consul\n</code></pre></li> <li>Verify quorum: <code>consul operator raft list-peers</code></li> <li>Verify version: <code>consul members</code> and <code>consul version</code></li> </ol>"},{"location":"platform_services/service_discovery/fargate_service_discovery/","title":"Integrating Fargate Services Into Our Existing Systems Architecture","text":""},{"location":"platform_services/service_discovery/fargate_service_discovery/#the-goal","title":"The Goal","text":"<p>We want to start using ECS Fargate from AWS as a low-complexity way of running containerized workloads. It allows us to specify a set of containers that we would like to run as a single task, with simple autoscaling and resource allocation, and no need to manage the underlying servers. At face value this is a great option, even with the slight cost markup that it brings. It is highly likely that we will save at least that much money in engineering time.</p>"},{"location":"platform_services/service_discovery/fargate_service_discovery/#the-challenges","title":"The Challenges","text":"<p>In order to for us to properly integrate applications and services into our infrastructure, we need to be able to connect them to our Vault and Consul clusters. This allows us to expose the services that they provide to our other systems, as well as allowing us to simplify the configuration needed to let those applications connect to other system components. In an EC2 environment this is as simple as running a Consul and Vault agent, and setting up DNS routing to use the Consul agent running on localhost.</p> <p>Fargate makes this challenging due to the set of (reasonable) constraints that it places on the tasks that it runs. Among these is the fact that it can only use the DNS server at the VPC level for address resolution. This is due to its use of an Elastic Network Interface (ENI) for network traffic. It is possible for containers to communicate with their peer containers in a task group over localhost connections, but since DNS is a privileged process it is not straightforward to force those queries to rely on a sidecar in the grouping. Because it is not possible to use Consul as the DNS provider, it complicates the configuration of Vault agents for locating the server cluster that it needs to authenticate to.</p>"},{"location":"platform_services/service_discovery/fargate_service_discovery/#the-solution","title":"The Solution","text":"<p>In order to work around these constraints it is possible to use the AWS Cloud Map service as a means of registering and discovering services across our infrastructure. This in conjunction with the consul-aws application provides a means of setting up a bi-directional sync of services registered in Consul and services registered in AWS Cloud Map. By registering a cloud-map namespace for each of our VPCs and creating an ECS Fargate task to execute the consul-aws synchronization, we can maintain a consistent view of what services are available, regardless of whether they are communicating with the cloud-map DNS or the Consul DNS.</p>"},{"location":"platform_services/service_discovery/fargate_service_discovery/#design-challenges","title":"Design Challenges","text":"<p>While the use of Consul and AWS Cloud Map provides a low-complexity means of keeping things in sync, it does provide some friction in how applications are configured. This is due to the fact that the DNS names for a given service will be different depending on which system is being queried. For a service being discovered via Consul the query would be <code>&lt;service_name&gt;.service.consul</code>, whereas in AWS Cloud-Map it would be <code>&lt;service_name&gt;.&lt;cloud-map_namespace&gt;</code>. This is largely a manageable problem, since it will be primarily Fargate processes that need to interact with cloud-map and most other systems will use Consul, but it will require a clear understanding of which processes are communicating in which contexts.</p>"},{"location":"runbooks_post_mortems/20231030_xpro_outage/","title":"xPro Outage - October 30, 2023","text":""},{"location":"runbooks_post_mortems/20231030_xpro_outage/#timeline","title":"Timeline","text":"<p>Undetermined date within the past 6 months:</p> <ul> <li>AWS RDS config updated to default new credentials to use SCRAM rather than md5</li> </ul> <p>Monday, October 30th 2023 - 3:45PM - Configuration update deployed xPro Production Heroku stack via Salt. - 3:51PM - Pingdom alerts DevOps on-call. - 3:51PM - Alert Acknowledged by DevOps on-call (Mike Davidson). - 3:52PM - Verified that site was non-responsive. - 3:52PM - Begin detailed investigation. - ~3:53PM - Confirm log messages in Heroku regarding failed database logins: <pre><code>LOG C-0x55cb586d2540: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@127.0.0.1:55038 login attempt: db=db1 user=v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728 tls=no\nERROR S-0x55cb586d5580: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@3.209.36.28:5432 cannot do SCRAM authentication: wrong password type\nLOG C-0x55cb586d2e00: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@127.0.0.1:52474 closing because: server login failed: wrong password type (age=14s)\n</code></pre> - ~3:55PM Determined this matches the fingerprint of a previously encountered issue seen when setting up the new MITOpen environments. - 3:56PM Found previous PR that addressed this for MITOpen: - 3:56PM Started looking for code location for make the above modification. - 4:00PM Unable to locate code for this environment. Determined it is not managed by pulumi. - 4:03PM Started a zoom call. Attendees: Mike, Tobias, Sar - 4:05PM Decided making the database parameter group configuration will possibly take longer than is desirable. Opt for trying the alternative resolution. - ~4:07PM Role definition in vault is modified and verified to still generate credentials via the Vault UI. - 4:09PM Cached credentials are cleared and Heroku configuration is re-applied via Salt. - ~4:10PM Verified the issue is not resolved. - 4:12PM Cleared cached credentials again and Heroku configuration is re-applied via Salt. - 4:14PM Verified the issue is not resolved and that the credentials applied via Salt are truely different. - 4:15PM Decided alternative solution did not work. Begin implementing database parameter group fix known to work. - ~4:20PM A new database parameter group is created and applied to the running database. - ~4:23PM The database is listed as Available in the RDS web console. - 4:24PM Cleared cached credentials again and Heroku configuration is re-applied via Salt. - 4:25PM Verified the site is now available again and login/other database interactions work. - 4:25PM Pingdom automatically closes the OpsGenie alert.</p>"},{"location":"runbooks_post_mortems/20231030_xpro_outage/#root-cause","title":"Root Cause:","text":"<p>At some point in the last 6 months, the default behavior of the underlying RDS instance for xPro production was transitioned to using SCRAM password authentication rather than MD5 password authentication. Further information about this is documented here. The buildpack we use in Heroku for pgbouncer does not support SCRAM.</p> <p>When managing configuration items for applications deployed in Heroku, we invoke Vault via SaltStack to generate database credentials. These credentials are then cached on the Salt Master. Vault tracks the credentials it generates via internal objects known as 'leases'. When Salt checked + applied the configuration at 3:45PM, it saw that the lease was due to expire soon and was not eligible for renewal. This triggered Vault generating new credentials, however these new credentials were generated with the new default SCRAM configuration rather than the MD5 configuration supported by pgbouncer.</p> <p>When attempting to establish a pool of database connections, pgbouncer was rejected by the RDS instance for sending the wrong password type. It sent an MD5 password hash while the database expected SCRAM for the newly generated credential. Being unable to establish a connection to the database is a fatal error for the application and it was not able to finish starting up and serving requests.</p>"},{"location":"runbooks_post_mortems/20231030_xpro_outage/#action-items","title":"Action Items","text":"<ul> <li>Migrate management of XPro resources into Pulumi</li> <li>One of the key factors delaying the resolution of the outage was the fact that none, or nearly none, of the configuration for this stack was managed as code. In particular the RDS instance and its associated resources. Additionally, the RDS instance was using an instance of the default parameter group provided by AWS, which cannot be modified. As such, we were required to duplicate this default parameter group definitions, make the required modifications, and then associate the newly created group with the instance. This is a more time consuming task than modifying a parameter configuration on an already associated parameter group. All of this would have been mitigated if the resources were managed with pulumi, where we do not utilize AWS sourced default parameter groups.</li> </ul>"},{"location":"runbooks_post_mortems/oncall_runbook/","title":"On-Call Runbook","text":"<ul> <li>Style Guide</li> <li>ODL Open Discussions</li> <li>SaltStack</li> <li>XQueueWatcher</li> <li>OVS</li> <li>Bootcamp Ecommerce</li> <li>OpenEdX Residential MITx</li> <li>XPro</li> <li>MITXOnline</li> <li>Reddit</li> </ul>"},{"location":"runbooks_post_mortems/oncall_runbook/#introduction","title":"Introduction","text":"<p>This document is meant to be one stop shopping for your MIT OL Devops oncall needs.</p> <p>Please update this doc as you handle incidents whenever you're oncall.</p>"},{"location":"runbooks_post_mortems/oncall_runbook/#style-guide","title":"Style Guide","text":"<p>There should be a table of contents at the top of the document with links to each product heading. Your editor likely has a plugin to make this automatic.</p> <p>Each product gets its own top level heading.</p> <p>Entries that are keyed to a specific alert should have the relevant text in a second level heading under the product. Boil the alert down to the most relevant searchable text and omit specifics that will vary. For instance:</p> <p><pre><code>\"[Prometheus]: [FIRING:1] DiskUsageWarning mitx-production (xqwatcher filesystem /dev/root ext4 ip-10-7-0-78 integrations/linux_hos\"\n</code></pre> would boil down to <code>DiskUsageWarning xqwatcher</code> because the rest will change and make finding the right entry more difficult.</p> <p>Each entry should have at least two sections, Diagnosis and Mitigation. Use bold face for the section title. This will allow the oncall to get only as much Diagnosis in as required to identify the issue and focus on putting out the fire.</p>"},{"location":"runbooks_post_mortems/oncall_runbook/#products","title":"Products","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#odl-open-discussions","title":"ODL Open Discussions","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#invalidaccesskeynonprod-qa-odl-open-discussions-warning","title":"InvalidAccessKeyNonProd qa (odl-open-discussions warning)","text":"<p>Diagnosis</p> <p>You get an alert like \"[Prometheus]: [FIRING:1] InvalidAccessKeyNonProd qa (odl-open-discussions warning)\".</p> <p>Mitigation</p> <p>In the mitodl/ol-infrastructure Github repository, change directory to <code>src/mit/ol-infrastructure/src/ol_infrastructure/applications/open_discussions</code> and run `pulumi up'.</p>"},{"location":"runbooks_post_mortems/oncall_runbook/#saltstack","title":"SaltStack","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#memoryusagewarning-operations-","title":"MemoryUsageWarning operations- <p>Diagnosis</p> <p>You get an alert like: <code>[Prometheus]: [FIRING:1] MemoryUsageWarning operations-qa (memory ip-10-1-3-33 integrations/linux_host warning)</code>.</p> <p>You'll need an account and ssh key set up on the saltstack master hosts. This should happen when you join the team.</p> <p>Now, ssh into the salt master appropriate to the environment you received the alert for. The IP address is cited in the alert. So, for the above:</p> <p>(Substitute your username and the appropriate environment if not qa, e.g. production) <pre><code>ssh -l cpatti salt-qa.odl.mit.edu\n</code></pre></p> <p>Next, check free memory:</p> <pre><code>mdavidson@ip-10-1-3-33:~$ free -h\n              total        used        free      shared  buff/cache   available\nMem:           7.5G        7.2G        120M         79M        237M         66M\nSwap:            0B          0B          0B\n</code></pre> <p>In this case, the machine only has 120M free which isn't great.</p> <p>Mitigation</p> <p>We probably need to restart the Salt master service. Use the systemctl command for that:</p> <pre><code>root@ip-10-1-3-33:~#  systemctl restart salt-master\n</code></pre> <p>Now, wait a minute and then check free memory again. There should be significantly more available:</p> <pre><code>root@ip-10-1-3-33:~# free -h\n              total        used        free      shared  buff/cache   available\nMem:           7.5G        1.9G        5.3G         80M        280M        5.3G\nSwap:            0B          0B          0B\n</code></pre> <p>If what you see is something like the above, you're good to go. Problem solved (for now!)</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#xqueuewatcher","title":"XQueueWatcher","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#diskusagewarning-xqwatcher","title":"DiskUsageWarning xqwatcher <p>Diagnosis</p> <p>This happens every few months if the xqueue watcher nodes hang around for that long.</p> <p>Mitigation</p> <pre><code>From salt-pr master:\n\nsudo ssh -i /etc/salt/keys/aws/salt-production.pem ubuntu@10.7.0.78\nsudo su -\n\nroot@ip-10-7-0-78:~# df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        20G   16G  3.9G  81% /           &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; offending filesystem\ndevtmpfs        1.9G     0  1.9G   0% /dev\ntmpfs           1.9G  560K  1.9G   1% /dev/shm\ntmpfs           389M  836K  389M   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\ntmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/loop1       56M   56M     0 100% /snap/core18/2751\n/dev/loop2       25M   25M     0 100% /snap/amazon-ssm-agent/6312\n/dev/loop0       25M   25M     0 100% /snap/amazon-ssm-agent/6563\n/dev/loop3       54M   54M     0 100% /snap/snapd/19361\n/dev/loop4       64M   64M     0 100% /snap/core20/1950\n/dev/loop6       56M   56M     0 100% /snap/core18/2785\n/dev/loop5       54M   54M     0 100% /snap/snapd/19457\n/dev/loop7       92M   92M     0 100% /snap/lxd/24061\n/dev/loop8       92M   92M     0 100% /snap/lxd/23991\n/dev/loop10      64M   64M     0 100% /snap/core20/1974\ntmpfs           389M     0  389M   0% /run/user/1000\n\nroot@ip-10-7-0-78:~# cd /edx/var           &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; intuition / memory\n\nroot@ip-10-7-0-78:/edx/var# du -h | sort -hr | head\n8.8G    .\n8.7G    ./log\n8.2G    ./log/xqwatcher         &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; Offender\n546M    ./log/supervisor\n8.0K    ./supervisor\n4.0K    ./xqwatcher\n4.0K    ./log/aws\n4.0K    ./aws\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# cd log/xqwatcher/\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# ls -tlrha\ntotal 8.2G\ndrwxr-xr-x 2 www-data xqwatcher 4.0K Mar 11 08:35 .\ndrwxr-xr-x 5 syslog   syslog    4.0K Jul 14 00:00 ..\n-rw-r--r-- 1 www-data www-data  8.2G Jul 14 14:12 xqwatcher.log             &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; big file\n\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# rm xqwatcher.log\n\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl restart supervisor.service\nJob for supervisor.service failed because the control process exited with error code.\nSee \"systemctl status supervisor.service\" and \"journalctl -xe\" for details.\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl restart supervisor.service       &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;  Restart it twice because ???\n\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl status supervisor.service\n\u25cf supervisor.service - supervisord - Supervisor process control system\n     Loaded: loaded (/etc/systemd/system/supervisor.service; enabled; vendor preset: enabled)\n     Active: active (running) since Fri 2023-07-14 14:12:51 UTC; 4min 48s ago\n       Docs: http://supervisord.org\n    Process: 1114385 ExecStart=/edx/app/supervisor/venvs/supervisor/bin/supervisord --configuration /edx/app/supervisor/supervisord.conf (code=exited, status=0/SUCCESS)\n   Main PID: 1114387 (supervisord)\n      Tasks: 12 (limit: 4656)\n     Memory: 485.8M\n     CGroup: /system.slice/supervisor.service\n             \u251c\u25001114387 /edx/app/supervisor/venvs/supervisor/bin/python /edx/app/supervisor/venvs/supervisor/bin/supervisord --configuration /edx/app/supervisor/supervisord.conf\n             \u2514\u25001114388 /edx/app/xqwatcher/venvs/xqwatcher/bin/python -m xqueue_watcher -d /edx/app/xqwatcher\n\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# ls -lthra\ntotal 644K\ndrwxr-xr-x 5 syslog   syslog    4.0K Jul 14 00:00 ..\ndrwxr-xr-x 2 www-data xqwatcher 4.0K Jul 14 14:12 .\n-rw-r--r-- 1 www-data www-data  636K Jul 14 14:17 xqwatcher.log                &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; New file being written to\nroot@ip-10-7-0-78:/edx/var/log/xqwatcher# df -h .\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        20G  7.4G   12G  38%                  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; acceptable utilization\n</code></pre>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#ovs","title":"OVS","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#prometheus-firing1-invalidaccesskeyproduction-apps-production-odl-video-service-critical","title":"[Prometheus]: [FIRING:1] InvalidAccessKeyProduction apps-production (odl-video-service critical) <p>Diagnosis</p> <p>This happens sometimes when the applications's instance S3 credentials become out of date.</p> <p>Mitigation</p> <p>Use the AWS EC2 web console and navigate to the EC2 -&gt; Auto Scaling Group pane. Search on: <code>odl-video-service-production</code></p> <p>Once you have the right ASG, click on the \"Instance Refresh\" tab and then click the \"Start Instance Refresh\" button.</p> <p>Be sure to un-check the \"Enable Skip Matching\" box, or your instance refresh will most likely not do anything at all.</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#request-by-deeveloper-to-add-videos","title":"Request by deeveloper to add videos <p>Diagnosis</p> <p>N/A - developer request</p> <p>Mitigation</p> <p>Use the AWS EC2 web console and find instances of type <code>odl-video-service-production</code> - detailed instructions for accessing the instance can be found here.</p> <p>The only difference in this case is that the user is <code>admin</code> rather than <code>ubuntu</code>. Stop when you get a shell prompt and rejoin this document.</p> <p>First, run:</p> <p><code>sudo docker compose ps</code> to see a list of running processes. In our case, we're looking for <code>app</code>. This isn't strictly necessary here as we know what we're looking for, but good to look before you leap anyway.</p> <p>You should see something like:</p> <pre><code>admin@ip-10-13-3-50:/etc/docker/compose$ sudo docker compose ps\nNAME                IMAGE                                 COMMAND                  SERVICE             CREATED             STATUS              PORTS\ncompose-app-1       mitodl/ovs-app:v0.69.0-5-gf76af37     \"/bin/bash -c ' slee\u2026\"   app                 3 weeks ago         Up 3 weeks          0.0.0.0:8087-&gt;8087/tcp, :::8087-&gt;8087/tcp, 8089/tcp\ncompose-celery-1    mitodl/ovs-app:v0.69.0-5-gf76af37     \"/bin/bash -c ' slee\u2026\"   celery              3 weeks ago         Up 3 weeks          8089/tcp\ncompose-nginx-1     pennlabs/shibboleth-sp-nginx:latest   \"/usr/bin/supervisor\u2026\"   nginx               3 weeks ago         Up 3 weeks          0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp\n</code></pre> <p>Now run:</p> <p><code>sudo docker compose exec -it app /bin/bash</code> which should get you a new, less colorful shell prompt.</p> <p>At this point you can run the manage.py command the developer gave you in slack. In my case, this is what I ran and the output I got:</p> <pre><code>mitodl@486c7fbba98b:/src$ python ./manage.py add_hls_video_to_edx --edx-course-id course-v1:xPRO+DECA_Boeing+SPOC_R0\nAttempting to post video(s) to edX...\nVideo successfully added to edX \u2013 VideoFile: CCADE_V11JW_Hybrid_Data_Formats_v1.mp4 (105434), edX url: https://courses.xpro.mit.edu/api/val/v0/videos/\n</code></pre> <p>You're all set!</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#bootcamp-ecommerce","title":"Bootcamp Ecommerce","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#prometheus-firing1-alternateinvalidaccesskeyproduction-production-bootcamp-ecommerce-critical","title":"[Prometheus]: [FIRING:1] AlternateInvalidAccessKeyProduction production (bootcamp-ecommerce critical) <p>Diagnosis</p> <p>N/A</p> <p>Mitigation</p> <p>You need to refresh the credentials the salt-proxy is using for Heroku to manage this app.</p> <ul> <li>ssh to the salt production server: <code>ssh salt-production.odl.mit.edu</code></li> <li>Run the salt proxy command to refresh creds: <code>salt proxy-bootcamps-production state.sls heroku.update_heroku_config</code>. You should see output similar to the following:</li> </ul> <pre><code>cpatti@ip-10-0-2-195:~$ sudo salt proxy-bootcamps-production state.sls heroku.update_heroku_config\nproxy-bootcamps-production:\n----------\n          ID: update_heroku_bootcamp-ecommerce_config\n    Function: heroku.update_app_config_vars\n        Name: bootcamp-ecommerce\n      Result: True\n     Comment:\n     Started: 14:43:58.916128\n    Duration: 448.928 ms\n     Changes:\n              ----------\n              new:\n                  ----------\n\n** 8&lt; snip 8&lt; secret squirrel content elided **\n\nSummary for proxy-bootcamps-production\n------------\nSucceeded: 1 (changed=1)\nFailed:    0\n------------\nTotal states run:     1\nTotal run time: 448.928 ms\ncpatti@ip-0-2-195:~$\n</code></pre>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#openedx-residential-mitx","title":"OpenEdX Residential MITx","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#task-handler-raised-error-operationalerror1045-access-denied-for-user-v-edxa-fmt0kbl5x1070237-using-password-yes","title":"Task handler raised error: \"OperationalError(1045, \"Access denied for user 'v-edxa-fmT0KbL5X'@'10.7.0.237' (using password: YES) <p>Diagnosis</p> <p>If the oncall receives this page, instances credentials to access Vault and the secrets it contains have lapsed.</p> <p>Mitigation</p> <p>Fixing this issue currently requires an instance refresh, as the newly launched instances will have all the necessary credentials.</p> <p>From the EC2 console, on the left hand side, click \"Auto Scaling Groups\", then type 'edxapp-web-mitx-' e.g. 'edxapp-web-mitx-production'. This should yield 1 result with something like 'edxapp-web-autoscaling-group-XXXX' in the 'Name' column. Click that. <p>Now click the \"Instance Refresh\" tab.</p> <p>Click \"Start instance refresh\".</p> <p>Be sure to un-check the \"Enable Skip Matching\" box, or your instance refresh will most likely not do anything at all.</p> <p>Monitor the instance refresh to ensure it completes successfully. If you have been receiving multiple similar pages, they should stop coming in. If they continue, please escalate this incident as this problem is user visible and thus high impact to customers.</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#xpro","title":"XPro","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#apiexception-hubspot_xprotaskssync_contact_with_hubspot","title":"ApiException hubspot_xpro.tasks.sync_contact_with_hubspot <p>Diagnosis</p> <p>This error is thrown when the Hubspot API key has expired.</p> <p>You'll see an error similar to this one in Sentry.</p> <p>Mitigation</p> <p>The fix for this is to generate a new API key in Hubspot and then get that key into Vault, triggering the appropriate pipeline deployment afterwards.</p> <p>First, generate a new API key in Hubspot. You can do this by logging into Hubspot,</p> <p>You can do this using the username/password and TOTP token found in [Vault](https://vault-production.odl.mit.edu/ui/vault/secrets/platform-secrets/kv/hubspot/details?version=1.</p> <p>Once you're logged in, click \"Open\" next to \"MIT XPro\" in the Accounts list.</p> <p>Then, click on the gear icon in the upper right corner of the page and select \"Integrations\" -&gt; \"Private Apps\" in the sidebar on the left.</p> <p>You should then see the XPRo private app and beneath that a link for \"View Access Token\". Click that, then click on the \"Manage Token\" link.</p> <p>On this screen, you should see a \"Rotate\" button, click that to generate a new API key.</p> <p>Now that you've generated your new API token, you'll need to get that token into Vault using SOPS. You can find the right secrets file for this in Github here.</p> <p>The process for deploying secrets deserves its own document, so after adding the new API token to the SOPS decrypted secrets file you just generated, commit it to Github, ensure it runs through the appropriate pipelines and ends up in Vault.</p> <p>You can find the ultimate home of the XPro Hubspot API key in Vault here.</p> <p>Once the new API token is in the correct spot, you'll need to ensure that new token gets deployed to production in Heroku by tracking its progress in this pipeline.</p> <p>You will likely need to close Concourse Github workflow issues to make this happen. See its users guide for details.</p> <p>Once that's complete, you should have mitigated this issue. Keep checking that Sentry page to ensure that the Last Seen value reflects something appropriately long ago and you can resolve this ticket.</p> <p>If you are asked to run a sync to Hubspot: - Inform the requester, preferably on the #product-xpro Slack that the process will take quite a long time. If this is time critical they may ask you to run only parts of the sync. You can find documentation on the command you'll run here.</p> <p>Since XPro runs on Heroku, you'll need to get a Heroku console shell to run the management command. You can get to that shell by logging into heroku with the Heroku CLI and running:</p> <pre><code>heroku run /bin/bash -a xpro-production\n</code></pre> <p>It takes a while but you will eventually get your shell prompt.</p> <p>From there, run the following commands. To sync all variants:</p> <pre><code>./manage.py sync_db_to_hubspot create\n</code></pre> <p>If you're asked to run only one, for example deals, you can consult the documentation linked above and see that you should add the <code>--deals</code> flag to the invocation.</p> <p>Be sure to inform the requester of what you see for output and add it to the ticket for this issue if there is one.</p> <p>If you see the command fail with an exception, note the HTTP response code. In particular a 401 means that the API key is likely out of date. A 409 signals a conflict (e.g. dupe email) that will likely be handled by conflict resolution code and thus can probably be ignored.</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#mitxonline","title":"MITXOnline","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#cybersource-credentials-potentially-out-of-date","title":"Cybersource credentials potentially out of date <p>Diagnosis</p> <p>Often we will get a report like this indicating that one of our Cybersource credentials is out of date.</p> <p>Mitigation</p> <p>Since we have no access to the Cybersource web UI, we must send E-mail to: sbmit@mit.edu to validate the status of the current credential or request a new one.</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#grading-celery-task-failed-stub-entry-needs-love","title":"Grading Celery Task Failed (STUB entry. Needs love) <p>Diagnosis</p> <p>Usually we'll get reports from our users telling us that grading tasks have failed.</p> <p>At that point we should surf to celery monitoring and login with your Keycloak Platform Engineering realm credentials.</p> <p>Then, get the course ID for the failed grading tasks and search for it in Celery Monitoring by entering the course key in the kwargs input, surrounded by {' and '}, for example {'course-v1:MITxT+14.310x+1T2024'}.</p> <p>Mitigation</p> <p>You may well be asked to run the <code>compute_graded</code> management command on the LMS for mitxonline. (TODO: Needs details. How do we get there? etc.)</p>","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#reddit","title":"Reddit","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#prometheus-firing1-diskusagewarning-production-apps-reddit-filesystem-devnvme0n1p1-ext4-ip-10-13-1-59-integrationslinux_","title":"[Prometheus]: [FIRING:1] DiskUsageWarning production-apps (reddit filesystem /dev/nvme0n1p1 ext4 ip-10-13-1-59 integrations/linux_","text":""},{"location":"runbooks_post_mortems/oncall_runbook/#pingdom-open-discussions-production-home-page-has-an-alert","title":"[Pingdom] Open Discussions production home page has an alert <p>Diagnosis</p> <p>We often get low disk errors on our reddit nodes, but in this case the low disk alert was paired with a pingdom alert on open-discussions.  This may mean that pgbouncer is in trouble on reddit, likely because its credentials are out of date.</p> <p>You can get a view into what's happening by logging into the node cited in the disk usage ticket and typing:</p> <pre><code>salt reddit-production* state.sls reddit.config,pgbouncer\n</code></pre> <p>Mitigation</p> <p>Once you've determined that pgbouncer is indeed sad, you can try a restart / credential refresh with the following command:</p> <pre><code>salt reddit-production* state.sls reddit.config\n</code></pre>","text":""},{"location":"sso_onboarding/sso_config/","title":"Integrating With MIT Learn SSO","text":""},{"location":"sso_onboarding/sso_config/#overview","title":"Overview","text":"<p>This document provides guidance for configuring single sign-on (SSO) with our platform using either OpenID Connect (OIDC) or SAML protocols. This integration allows your users to access our services using their existing organizational credentials from any compatible identity provider.</p>"},{"location":"sso_onboarding/sso_config/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Prerequisites</li> <li>Protocol Selection</li> <li>General Configuration Requirements</li> <li>Required Information Collection</li> <li>Information to Provide Our Team</li> <li>Configuration Exchange Process</li> <li>Testing the Integration</li> <li>Next Steps</li> <li>Support</li> <li>Provider-Specific Examples</li> <li>Microsoft Entra ID - OIDC</li> <li>Microsoft Entra ID - SAML</li> </ul>"},{"location":"sso_onboarding/sso_config/#prerequisites","title":"Prerequisites","text":"<ul> <li>Administrative access to your identity provider (IdP)</li> <li>The following information will be provided by our team:</li> <li>Keycloak realm URL</li> <li>Keycloak realm public certificate</li> <li>Organization-specific configuration details</li> </ul>"},{"location":"sso_onboarding/sso_config/#protocol-selection","title":"Protocol Selection","text":"<p>We recommend OpenID Connect (OIDC) for most implementations due to its modern design and ease of configuration.</p>"},{"location":"sso_onboarding/sso_config/#recommended-openid-connect-oidc","title":"Recommended: OpenID Connect (OIDC)","text":"<ul> <li>Best for: Modern applications, mobile apps, API access</li> <li>Advantages: JSON-based, built on OAuth2, excellent token management</li> <li>Setup complexity: Low to Medium</li> <li>Supported by: Most modern identity providers</li> </ul>"},{"location":"sso_onboarding/sso_config/#alternative-saml-20","title":"Alternative: SAML 2.0","text":"<ul> <li>Best for: Enterprise applications, legacy system integration</li> <li>Advantages: Mature protocol, extensive enterprise support</li> <li>Setup complexity: Medium to High</li> <li>Supported by: All major enterprise identity providers</li> </ul>"},{"location":"sso_onboarding/sso_config/#general-configuration-requirements","title":"General Configuration Requirements","text":""},{"location":"sso_onboarding/sso_config/#oidc-configuration-requirements","title":"OIDC Configuration Requirements","text":"<p>Your identity provider will need to be configured with the following settings:</p> <p>Application Registration Details: - Application Name: <code>MIT Learn</code> (or your preferred name) - Redirect URI: <code>https://sso.ol.mit.edu/realms/olapps/broker/oidc/endpoint</code> - Response Type: <code>code</code> - Grant Type: <code>authorization_code</code></p> <p>Required Claims/Scopes: - <code>openid</code> (Sign users in) - <code>profile</code> (View users' basic profile) - <code>email</code> (View users' email address)</p> <p>Authentication Method: - Certificate-based authentication using the public certificate provided by our team</p>"},{"location":"sso_onboarding/sso_config/#saml-configuration-requirements","title":"SAML Configuration Requirements","text":"<p>Your identity provider will need to be configured with the following settings:</p> <p>Service Provider (SP) Details: - Entity ID: <code>https://sso.ol.mit.edu/realms/olapps</code> - URLs: Will be provided by our team after initial configuration</p> <p>Required Attributes: - Name ID Format: Email address - Email: User's email address - Given Name: User's first name - Surname: User's last name - Display Name: User's full display name</p> <p>Certificate Configuration: - Signing Algorithm: SHA-256 - Response Signing: Required</p>"},{"location":"sso_onboarding/sso_config/#required-information-collection","title":"Required Information Collection","text":""},{"location":"sso_onboarding/sso_config/#for-oidc-integration","title":"For OIDC Integration","text":"<p>Please collect the following information from your identity provider: - Client ID: Your application's unique identifier - OpenID Connect Discovery URL: Usually <code>https://[your-idp]/.well-known/openid-configuration</code> - Tenant/Domain Information: If applicable to your provider</p>"},{"location":"sso_onboarding/sso_config/#for-saml-integration","title":"For SAML Integration","text":"<p>Please collect the following information from your identity provider: - Federation Metadata URL: If available (recommended - contains all configuration details below)</p> <p>OR if metadata URL is not available, provide these individual items: - Identity Provider Entity ID: Your IdP's unique identifier - Single Sign-On URL: Where users are redirected for authentication - Single Logout URL: Where users are redirected for logout - X.509 Certificate: Your IdP's signing certificate - Federation Metadata URL: If available (optional but recommended)</p>"},{"location":"sso_onboarding/sso_config/#information-to-provide-our-team","title":"Information to Provide Our Team","text":"<p>Please fill out the following Google Form with your configuration details.</p> <p>Important: Include all the information gathered in the Required Information Collection section above.</p>"},{"location":"sso_onboarding/sso_config/#configuration-exchange-process","title":"Configuration Exchange Process","text":"<p>After you submit your information:</p> <ol> <li>Our team will configure your organization in our system and provide you with:</li> <li>Public certificate for authentication (OIDC and SAML)</li> <li>Organization-specific endpoints (SAML only)</li> <li> <p>Service Provider metadata file (SAML only)</p> </li> <li> <p>You will need to update your identity provider with the information we provide</p> </li> <li> <p>Testing can begin once both sides have completed their configuration</p> </li> </ol> <p>Note: SAML configurations require additional endpoint information from our team before you can complete your setup.</p>"},{"location":"sso_onboarding/sso_config/#testing-the-integration","title":"Testing the Integration","text":"<p>Note: Testing can only be performed after the configuration exchange process above is complete.</p>"},{"location":"sso_onboarding/sso_config/#test-user-setup","title":"Test User Setup","text":"<ol> <li>Create a test user in your identity provider</li> <li>Ensure the test user has the required attributes (email, name, etc.)</li> <li>Assign the test user access to the MIT Learn application</li> </ol>"},{"location":"sso_onboarding/sso_config/#testing-steps","title":"Testing Steps","text":"<ol> <li>Navigate to our platform's login page</li> <li>Click \"Log in\"</li> <li>Enter the email address for your test user</li> <li>Click Next</li> <li>You should be redirected to your identity provider for authentication</li> <li>Enter your credentials at your identity provider</li> <li>After successful authentication, you will be logged into the Learn web site</li> <li>Confirm the test user's name and information on the Profile page at https://learn.mit.edu/dashboard/profile</li> </ol>"},{"location":"sso_onboarding/sso_config/#common-troubleshooting","title":"Common Troubleshooting","text":""},{"location":"sso_onboarding/sso_config/#oidc-specific-issues","title":"OIDC-specific Issues","text":"<ul> <li>Invalid redirect URI: Verify the reply URL matches exactly</li> <li>Token validation errors: Check clock synchronization between systems</li> <li>Missing user attributes: Verify claim mappings in your identity provider</li> <li>Permission denied: Ensure users are assigned to the application</li> </ul>"},{"location":"sso_onboarding/sso_config/#saml-specific-issues","title":"SAML-specific Issues","text":"<ul> <li>Invalid SAML Response: Check certificate validity and signing configuration</li> <li>Attribute mapping errors: Verify claim names and formats match requirements</li> <li>Invalid destination: Ensure ACS URL matches exactly</li> <li>Clock skew issues: Verify time synchronization between systems</li> </ul>"},{"location":"sso_onboarding/sso_config/#next-steps","title":"Next Steps","text":"<ol> <li>Choose your preferred protocol (OIDC recommended)</li> <li>Configure your identity provider using the requirements above</li> <li>Provide the required configuration details to our team via the form</li> <li>Wait for confirmation that our team has completed the configuration</li> <li>Test the integration with a small group of users</li> <li>Schedule a validation call to verify the integration</li> <li>Plan your user rollout and communication strategy</li> </ol>"},{"location":"sso_onboarding/sso_config/#support","title":"Support","text":"<p>For technical questions during setup: - Contact our integration team at: mitlearn-support@mit.edu - Include any error messages or screenshots for faster resolution</p> <p>For ongoing operational support after go-live: - Use your standard support channels - SSO-related issues should include user Principal Name and timestamp</p>"},{"location":"sso_onboarding/sso_config/#provider-specific-examples","title":"Provider-Specific Examples","text":"<p>The following sections provide detailed, step-by-step instructions for specific identity providers. Use these as reference implementations of the general requirements outlined above.</p>"},{"location":"sso_onboarding/sso_config/#microsoft-entra-id-oidc","title":"Microsoft Entra ID OIDC","text":""},{"location":"sso_onboarding/sso_config/#step-1-create-application-registration-in-entra-id","title":"Step 1: Create Application Registration in Entra ID","text":"<ol> <li>Navigate to the Azure portal (portal.azure.com)</li> <li>Go to Microsoft Entra ID \u2192 App registrations</li> <li>Click New registration</li> <li>Configure the following settings:</li> <li>User-facing display name for this application: <code>MIT Learn</code></li> <li>Supported account types: Depends on their setup and what accounts they want to allow</li> <li>Redirect URI: Web - <code>https://sso.ol.mit.edu/realms/olapps/broker/oidc/endpoint</code></li> <li>Click Register</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-2-configure-application-registration-secrets","title":"Step 2: Configure Application Registration Secrets","text":"<ol> <li>In your new application, go to Certificates &amp; secrets</li> <li>Select the Certificates tab and click Upload certificate</li> <li>Upload Public certificate provided to you by our team</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-3-configure-application-registration-api-permissions","title":"Step 3: Configure Application Registration API permissions","text":"<ol> <li>In your new application, go to API permissions</li> <li>Ensure the following Microsoft Graph permissions:</li> <li><code>openid</code> (Sign users in)</li> <li><code>profile</code> (View users' basic profile)</li> <li><code>email</code> (View users' email address)</li> <li><code>User.Read</code> (Sign in and read user profile)</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-4-collect-configuration-details","title":"Step 4: Collect Configuration Details","text":"<p>Gather the following information to provide to our team: - Application (Client) ID: Found in App Registration overview - OpenID Connect Metadata URL: <code>https://login.microsoftonline.com/[TENANT_ID]/v2.0/.well-known/openid_configuration</code></p> <p>(Note: You can find your Tenant ID on the Overview page of your Microsoft Entra ID instance in the Azure portal.)</p>"},{"location":"sso_onboarding/sso_config/#microsoft-entra-id-saml","title":"Microsoft Entra ID SAML","text":""},{"location":"sso_onboarding/sso_config/#step-1-create-enterprise-application-in-entra-id","title":"Step 1: Create Enterprise Application in Entra ID","text":"<ol> <li>Navigate to the Azure portal (portal.azure.com)</li> <li>Go to Microsoft Entra ID \u2192 Enterprise applications</li> <li>Click New application</li> <li>Click Create your own application</li> <li>Configure the following settings:</li> <li>Name: <code>MIT Learn</code></li> <li>What are you looking to do with your application?: Select \"Integrate any other application you don't find in the gallery (Non-gallery)\"</li> <li>Click Create</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-2-configure-basic-saml-configuration","title":"Step 2: Configure Basic SAML Configuration","text":"<ol> <li>In your new enterprise application, go to Single sign-on</li> <li>Select SAML as the single sign-on method</li> <li>In the Basic SAML Configuration section, click Edit</li> <li>Configure the following settings:</li> <li>Identifier (Entity ID): <code>https://sso.ol.mit.edu/realms/olapps</code></li> <li>Reply URL (Assertion Consumer Service URL): <code>https://sso.ol.mit.edu/realms/olapps/broker/saml/endpoint</code></li> <li>Sign on URL: <code>https://sso.ol.mit.edu/realms/olapps/broker/saml/endpoint</code></li> <li>Relay State: Leave blank</li> <li>Logout URL: <code>https://sso.ol.mit.edu/realms/olapps/broker/saml/endpoint</code></li> <li>Click Save</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-3-configure-attributes-claims","title":"Step 3: Configure Attributes &amp; Claims","text":"<ol> <li>In the Attributes &amp; Claims section, click Edit</li> <li>Ensure the following claims are present:</li> <li>Unique User Identifier (Name ID): user.userprincipalname</li> <li>emailaddress: user.mail</li> <li>givenname: user.givenname</li> <li>surname: user.surname</li> <li>name: user.displayname</li> <li>Set Name identifier format to Email address</li> <li>Click Save</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-4-configure-saml-signing-certificate","title":"Step 4: Configure SAML Signing Certificate","text":"<ol> <li>In the SAML Certificates section, note the following:</li> <li>App Federation Metadata Url: Copy this URL to provide to our team</li> <li>Certificate (Base64): Download this certificate</li> <li>Federation Metadata XML: Download this file</li> <li>In the Advanced Certificate Settings, ensure:</li> <li>Signing Option: Sign SAML response</li> <li>Signing Algorithm: SHA-256</li> </ol>"},{"location":"sso_onboarding/sso_config/#step-5-collect-configuration-details","title":"Step 5: Collect Configuration Details","text":"<p>Gather the following information to provide to our team: - Azure AD Identifier (Entity ID): Found in the Set up [Application Name] section - Login URL: Found in the Set up [Application Name] section - Logout URL: Found in the Set up [Application Name] section - App Federation Metadata URL: From Step 4 - Certificate (Base64): Downloaded file from Step 4 - Federation Metadata XML: Downloaded file from Step 4</p>"},{"location":"sso_onboarding/sso_config/#security-considerations-for-entra-id","title":"Security Considerations for Entra ID","text":""},{"location":"sso_onboarding/sso_config/#conditional-access-policies","title":"Conditional Access Policies","text":"<p>Consider implementing Conditional Access policies for additional security: - Require multi-factor authentication - Restrict access based on location or device compliance - Implement risk-based access controls</p>"}]}