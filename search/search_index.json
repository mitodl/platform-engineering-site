{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Handy Links For Engineering Celery Monitoring Production Concourse Pipelines Production Directory of MIT OL's Applications MIT OL Engineering Github Discussions MIT Online Learning Intranet For Platform Engineering Platform Engineer Oncall Runbook How To Update This Site Grafana Production","title":"Home"},{"location":"#handy-links","text":"","title":"Handy Links"},{"location":"#for-engineering","text":"Celery Monitoring Production Concourse Pipelines Production Directory of MIT OL's Applications MIT OL Engineering Github Discussions MIT Online Learning Intranet","title":"For Engineering"},{"location":"#for-platform-engineering","text":"Platform Engineer Oncall Runbook How To Update This Site Grafana Production","title":"For Platform Engineering"},{"location":"discoveries/fargate_service_discovery/","text":"Integrating Fargate Services Into Our Existing Systems Architecture The Goal We want to start using ECS Fargate from AWS as a low-complexity way of running containerized workloads. It allows us to specify a set of containers that we would like to run as a single task, with simple autoscaling and resource allocation, and no need to manage the underlying servers. At face value this is a great option, even with the slight cost markup that it brings. It is highly likely that we will save at least that much money in engineering time. The Challenges In order for us to properly integrate applications and services into our infrastructure, we need to be able to connect them to our Vault and Consul clusters. This allows us to expose the services that they provide to our other systems, as well as allowing us to simplify the configuration needed to let those applications connect to other system components. In an EC2 environment this is as simple as running a Consul and Vault agent, and setting up DNS routing to use the Consul agent running on localhost. Fargate makes this challenging due to the set of (reasonable) constraints that it places on the tasks that it runs. Among these is the fact that it can only use the DNS server at the VPC level for address resolution. This is due to its use of an Elastic Network Interface (ENI) for network traffic. It is possible for containers to communicate with their peer containers in a task group over localhost connections, but since DNS is a privileged process it is not straightforward to force those queries to rely on a sidecar in the grouping. Because it is not possible to use Consul as the DNS provider, it complicates the configuration of Vault agents for locating the server cluster that it needs to authenticate to. The Solution In order to work around these constraints it is possible to use the AWS Cloud Map service as a means of registering and discovering services across our infrastructure. This in conjunction with the consul-aws application provides a means of setting up a bi-directional sync of services registered in Consul and services registered in AWS Cloud Map . By registering a cloud-map namespace for each of our VPCs and creating an ECS Fargate task to execute the consul-aws synchronization, we can maintain a consistent view of what services are available, regardless of whether they are communicating with the cloud-map DNS or the Consul DNS. Design Challenges While the use of Consul and AWS Cloud Map provides a low-complexity means of keeping things in sync, it does provide some friction in how applications are configured. This is due to the fact that the DNS names for a given service will be different depending on which system is being queried. For a service being discovered via Consul the query would be <service_name>.service.consul , whereas in AWS Cloud-Map it would be <service_name>.<cloud-map_namespace> . This is largely a manageable problem, since it will be primarily Fargate processes that need to interact with cloud-map and most other systems will use Consul, but it will require a clear understanding of which processes are communicating in which contexts.","title":"Integrating Fargate Services Into Our Existing Systems Architecture"},{"location":"discoveries/fargate_service_discovery/#integrating-fargate-services-into-our-existing-systems-architecture","text":"","title":"Integrating Fargate Services Into Our Existing Systems Architecture"},{"location":"discoveries/fargate_service_discovery/#the-goal","text":"We want to start using ECS Fargate from AWS as a low-complexity way of running containerized workloads. It allows us to specify a set of containers that we would like to run as a single task, with simple autoscaling and resource allocation, and no need to manage the underlying servers. At face value this is a great option, even with the slight cost markup that it brings. It is highly likely that we will save at least that much money in engineering time.","title":"The Goal"},{"location":"discoveries/fargate_service_discovery/#the-challenges","text":"In order for us to properly integrate applications and services into our infrastructure, we need to be able to connect them to our Vault and Consul clusters. This allows us to expose the services that they provide to our other systems, as well as allowing us to simplify the configuration needed to let those applications connect to other system components. In an EC2 environment this is as simple as running a Consul and Vault agent, and setting up DNS routing to use the Consul agent running on localhost. Fargate makes this challenging due to the set of (reasonable) constraints that it places on the tasks that it runs. Among these is the fact that it can only use the DNS server at the VPC level for address resolution. This is due to its use of an Elastic Network Interface (ENI) for network traffic. It is possible for containers to communicate with their peer containers in a task group over localhost connections, but since DNS is a privileged process it is not straightforward to force those queries to rely on a sidecar in the grouping. Because it is not possible to use Consul as the DNS provider, it complicates the configuration of Vault agents for locating the server cluster that it needs to authenticate to.","title":"The Challenges"},{"location":"discoveries/fargate_service_discovery/#the-solution","text":"In order to work around these constraints it is possible to use the AWS Cloud Map service as a means of registering and discovering services across our infrastructure. This in conjunction with the consul-aws application provides a means of setting up a bi-directional sync of services registered in Consul and services registered in AWS Cloud Map . By registering a cloud-map namespace for each of our VPCs and creating an ECS Fargate task to execute the consul-aws synchronization, we can maintain a consistent view of what services are available, regardless of whether they are communicating with the cloud-map DNS or the Consul DNS.","title":"The Solution"},{"location":"discoveries/fargate_service_discovery/#design-challenges","text":"While the use of Consul and AWS Cloud Map provides a low-complexity means of keeping things in sync, it does provide some friction in how applications are configured. This is due to the fact that the DNS names for a given service will be different depending on which system is being queried. For a service being discovered via Consul the query would be <service_name>.service.consul , whereas in AWS Cloud-Map it would be <service_name>.<cloud-map_namespace> . This is largely a manageable problem, since it will be primarily Fargate processes that need to interact with cloud-map and most other systems will use Consul, but it will require a clear understanding of which processes are communicating in which contexts.","title":"Design Challenges"},{"location":"how_to/OpenEdX_Prove_Build_Was_Deployed/","text":"Proving That A Particular OpenEdX Build Was Deployed To a Product Environment This doc is NOT exhaustive. It assumes prior knowledge of our products and EC2. I can fill in the exhaustive details later if there's ever time :) There are two methods for determining this. AMI Find the instance in the AWS console. For instance if looking for an XPro CI instance, use edxapp-web-xpro-ci to find an XPro CI webserver. Select it. Click \"AMI\" an select the instance's AMI image. Under the \"Tags\" tab one of the tags is edx_sha . That's the Git repository hash this container is built from. Container Contents Log onto the EC2 instance you want to check. Get a shell inside an LMS/CMS container e.g. cd /etc/docker/compose sudo -s docker compose exec -it cms bash Once inside you can change directory to /openedx/edx-platform and run git log . That will show you the commit that built the container.","title":"Proving That A Particular OpenEdX Build Was Deployed To a Product Environment"},{"location":"how_to/OpenEdX_Prove_Build_Was_Deployed/#proving-that-a-particular-openedx-build-was-deployed-to-a-product-environment","text":"This doc is NOT exhaustive. It assumes prior knowledge of our products and EC2. I can fill in the exhaustive details later if there's ever time :) There are two methods for determining this.","title":"Proving That A Particular OpenEdX Build Was Deployed To a Product Environment"},{"location":"how_to/OpenEdX_Prove_Build_Was_Deployed/#ami","text":"Find the instance in the AWS console. For instance if looking for an XPro CI instance, use edxapp-web-xpro-ci to find an XPro CI webserver. Select it. Click \"AMI\" an select the instance's AMI image. Under the \"Tags\" tab one of the tags is edx_sha . That's the Git repository hash this container is built from.","title":"AMI"},{"location":"how_to/OpenEdX_Prove_Build_Was_Deployed/#container-contents","text":"Log onto the EC2 instance you want to check. Get a shell inside an LMS/CMS container e.g. cd /etc/docker/compose sudo -s docker compose exec -it cms bash Once inside you can change directory to /openedx/edx-platform and run git log . That will show you the commit that built the container.","title":"Container Contents"},{"location":"how_to/access_openedx_djange_manage/","text":"How To Access An OpenEdX Django Admin manage.py Pre-requisites You will need the following pieces of information to get started: - The product you want to access e.g. mitxonline, ocw-studio, xpro, mitx - The environment you want - one of ci, qa, or production. - Valid credentials to login to the MIT Open Learning AWS Account - You should most likely have been given these as a part of your onboarding. - The MIT oldevops AWS key file - oldevops.pem which at the time of this document's writing can be accessed from (TBD: I forget and have asaked my team to remind me.) Finding The Right EC2 Instance Log into the AWS console / web UI with your MIT issued credentials. Click the service selector (the tightly grouped bunch of white square boxes in the upper left) and choose EC2. Now click \"Instances (running)\" You will now see a text box with the prompt \"Find instances by attribute...\" In this box, type 'edxapp-worker- - ' - for instance, for mitxonline production you would type 'edxapp-worker-mitxonline-production' and hit enter/return. You should now see a list of instances named edxapp-worker-mitxonline-production You'll need to temporarily add ssh access to the security group for your instance. Right click on the first instance in the list and pick Security, then Change Security Groups. Type 'ssh' into the 'add security groups' text box. You should see a group appropriate to your product, in the case of mitxonline-production I see mitxonline-production-public-ssh. Select that group and click \"Add security group\" then click the orange Save button at the bottom of the page. Now, left click on the first instance in the list which should expand into instance detail. Click the little square within a square Copy icon next to the \"Public IPV4 Address\". Making The Connection From your laptop, use the oldevops.pem key to ssh to the ubuntu user on the machine whose IP you copied from the previous step. So for instance: ssh -i oldevops.pem ubuntu@34.204.173.109 At this point you should be good to go and should see a prompt that looks something like: ubuntu@ip-10-22-3-162:~$","title":"How To Access An OpenEdX Django Admin manage.py"},{"location":"how_to/access_openedx_djange_manage/#how-to-access-an-openedx-django-admin-managepy","text":"","title":"How To Access An OpenEdX Django Admin manage.py"},{"location":"how_to/access_openedx_djange_manage/#pre-requisites","text":"You will need the following pieces of information to get started: - The product you want to access e.g. mitxonline, ocw-studio, xpro, mitx - The environment you want - one of ci, qa, or production. - Valid credentials to login to the MIT Open Learning AWS Account - You should most likely have been given these as a part of your onboarding. - The MIT oldevops AWS key file - oldevops.pem which at the time of this document's writing can be accessed from (TBD: I forget and have asaked my team to remind me.)","title":"Pre-requisites"},{"location":"how_to/access_openedx_djange_manage/#finding-the-right-ec2-instance","text":"Log into the AWS console / web UI with your MIT issued credentials. Click the service selector (the tightly grouped bunch of white square boxes in the upper left) and choose EC2. Now click \"Instances (running)\" You will now see a text box with the prompt \"Find instances by attribute...\" In this box, type 'edxapp-worker- - ' - for instance, for mitxonline production you would type 'edxapp-worker-mitxonline-production' and hit enter/return. You should now see a list of instances named edxapp-worker-mitxonline-production You'll need to temporarily add ssh access to the security group for your instance. Right click on the first instance in the list and pick Security, then Change Security Groups. Type 'ssh' into the 'add security groups' text box. You should see a group appropriate to your product, in the case of mitxonline-production I see mitxonline-production-public-ssh. Select that group and click \"Add security group\" then click the orange Save button at the bottom of the page. Now, left click on the first instance in the list which should expand into instance detail. Click the little square within a square Copy icon next to the \"Public IPV4 Address\".","title":"Finding The Right EC2 Instance"},{"location":"how_to/access_openedx_djange_manage/#making-the-connection","text":"From your laptop, use the oldevops.pem key to ssh to the ubuntu user on the machine whose IP you copied from the previous step. So for instance: ssh -i oldevops.pem ubuntu@34.204.173.109 At this point you should be good to go and should see a prompt that looks something like: ubuntu@ip-10-22-3-162:~$","title":"Making The Connection"},{"location":"how_to/concourse_github_issues_user_guide/","text":"Concourse github Issues Workflow User Guide The Problem Here at MIT OL we use Concourse CI . It's an incredibly powerful package for managing complex continuous integration workflows, and we leverage its power in all kinds of interesting ways. What that means for you, the developer however is that our pipelines can be difficult to get one's head around and understand. As an example, this is our edx platform meta-pipeline. The Ask Most of the time, there are exactly two questions software developers wants answered when it comes to deploying their software: How can I tell when my code has been deployed to $X? How can I trigger my code to be deployed to $X? The Solution Thankfully, our director Tobias came up with an excellent and novel solution. What's one of the most common non code mechanism developers use to govern their work? Github Issues! A Traffic Light For Deploys Now that you've indulged me with a full page of setup. Let's get down to brass tacks and answer those two most common questions devs have about their deploys: How can I tell when my code has been deployed to $X? We're currently keeping the Github Issues that govern our pipelines on MIT's internal Github, because the public one has throttles that were shooting us in the foot :) So, take a look at the issues for the concourse-workflow repo. Right now, that page looks something like this (simplified view): [bot] Pulumi ol-infrastructure-vault-encryption_mounts substructure.vault.encryption_mounts.operations.Production deployed. DevOps finalized-deployment pipeline-workflow product:infrastructure #1427 opened yesterday by tmacey [bot] Pulumi ol-infrastructure-forum-server applications.forum.xpro.Production deployed. DevOps finalized-deployment pipeline-workflow product:infrastructure #1426 opened yesterday by tmacey [bot] Pulumi ol-infrastructure-dagster-server applications.dagster.QA deployed. DevOps pipeline-workflow product:infrastructure promotion-to-production #1425 opened yesterday by tmacey Let's say I'm someone on the data platform team and I'm wondering whether or not my changes have been deployed in the Dagster project. Aha! I look down the list and see that the Dagster project has been deployed to QA, but not to production. Let's click on that issue and take a look. At the time of this writing, this issue is Open, which means concourse Github issues is waiting for us to tell it that we're ready to deploy these changes to production. We can see everything this deployment would contain because for each Concourse build that's happened since this issue was created, a comment was added along with the build log. If we're ready to move these changes to production, we just Close this issue. That's all there is to it! If you want to watch your change's progress, just click one of those build log links which will bring you to the pipeline where that's happening. You should see a new build in progress. Just click that tab to see the details. And that's all there is to know! It's a simple system for busy people. If you have any questions, don't hesitate to reach out to @sre on slack or send us E-mail at oldevops@mit.edu Thanks for taking the time to read this doc! Obviously feel free to suggest any improvements or let me know if anything's unclear.","title":"Concourse github Issues Workflow User Guide"},{"location":"how_to/concourse_github_issues_user_guide/#concourse-github-issues-workflow-user-guide","text":"","title":"Concourse github Issues Workflow User Guide"},{"location":"how_to/concourse_github_issues_user_guide/#the-problem","text":"Here at MIT OL we use Concourse CI . It's an incredibly powerful package for managing complex continuous integration workflows, and we leverage its power in all kinds of interesting ways. What that means for you, the developer however is that our pipelines can be difficult to get one's head around and understand. As an example, this is our edx platform meta-pipeline.","title":"The Problem"},{"location":"how_to/concourse_github_issues_user_guide/#the-ask","text":"Most of the time, there are exactly two questions software developers wants answered when it comes to deploying their software: How can I tell when my code has been deployed to $X? How can I trigger my code to be deployed to $X?","title":"The Ask"},{"location":"how_to/concourse_github_issues_user_guide/#the-solution","text":"Thankfully, our director Tobias came up with an excellent and novel solution. What's one of the most common non code mechanism developers use to govern their work? Github Issues!","title":"The Solution"},{"location":"how_to/concourse_github_issues_user_guide/#a-traffic-light-for-deploys","text":"Now that you've indulged me with a full page of setup. Let's get down to brass tacks and answer those two most common questions devs have about their deploys: How can I tell when my code has been deployed to $X? We're currently keeping the Github Issues that govern our pipelines on MIT's internal Github, because the public one has throttles that were shooting us in the foot :) So, take a look at the issues for the concourse-workflow repo. Right now, that page looks something like this (simplified view): [bot] Pulumi ol-infrastructure-vault-encryption_mounts substructure.vault.encryption_mounts.operations.Production deployed. DevOps finalized-deployment pipeline-workflow product:infrastructure #1427 opened yesterday by tmacey [bot] Pulumi ol-infrastructure-forum-server applications.forum.xpro.Production deployed. DevOps finalized-deployment pipeline-workflow product:infrastructure #1426 opened yesterday by tmacey [bot] Pulumi ol-infrastructure-dagster-server applications.dagster.QA deployed. DevOps pipeline-workflow product:infrastructure promotion-to-production #1425 opened yesterday by tmacey Let's say I'm someone on the data platform team and I'm wondering whether or not my changes have been deployed in the Dagster project. Aha! I look down the list and see that the Dagster project has been deployed to QA, but not to production. Let's click on that issue and take a look. At the time of this writing, this issue is Open, which means concourse Github issues is waiting for us to tell it that we're ready to deploy these changes to production. We can see everything this deployment would contain because for each Concourse build that's happened since this issue was created, a comment was added along with the build log. If we're ready to move these changes to production, we just Close this issue. That's all there is to it! If you want to watch your change's progress, just click one of those build log links which will bring you to the pipeline where that's happening. You should see a new build in progress. Just click that tab to see the details. And that's all there is to know! It's a simple system for busy people. If you have any questions, don't hesitate to reach out to @sre on slack or send us E-mail at oldevops@mit.edu Thanks for taking the time to read this doc! Obviously feel free to suggest any improvements or let me know if anything's unclear.","title":"A Traffic Light For Deploys"},{"location":"how_to/consul_incantations/","text":"Misbehaving consul cluster incident from 20231004-20231905 Overview At some point operations-production consul cluster fell over and stopped being useful. The root cause was not immediately know but was almost definitely related to an upgrade to 1.16.2 from ??? (no good record indication what the cluster was on before all this went down...) Key takeaway from the logs was this cryptic message about being unable to restore snapshot: {\"@level\":\"error\",\"@message\":\"failed to restore snapshot\",\"@module\":\"agent.server.raft\",\"@timestamp\":\"2023-10-04T14:17:40.200037Z\",\"error\":\"failed to restore snapshot 1156-86945697-1696429059987: failed inserting acl token: missing value for index 'accessor'\"} This is something that happens anytime a consul server restarts, it gets a copy of the raft from the other servers currently running and restores it. But, it is failing to do that and crashing. Initial Response Tobias was able to revive the cluster by downgrading it to 1.14.10 and it was then able to restore the snapshot it received from other nodes / on the filesystem (unclear how broken the cluster was at this time). Research Looking up that message returned one very-not-promising result from the hashicorp forums. Resolution Ultimitely spent a lot of time reading and pursuing dead ends but what I believe ulimitely resolved the issue was the following: Step through each consul server in the cluster and ensure: a. it is on version 1.14.10 b. It has this acl stanza in 00-default.json : \"acl\": {\"enabled\": true, \"default_policy\": \"allow\", \"enable_token_persistence\": false} c. Restart the servers one at a time to ensure the quorum never drops below 3 (or 2 if you're in non-prod). Now you should be able to issue acl commands using the consul cli. a. They won't work though because you don't have a token and in this particular case the cluster says it is not elgible for bootstrapping. b. At some point in history this very special cluster had ACLs enabled and then disabled? Possibly? We need to reset/recover the ACL master token. Follow the procedure here . The output of that command should be \"Bootstrap Token (Global Management)\" an AccessorID and SecretID . Export the secret ID in your terminal as CONSUL_HTTP_TOKEN . Then you can do consul acl token list and one of them should be labeled \"Master Token\". a. export the SecretID from the Master Token as CONSUL_HTTP_TOKEN or just save it off somewhere. Repeat step 1, loop through all the servers and remove the acl stanza, restarting one at a time to ensure quorum. Once all nodes are running again and ACL is disabled, start upgrading the nodes. one at a time, to 1.16.2. wget https://releases.hashicorp.com/consul/1.16.2/consul_1.16.2_linux_amd64.zip unzip consul_1.16.2_linux_amd64.zip mv consul consul_1_16_2 systemctl stop consul cp consul_1_16_2 /usr/local/bin/consul cd /var/lib cp -r consul consul_bak cd consul rm -rf serf/ raft/ server_metadata.json checkpoint-signature systemctl start consul Verify quorum: consul operator raft list-peers Verify version: consul members and consul version","title":"Misbehaving consul cluster incident from 20231004-20231905"},{"location":"how_to/consul_incantations/#misbehaving-consul-cluster-incident-from-20231004-20231905","text":"","title":"Misbehaving consul cluster incident from 20231004-20231905"},{"location":"how_to/consul_incantations/#overview","text":"At some point operations-production consul cluster fell over and stopped being useful. The root cause was not immediately know but was almost definitely related to an upgrade to 1.16.2 from ??? (no good record indication what the cluster was on before all this went down...) Key takeaway from the logs was this cryptic message about being unable to restore snapshot: {\"@level\":\"error\",\"@message\":\"failed to restore snapshot\",\"@module\":\"agent.server.raft\",\"@timestamp\":\"2023-10-04T14:17:40.200037Z\",\"error\":\"failed to restore snapshot 1156-86945697-1696429059987: failed inserting acl token: missing value for index 'accessor'\"} This is something that happens anytime a consul server restarts, it gets a copy of the raft from the other servers currently running and restores it. But, it is failing to do that and crashing.","title":"Overview"},{"location":"how_to/consul_incantations/#initial-response","text":"Tobias was able to revive the cluster by downgrading it to 1.14.10 and it was then able to restore the snapshot it received from other nodes / on the filesystem (unclear how broken the cluster was at this time).","title":"Initial Response"},{"location":"how_to/consul_incantations/#research","text":"Looking up that message returned one very-not-promising result from the hashicorp forums.","title":"Research"},{"location":"how_to/consul_incantations/#resolution","text":"Ultimitely spent a lot of time reading and pursuing dead ends but what I believe ulimitely resolved the issue was the following: Step through each consul server in the cluster and ensure: a. it is on version 1.14.10 b. It has this acl stanza in 00-default.json : \"acl\": {\"enabled\": true, \"default_policy\": \"allow\", \"enable_token_persistence\": false} c. Restart the servers one at a time to ensure the quorum never drops below 3 (or 2 if you're in non-prod). Now you should be able to issue acl commands using the consul cli. a. They won't work though because you don't have a token and in this particular case the cluster says it is not elgible for bootstrapping. b. At some point in history this very special cluster had ACLs enabled and then disabled? Possibly? We need to reset/recover the ACL master token. Follow the procedure here . The output of that command should be \"Bootstrap Token (Global Management)\" an AccessorID and SecretID . Export the secret ID in your terminal as CONSUL_HTTP_TOKEN . Then you can do consul acl token list and one of them should be labeled \"Master Token\". a. export the SecretID from the Master Token as CONSUL_HTTP_TOKEN or just save it off somewhere. Repeat step 1, loop through all the servers and remove the acl stanza, restarting one at a time to ensure quorum. Once all nodes are running again and ACL is disabled, start upgrading the nodes. one at a time, to 1.16.2. wget https://releases.hashicorp.com/consul/1.16.2/consul_1.16.2_linux_amd64.zip unzip consul_1.16.2_linux_amd64.zip mv consul consul_1_16_2 systemctl stop consul cp consul_1_16_2 /usr/local/bin/consul cd /var/lib cp -r consul consul_bak cd consul rm -rf serf/ raft/ server_metadata.json checkpoint-signature systemctl start consul Verify quorum: consul operator raft list-peers Verify version: consul members and consul version","title":"Resolution"},{"location":"how_to/deploying_concourse/","text":"Deploying Concourse Summary This document will detail the best practice we use to develop and deploy changes to our Concourse pipeline web and worker servers. Developing As with most projects here at MIT OL's Devops team, you'll want to start by checking out the ol-infrastructure project to your local workspace. As usual, be sure to run poetry install so all the right dependencies will be cached and ready to run your newly changed pyinfra code. Now change directory to the src/bilder/images/concourse folder. Here you'll see a number of files. The most important for our purposes is deploy.py . Very likely, any chances you might want to make will be in this file. Local Testing Apparently I'm alone on this bus, but I personally enjoy testing changes on my machine before I commit them to Git. The testing doesn't have to be deep, something like syntax and being end to end runnable are good enough for me. In any case, in order to locally build the docker container, run the following command: pyinfra @docker/debian:bookworm deploy.py NOTA BENE : I am not entirely sure about the Debian Bookworm base container I'm using here, it worked well enough for my very shallow testing purposes. TODO : Figure out what we actually use for a base container and cite that in this doc. If you have a syntax error or an error in your pyinfra code that prevents the container from building, you will see that on your screen and can debug it accordingly. Otherwise, you'll see a message about the container building successfully. Build The Image Now that we've very shallowly 'smoke tested' our code, let's run the actual official script to build the AMI with packer and stage it for later deployment. From the same directory, run the following command: pr packer build -on-error=ask -var node_type=web -var app_name=concourse -only amazon-ebs.third-party src/bilder/images You will see a LOT of output and the process could take up to 20 minutes. Initial Testing in CI For most of the rest of the journey to production, you'll be using a pipeline rather than doing it by hand with Pulumi, but for your first time, you'll probably want to do it by hand and then minutely monitor the resultant deployed image in CI to ensure everything went smoothly. TODO: Add curl smoke test for web node Deployment to QA and Beyond For the remainder of your change's journey through QA and ultimately to production, you'll be using the Concourse pipeline . TODO: Add further testing details","title":"Deploying Concourse"},{"location":"how_to/deploying_concourse/#deploying-concourse","text":"","title":"Deploying Concourse"},{"location":"how_to/deploying_concourse/#summary","text":"This document will detail the best practice we use to develop and deploy changes to our Concourse pipeline web and worker servers.","title":"Summary"},{"location":"how_to/deploying_concourse/#developing","text":"As with most projects here at MIT OL's Devops team, you'll want to start by checking out the ol-infrastructure project to your local workspace. As usual, be sure to run poetry install so all the right dependencies will be cached and ready to run your newly changed pyinfra code. Now change directory to the src/bilder/images/concourse folder. Here you'll see a number of files. The most important for our purposes is deploy.py . Very likely, any chances you might want to make will be in this file.","title":"Developing"},{"location":"how_to/deploying_concourse/#local-testing","text":"Apparently I'm alone on this bus, but I personally enjoy testing changes on my machine before I commit them to Git. The testing doesn't have to be deep, something like syntax and being end to end runnable are good enough for me. In any case, in order to locally build the docker container, run the following command: pyinfra @docker/debian:bookworm deploy.py NOTA BENE : I am not entirely sure about the Debian Bookworm base container I'm using here, it worked well enough for my very shallow testing purposes. TODO : Figure out what we actually use for a base container and cite that in this doc. If you have a syntax error or an error in your pyinfra code that prevents the container from building, you will see that on your screen and can debug it accordingly. Otherwise, you'll see a message about the container building successfully.","title":"Local Testing"},{"location":"how_to/deploying_concourse/#build-the-image","text":"Now that we've very shallowly 'smoke tested' our code, let's run the actual official script to build the AMI with packer and stage it for later deployment. From the same directory, run the following command: pr packer build -on-error=ask -var node_type=web -var app_name=concourse -only amazon-ebs.third-party src/bilder/images You will see a LOT of output and the process could take up to 20 minutes.","title":"Build The Image"},{"location":"how_to/deploying_concourse/#initial-testing-in-ci","text":"For most of the rest of the journey to production, you'll be using a pipeline rather than doing it by hand with Pulumi, but for your first time, you'll probably want to do it by hand and then minutely monitor the resultant deployed image in CI to ensure everything went smoothly. TODO: Add curl smoke test for web node","title":"Initial Testing in CI"},{"location":"how_to/deploying_concourse/#deployment-to-qa-and-beyond","text":"For the remainder of your change's journey through QA and ultimately to production, you'll be using the Concourse pipeline . TODO: Add further testing details","title":"Deployment to QA and Beyond"},{"location":"how_to/enable_rapid_response_edxapp/","text":"Enable Rapid Responde in edxapp Reference https://github.com/mitodl/rapid-response-xblock/blob/master/README.md How-To Common env.yaml file must have FEATURES:ALLOW_ALL_ADVANCED_COMPONENTS: True Common env.yaml file must have ENABLE_RAPID_RESPONSE_AUTHOR_VIEW: True Rapid-response package pip packages must be installed in the image rapid-response-xblock and ol-openedx-rapid-response-reports . Refer to existing package listings in the openedx docker configs. In the LMS Admin UI -> /admin/lms_xblock/xblockasidesconfig/ create an enabled record. In the CMS Admin UI -> /admin/xblock_config/studioconfig/ create an 'enabled' record. Verify via studio on a test/demo course, find an existing multiple choice problem or create a new one. After creation in the 'unit view', you should now have a checkbox at the bottom of a multiple choice problem that will say 'Enable problem for rapid-response'.","title":"Enable Rapid Responde in edxapp"},{"location":"how_to/enable_rapid_response_edxapp/#enable-rapid-responde-in-edxapp","text":"","title":"Enable Rapid Responde in edxapp"},{"location":"how_to/enable_rapid_response_edxapp/#reference","text":"https://github.com/mitodl/rapid-response-xblock/blob/master/README.md","title":"Reference"},{"location":"how_to/enable_rapid_response_edxapp/#how-to","text":"Common env.yaml file must have FEATURES:ALLOW_ALL_ADVANCED_COMPONENTS: True Common env.yaml file must have ENABLE_RAPID_RESPONSE_AUTHOR_VIEW: True Rapid-response package pip packages must be installed in the image rapid-response-xblock and ol-openedx-rapid-response-reports . Refer to existing package listings in the openedx docker configs. In the LMS Admin UI -> /admin/lms_xblock/xblockasidesconfig/ create an enabled record. In the CMS Admin UI -> /admin/xblock_config/studioconfig/ create an 'enabled' record. Verify via studio on a test/demo course, find an existing multiple choice problem or create a new one. After creation in the 'unit view', you should now have a checkbox at the bottom of a multiple choice problem that will say 'Enable problem for rapid-response'.","title":"How-To"},{"location":"how_to/faking_it_with_consul_and_vault/","text":"Faking it with Consul and Vault Why? Sometimes you want to test an AMI build process without all all the supporting stuff that allows an EC2 instance to talk to vault and consul like a good little box. All that supporting stuff typically happens are instance start time and rides in on user_data which you don't have if you fired up an EC2 by hand just to see what your AMI looks like so far. So, for development purposes here is an abbreviated guide of faking it until you make it. Consul From a like-minded instance, find the proper IAM instance profile ARN. In my case I'm making a new type of EC2 box for edxapp / mite-staging / ci, so I went to one of the existing boxes for that environment, copied the Instance Profile ARN and associated it to my new, one-off, manually created EC2 box. This is essential. Second, from the same like-minded instance, nab /etc/consul.d/99-autojoin.json . {\"retry_join\": [\"provider=aws tag_key=consul_env tag_value=mitx-staging-ci\"], \"datacenter\": \"mitx-staging-ci\"} Copy that config to your one-off box and restart consul and confirm you are now joined to the cluster with consul catalog nodes or whatever you fancy. Vault This requires using vault >= 1.13.x which introduced the token_file auto_auth method. On your one-off EC2 box, backup the existing /etc/vault/vault.json file and change the auto_auth block to be like the following: \"auto_auth\": { \"method\": { \"type\": \"token_file\", \"config\": { \"token_file_path\": \"/etc/vault/vault_token\" } }, \"sink\": [ { \"type\": \"file\", \"derive_key\": false, \"config\": [ { \"path\": \"/etc/vault/vault_agent_token\" } ] } ] }, In the vault UI, top right side little man icon, click 'Copy Token' and put that into /etc/vault/vault_token echo -n \"<token>\" > /etc/vault/vault_token Restart vault and verify that it is happy and healthy. Disclaimer You basically just gave this node a root token to vault ... so, you know ... don't do this anywhere besides CI and don't leave it hanging around. This is just for debugging.","title":"Faking it with Consul and Vault"},{"location":"how_to/faking_it_with_consul_and_vault/#faking-it-with-consul-and-vault","text":"","title":"Faking it with Consul and Vault"},{"location":"how_to/faking_it_with_consul_and_vault/#why","text":"Sometimes you want to test an AMI build process without all all the supporting stuff that allows an EC2 instance to talk to vault and consul like a good little box. All that supporting stuff typically happens are instance start time and rides in on user_data which you don't have if you fired up an EC2 by hand just to see what your AMI looks like so far. So, for development purposes here is an abbreviated guide of faking it until you make it.","title":"Why?"},{"location":"how_to/faking_it_with_consul_and_vault/#consul","text":"From a like-minded instance, find the proper IAM instance profile ARN. In my case I'm making a new type of EC2 box for edxapp / mite-staging / ci, so I went to one of the existing boxes for that environment, copied the Instance Profile ARN and associated it to my new, one-off, manually created EC2 box. This is essential. Second, from the same like-minded instance, nab /etc/consul.d/99-autojoin.json . {\"retry_join\": [\"provider=aws tag_key=consul_env tag_value=mitx-staging-ci\"], \"datacenter\": \"mitx-staging-ci\"} Copy that config to your one-off box and restart consul and confirm you are now joined to the cluster with consul catalog nodes or whatever you fancy.","title":"Consul"},{"location":"how_to/faking_it_with_consul_and_vault/#vault","text":"This requires using vault >= 1.13.x which introduced the token_file auto_auth method. On your one-off EC2 box, backup the existing /etc/vault/vault.json file and change the auto_auth block to be like the following: \"auto_auth\": { \"method\": { \"type\": \"token_file\", \"config\": { \"token_file_path\": \"/etc/vault/vault_token\" } }, \"sink\": [ { \"type\": \"file\", \"derive_key\": false, \"config\": [ { \"path\": \"/etc/vault/vault_agent_token\" } ] } ] }, In the vault UI, top right side little man icon, click 'Copy Token' and put that into /etc/vault/vault_token echo -n \"<token>\" > /etc/vault/vault_token Restart vault and verify that it is happy and healthy.","title":"Vault"},{"location":"how_to/faking_it_with_consul_and_vault/#disclaimer","text":"You basically just gave this node a root token to vault ... so, you know ... don't do this anywhere besides CI and don't leave it hanging around. This is just for debugging.","title":"Disclaimer"},{"location":"how_to/getting_started_with_concourse/","text":"Getting Started Building Concourse Pipelines in Python Getting Set Up Use the Tutorial on the Concourse website under Docs->Getting Started to get set up. Do NOT use any other tutorials you find linked elsewhere. Some are out of date and the failure modes can be incredibly hard to debug! If you're using an Arm64 Mac (e.g. M1, M2, etc.) just substitute this platform specific image for the one cited in the tutorial. So the first line of your docker-compose.yml should look something like this: image: rdclda/concourse:7.7.1 Nota Bene: You should probably use the latest version of this. At the time of this writing the latest has a bug but that will likely be fixed by the time anyone reads this. You'll also need to install Docker Desktop for Mac. Be sure that at the end of this process you have a Concourse instance running locally in your Docker install. If you didn't end up running docker-compose up -d or similar then you may have missed something. Watch for errors and check that your newly launched Concourse is healthy and that you can browse to the localhost URL cited in the tutorial which should get you the Concourse main page with the giant spinning turbine blade icon. Now We're Cooking with Gas! Learning How Concourse Works By Building Pipelines Work through the entire tutorial including building and actually creating pipelines from all the examples. This is critical so you'll have an understanding of what all the component parts are and how they fit together as you code your Python in later steps. Actually make a point of working with some of the later examples involving Git repos. Commit some chnges to your test repo and watch them flow through the pipeline. Pretty neat eh? You may notice that some of the later examples get pretty unwieldy and become difficult to get right. It can be tricky figuring out what level of indentation is correct just by eyeballing it. You might find the yamllint utility helpful for this as it will tell you when there are syntax errors. Ignore its whining about improper indent levels and focus on the errors :) It Gets Easier - Building Pipelines in Python Thankfully, we have been spared the pain of coding pipelines in YAML by virtue of a Python wrapper that Tobias Macey wrote. Each YAML section is wrapped in a Python object. It's a 1 to 1 mapping because the Python models are auto-generated from the schema defined by Concourse. Tahe a look at the simplest hello-world tutorial example converted into Python here . I've put the explanatory comments inline to make it easier to understand what's going on. Actually Building a Pipeline From Your Python Our Concourse pipeline Python build scripts, like everything else we maintain, is managed by Poetry . So, to actually invoke our hello world script and get ready to actually create the pipeline, we run: poetry run python3 ./hello.py This will emit the JSON our Python produces, along with an actual Concourse fly invocation at the end. This assumes you're already properly logged into your Concourse instance. For our purposes use the same one you set up previously to run through the tutorials. The output should look something like this: { \"jobs\": [ { \"build_logs_to_retain\": null, \"max_in_flight\": 1.0, \"serial\": null, \"old_name\": null, \"on_success\": null, \"ensure\": null, \"on_error\": null, \"disable_manual_trigger\": null, \"serial_groups\": null, \"build_log_retention\": null, \"name\": \"deploy-hello-world\", \"plan\": [ { \"config\": { \"image_resource\": { \"source\": { \"repository\": \"busybox\", \"tag\": \"latest\" }, \"params\": null, \"version\": null, \"type\": \"registry-image\" }, \"caches\": null, \"run\": { \"args\": [ \"Hello, World!\" ], \"user\": null, \"path\": \"echo\", \"dir\": null }, \"inputs\": null, \"platform\": \"linux\", \"params\": null, \"container_limits\": null, \"outputs\": null, \"rootfs_uri\": null }, \"file\": null, \"params\": null, \"task\": \"hello-task\", \"privileged\": null, \"vars\": null, \"output_mapping\": null, \"image\": null, \"input_mapping\": null, \"container_limits\": null } ], \"interruptible\": null, \"public\": null, \"on_failure\": null, \"on_abort\": null } ], \"groups\": null, \"resource_types\": null, \"var_sources\": null, \"display\": null, \"resources\": null } fly -t pr-inf sp -p misc-cloud-hello -c definition.json Since we'll be using fly to create our pipeline in the tutorial Concourse instance, use -t tutorial rather than pr-inf and make any other necessary substitutions according to your environment. That's it! You should now have the basic Hello World pipeline created from your Python source and operating properly, printing that famous phrase as a result. TODO Add a more meaty example like the the-artifact example so we get to show inputs and outputs.","title":"Getting Started Building Concourse Pipelines in Python"},{"location":"how_to/getting_started_with_concourse/#getting-started-building-concourse-pipelines-in-python","text":"","title":"Getting Started Building Concourse Pipelines in Python"},{"location":"how_to/getting_started_with_concourse/#getting-set-up","text":"Use the Tutorial on the Concourse website under Docs->Getting Started to get set up. Do NOT use any other tutorials you find linked elsewhere. Some are out of date and the failure modes can be incredibly hard to debug! If you're using an Arm64 Mac (e.g. M1, M2, etc.) just substitute this platform specific image for the one cited in the tutorial. So the first line of your docker-compose.yml should look something like this: image: rdclda/concourse:7.7.1 Nota Bene: You should probably use the latest version of this. At the time of this writing the latest has a bug but that will likely be fixed by the time anyone reads this. You'll also need to install Docker Desktop for Mac. Be sure that at the end of this process you have a Concourse instance running locally in your Docker install. If you didn't end up running docker-compose up -d or similar then you may have missed something. Watch for errors and check that your newly launched Concourse is healthy and that you can browse to the localhost URL cited in the tutorial which should get you the Concourse main page with the giant spinning turbine blade icon.","title":"Getting Set Up"},{"location":"how_to/getting_started_with_concourse/#now-were-cooking-with-gas-learning-how-concourse-works-by-building-pipelines","text":"Work through the entire tutorial including building and actually creating pipelines from all the examples. This is critical so you'll have an understanding of what all the component parts are and how they fit together as you code your Python in later steps. Actually make a point of working with some of the later examples involving Git repos. Commit some chnges to your test repo and watch them flow through the pipeline. Pretty neat eh? You may notice that some of the later examples get pretty unwieldy and become difficult to get right. It can be tricky figuring out what level of indentation is correct just by eyeballing it. You might find the yamllint utility helpful for this as it will tell you when there are syntax errors. Ignore its whining about improper indent levels and focus on the errors :)","title":"Now We're Cooking with Gas! Learning How Concourse Works By Building Pipelines"},{"location":"how_to/getting_started_with_concourse/#it-gets-easier-building-pipelines-in-python","text":"Thankfully, we have been spared the pain of coding pipelines in YAML by virtue of a Python wrapper that Tobias Macey wrote. Each YAML section is wrapped in a Python object. It's a 1 to 1 mapping because the Python models are auto-generated from the schema defined by Concourse. Tahe a look at the simplest hello-world tutorial example converted into Python here . I've put the explanatory comments inline to make it easier to understand what's going on.","title":"It Gets Easier - Building Pipelines in Python"},{"location":"how_to/getting_started_with_concourse/#actually-building-a-pipeline-from-your-python","text":"Our Concourse pipeline Python build scripts, like everything else we maintain, is managed by Poetry . So, to actually invoke our hello world script and get ready to actually create the pipeline, we run: poetry run python3 ./hello.py This will emit the JSON our Python produces, along with an actual Concourse fly invocation at the end. This assumes you're already properly logged into your Concourse instance. For our purposes use the same one you set up previously to run through the tutorials. The output should look something like this: { \"jobs\": [ { \"build_logs_to_retain\": null, \"max_in_flight\": 1.0, \"serial\": null, \"old_name\": null, \"on_success\": null, \"ensure\": null, \"on_error\": null, \"disable_manual_trigger\": null, \"serial_groups\": null, \"build_log_retention\": null, \"name\": \"deploy-hello-world\", \"plan\": [ { \"config\": { \"image_resource\": { \"source\": { \"repository\": \"busybox\", \"tag\": \"latest\" }, \"params\": null, \"version\": null, \"type\": \"registry-image\" }, \"caches\": null, \"run\": { \"args\": [ \"Hello, World!\" ], \"user\": null, \"path\": \"echo\", \"dir\": null }, \"inputs\": null, \"platform\": \"linux\", \"params\": null, \"container_limits\": null, \"outputs\": null, \"rootfs_uri\": null }, \"file\": null, \"params\": null, \"task\": \"hello-task\", \"privileged\": null, \"vars\": null, \"output_mapping\": null, \"image\": null, \"input_mapping\": null, \"container_limits\": null } ], \"interruptible\": null, \"public\": null, \"on_failure\": null, \"on_abort\": null } ], \"groups\": null, \"resource_types\": null, \"var_sources\": null, \"display\": null, \"resources\": null } fly -t pr-inf sp -p misc-cloud-hello -c definition.json Since we'll be using fly to create our pipeline in the tutorial Concourse instance, use -t tutorial rather than pr-inf and make any other necessary substitutions according to your environment. That's it! You should now have the basic Hello World pipeline created from your Python source and operating properly, printing that famous phrase as a result. TODO Add a more meaty example like the the-artifact example so we get to show inputs and outputs.","title":"Actually Building a Pipeline From Your Python"},{"location":"how_to/heroku_config_vars/","text":"Managing Heroku Config Vars with Pulumi heroku.app.ConfigAssociation The resource/mechanism we are using to manage config vars in Heroku is called a 'ConfigAssociation' which is documented (here)[https://www.pulumi.com/registry/packages/heroku/api-docs/app/configassociation/]. A ConfigAssociation takes in an application ID and two sets of variable maps: sensitive_vars and vars . The only difference being that sensitive_vars will not be output during up operations. Required config for the pulumi provider The pulumiverse_heroku provider requries a configuration item in the pulumi stacks named heroku:apiKey . We don't want to have to encrypt that api key in two dozen different stacks so we wrapped our provider config with a setup_heroku_provider() function much the same way we do vault. This can be seen (here)[https://github.com/mitodl/ol-infrastructure/blob/main/src/ol_infrastructure/lib/heroku.py]. In lieu of setting heroku:apiKey in ever stack, we can set heroku:user which this function will then do a lookup in the backgroun out of sops config to get the apporpriate apiKey value. Four Flavors of Vars While we don't yet have a component resource or abstraction available for setting up Config Vars in a simpler fashion, we do have a basic blueprint available with the MITOpen application. Unchanging Values These are not really variables because they represent Key:Value mappings that are unchanging between environments. That is, Production and QA will have the same value set for the same environment. These values are specified directly in the python code under heroku_vars Simple Environment Specific Vars These are simple 1-to-1 mappings from a value stored in the Pulumi configuration under heroku_app:vars: . This map contains the variable names, in their final forms using all-caps, and the static values that are applicable to the environment. Interpolated Environment Specific Vars These are key:value mappings that are used in more complicated manners than a simple 1-to-1 mapping as with the simple settings. These values are stored in the Pulumi configuration under heroku_app:interpolated_vars: in lower-case, signifying that they do not directly become Config Vars in Heroku. These more involved interpolations take place during the construction of the heroku_interpolated_vars dictionary. Secrets Many Config Vars that we use represent values that can be considered secret or otherwise sensitive and should not be publicly disclosed. Nothing from these vars is derived from values stored in the Pulumi configuration, rather they are obtained either from SOPS config or directly from vault at stack application time. Secrets are complicated to work with and it is best to reach out to DevOps for assistence in getting your new secret configuration var setup.","title":"Heroku config vars"},{"location":"how_to/heroku_config_vars/#managing-heroku-config-vars-with-pulumi","text":"","title":"Managing Heroku Config Vars with Pulumi"},{"location":"how_to/heroku_config_vars/#herokuappconfigassociation","text":"The resource/mechanism we are using to manage config vars in Heroku is called a 'ConfigAssociation' which is documented (here)[https://www.pulumi.com/registry/packages/heroku/api-docs/app/configassociation/]. A ConfigAssociation takes in an application ID and two sets of variable maps: sensitive_vars and vars . The only difference being that sensitive_vars will not be output during up operations.","title":"heroku.app.ConfigAssociation"},{"location":"how_to/heroku_config_vars/#required-config-for-the-pulumi-provider","text":"The pulumiverse_heroku provider requries a configuration item in the pulumi stacks named heroku:apiKey . We don't want to have to encrypt that api key in two dozen different stacks so we wrapped our provider config with a setup_heroku_provider() function much the same way we do vault. This can be seen (here)[https://github.com/mitodl/ol-infrastructure/blob/main/src/ol_infrastructure/lib/heroku.py]. In lieu of setting heroku:apiKey in ever stack, we can set heroku:user which this function will then do a lookup in the backgroun out of sops config to get the apporpriate apiKey value.","title":"Required config for the pulumi provider"},{"location":"how_to/heroku_config_vars/#four-flavors-of-vars","text":"While we don't yet have a component resource or abstraction available for setting up Config Vars in a simpler fashion, we do have a basic blueprint available with the MITOpen application.","title":"Four Flavors of Vars"},{"location":"how_to/heroku_config_vars/#unchanging-values","text":"These are not really variables because they represent Key:Value mappings that are unchanging between environments. That is, Production and QA will have the same value set for the same environment. These values are specified directly in the python code under heroku_vars","title":"Unchanging Values"},{"location":"how_to/heroku_config_vars/#simple-environment-specific-vars","text":"These are simple 1-to-1 mappings from a value stored in the Pulumi configuration under heroku_app:vars: . This map contains the variable names, in their final forms using all-caps, and the static values that are applicable to the environment.","title":"Simple Environment Specific Vars"},{"location":"how_to/heroku_config_vars/#interpolated-environment-specific-vars","text":"These are key:value mappings that are used in more complicated manners than a simple 1-to-1 mapping as with the simple settings. These values are stored in the Pulumi configuration under heroku_app:interpolated_vars: in lower-case, signifying that they do not directly become Config Vars in Heroku. These more involved interpolations take place during the construction of the heroku_interpolated_vars dictionary.","title":"Interpolated Environment Specific Vars"},{"location":"how_to/heroku_config_vars/#secrets","text":"Many Config Vars that we use represent values that can be considered secret or otherwise sensitive and should not be publicly disclosed. Nothing from these vars is derived from values stored in the Pulumi configuration, rather they are obtained either from SOPS config or directly from vault at stack application time. Secrets are complicated to work with and it is best to reach out to DevOps for assistence in getting your new secret configuration var setup.","title":"Secrets"},{"location":"how_to/heroku_dynosaur_management/","text":"Heroku Dynosaur Management Pipeline The pipeline can be found here . And the pipeline definition can be found here . There are two dicts in the pipeline defintion which define the various production and QA applications. The definition of each includes the name of the application as the key to the dict, and a substructure that lists the application owner and a list of dyno name/qty/size combinations. The owner is needed to perform an apiKey lookup from pre-existing SOPS data within the repo. At the moment we are not resetting the web node counts / configurations because HireFire has a hand in managing those.","title":"Heroku Dynosaur Management"},{"location":"how_to/heroku_dynosaur_management/#heroku-dynosaur-management","text":"","title":"Heroku Dynosaur Management"},{"location":"how_to/heroku_dynosaur_management/#pipeline","text":"The pipeline can be found here . And the pipeline definition can be found here . There are two dicts in the pipeline defintion which define the various production and QA applications. The definition of each includes the name of the application as the key to the dict, and a substructure that lists the application owner and a list of dyno name/qty/size combinations. The owner is needed to perform an apiKey lookup from pre-existing SOPS data within the repo. At the moment we are not resetting the web node counts / configurations because HireFire has a hand in managing those.","title":"Pipeline"},{"location":"how_to/heroku_log_drains/","text":"Configure a Log Drain To get logs from heroku applications -> Grafana we need to configure a 'log drain' in heroku for each application. This is pretty straight forward but does require collecting a few pieces of information first: The address of the 'vector-log-proxy' log-proxy-ci.odl.mit.edu log-proxy-qa.odl.mit.edu log-proxy.odl.mit.edu The basic auth password for the 'vector-log-proxy' server. sops -d src/bridge/secrets/vector/vector_log_proxy.ci.yaml heroku section, username: vector-log-proxy sops -d src/bridge/secrets/vector/vector_log_proxy.qa.yaml heroku section, username: vector-log-proxy sops -d src/bridge/secrets/vector/vector_log_proxy.production.yaml heroku section, username: vector-log-proxy Next we configure the log drain using the heroku-cli. There is no web interface for configuring this. heroku drains:add -a <HEROKU_APP_NAME> 'https://vector-log-proxy:<PASSWORD_FROM_SOPS>@<URL_FROM_ABOVE>:9000/events?app_name=<APP_NAME_WO_ENV_INFO>&environment=<ENV>&service=heroku' And you'll get a response like: Successfully added drain https://vector-log-proxy:<The rest of the URL you just used> app_name , environment , and service all refer directly to the organization fields used to categorize the logs in Grafana. References https://devcenter.heroku.com/articles/logplex https://devcenter.heroku.com/articles/log-drains#https-drains","title":"Heroku log drains"},{"location":"how_to/heroku_log_drains/#configure-a-log-drain","text":"To get logs from heroku applications -> Grafana we need to configure a 'log drain' in heroku for each application. This is pretty straight forward but does require collecting a few pieces of information first: The address of the 'vector-log-proxy' log-proxy-ci.odl.mit.edu log-proxy-qa.odl.mit.edu log-proxy.odl.mit.edu The basic auth password for the 'vector-log-proxy' server. sops -d src/bridge/secrets/vector/vector_log_proxy.ci.yaml heroku section, username: vector-log-proxy sops -d src/bridge/secrets/vector/vector_log_proxy.qa.yaml heroku section, username: vector-log-proxy sops -d src/bridge/secrets/vector/vector_log_proxy.production.yaml heroku section, username: vector-log-proxy Next we configure the log drain using the heroku-cli. There is no web interface for configuring this. heroku drains:add -a <HEROKU_APP_NAME> 'https://vector-log-proxy:<PASSWORD_FROM_SOPS>@<URL_FROM_ABOVE>:9000/events?app_name=<APP_NAME_WO_ENV_INFO>&environment=<ENV>&service=heroku' And you'll get a response like: Successfully added drain https://vector-log-proxy:<The rest of the URL you just used> app_name , environment , and service all refer directly to the organization fields used to categorize the logs in Grafana.","title":"Configure a Log Drain"},{"location":"how_to/heroku_log_drains/#references","text":"https://devcenter.heroku.com/articles/logplex https://devcenter.heroku.com/articles/log-drains#https-drains","title":"References"},{"location":"how_to/import_existing_resources_with_pulumi/","text":"Importing Resources That Already Exist Into Your Pulumi Stack Summary There are several ways to skin this particular cat but Tobias has shown me one that works really well so that's what we'll outline here. There's another approach using the pulumi import CLI command but in my experience that brings in a bunch of extra attributes we don't want. (There may be ways to tune this, I just don't know them.) Whole Cloth Before you start importing, code your resources the same way you always would. For example, if you need an S3 bucket, use all the usual Pulumi code - s3.Bucket etc. You want to code such that in a disaster recovery scenario, if we were staring from scratch, the resources would get build 100% correctly and the applications they support would pass all monitoring checks and smoke tests and function normally. Bring On The Special (Import) Sauce If you know you want to keep existing resources while having Pulumi create the rest, you should tell it to import these resources by passing in a ResourceOptions object at resource creation time. Here's an example of our S3 bucket: bootcamps_storage_bucket_name = f\"ol-bootcamps-app-{stack_info.env_suffix}\" bootcamps_storage_bucket = s3.Bucket( f\"ol-bootcamps-app-{stack_info.env_suffix}\", ### ****THIS LINE IS THE IMPORT CLAUSE**** opts=ResourceOptions(import_=bootcamps_storage_bucket_name,ignore_changes=[]), bucket=bootcamps_storage_bucket_name, # ... Note that obviously the object you're creating will differ if it's not an S3 bucket, for example a Vault mount point. Tuning After having added the above code, go ahead and run pulumi up on your stack. At this stage DO NOT SAY YES to finalizing these changes if there are any diagnostic warnigns displayed. Here's an example of something like what we'd expect: # cpatti @ rocinante in ~/src/mit/ol-infrastructure/src/ol_infrastructure/applications/bootcamps on git:cpatti_pulumi_bootcamp x ol-infrastructure-yqmQEgvq-py3.11 [14:31:28] $ pulumi up -s applications.bootcamps_ecommerce.Production Previewing update (applications.bootcamps_ecommerce.Production): Type Name Plan Info + pulumi:pulumi:Stack ol-infrastructure-bootcamps-ecommerce-application-applications.bootcamps_ecommerce.Production create + \u251c\u2500 ol:infrastructure:aws:database:OLAmazonDB bootcamps-db-applications-production create + \u2502 \u251c\u2500 aws:rds:ParameterGroup bootcamps-db-applications-production-postgres-parameter-group create + \u2502 \u251c\u2500 aws:rds:Instance bootcamps-db-applications-production-postgres-instance create + \u2502 \u2514\u2500 aws:rds:Instance bootcamps-db-applications-production-postgres-replica create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-CPUUtilization-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-CPUUtilization-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-WriteLatency-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-WriteLatency-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-FreeStorageSpace-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-FreeStorageSpace-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-EBSIOBlance-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-EBSIOBalance%-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-DiskQueueDepth-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-DiskQueueDepth-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-ReadLatency-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-ReadLatency-simple-rds-alarm create + \u251c\u2500 ol:services:Vault:DatabaseBackend:postgresql bootcamps create + \u2502 \u2514\u2500 vault:index:Mount bootcamps-mount-point create + \u2502 \u2514\u2500 vault:database:SecretBackendConnection bootcamps-database-connection create + \u2502 \u251c\u2500 vault:database:SecretBackendRole bootcamps-database-role-approle create + \u2502 \u251c\u2500 vault:database:SecretBackendRole bootcamps-database-role-admin create + \u2502 \u251c\u2500 vault:database:SecretBackendRole bootcamps-database-role-readonly create + \u2502 \u2514\u2500 vault:database:SecretBackendRole bootcamps-database-role-app create + \u251c\u2500 pulumi:providers:vault vault-provider create + \u251c\u2500 aws:iam:Policy bootcamps-production-policy create = \u251c\u2500 aws:s3:Bucket ol-bootcamps-app-production import [diff: -tagsAll~tags]; 1 warning = \u251c\u2500 vault:index:Mount bootcamps-vault-secrets-storage import + \u251c\u2500 aws:ec2:SecurityGroup bootcamps-db-access-production create + \u2514\u2500 vault:aws:SecretBackendRole bootcamps-app-production create Diagnostics: aws:s3:Bucket (ol-bootcamps-app-production): warning: inputs to import do not match the existing resource; importing this resource will fail Outputs: bootcamps_app: { rds_host: output<string> } The two key bits of output to focus on here are the diagnostic warning towards the end: Diagnostics: aws:s3:Bucket (ol-bootcamps-app-production): warning: inputs to import do not match the existing resource; importing this resource will fail This tells us that Pulumi has detected a critical difference between the resource's state in the real world and Pulumi's model of what's there and what needs to change to arrive at the desired state. The next most important bit is nestled amongst Pulumi telling us what changes it plans to make: = \u251c\u2500 aws:s3:Bucket ol-bootcamps-app-production import [diff: -tagsAll~tags]; 1 warning This tells us that an attribute, in this case the tags that we're specifying the S3 bucket should have in our Pulumi source disagrees with what's actually sitting out there on EC2. In order to fix this discrepancy, we go back to our ResourceOptions line we added, adding the tags attribute into it to tell Pulumi to leave the current tags alone and not complain that they differ: opts=ResourceOptions(import_=bootcamps_storage_bucket_name,ignore_changes=[\"policy\",\"tags\"]), You'll also note that \"policy\" is in that attributes list. That's needed because Pulumi signalled a mismatch on a previous run. After these additions, your pulumi up should succeed and all the resources should be created with no further warnings. Cleaning Up After you've successfully built your environment with Pulumi and imported the existing resources, you'll want to remove all those ResourceOptions lines from your Pulumi model source as the import should only be done once.","title":"Importing Resources That Already Exist Into Your Pulumi Stack"},{"location":"how_to/import_existing_resources_with_pulumi/#importing-resources-that-already-exist-into-your-pulumi-stack","text":"","title":"Importing Resources That Already Exist Into Your Pulumi Stack"},{"location":"how_to/import_existing_resources_with_pulumi/#summary","text":"There are several ways to skin this particular cat but Tobias has shown me one that works really well so that's what we'll outline here. There's another approach using the pulumi import CLI command but in my experience that brings in a bunch of extra attributes we don't want. (There may be ways to tune this, I just don't know them.)","title":"Summary"},{"location":"how_to/import_existing_resources_with_pulumi/#whole-cloth","text":"Before you start importing, code your resources the same way you always would. For example, if you need an S3 bucket, use all the usual Pulumi code - s3.Bucket etc. You want to code such that in a disaster recovery scenario, if we were staring from scratch, the resources would get build 100% correctly and the applications they support would pass all monitoring checks and smoke tests and function normally.","title":"Whole Cloth"},{"location":"how_to/import_existing_resources_with_pulumi/#bring-on-the-special-import-sauce","text":"If you know you want to keep existing resources while having Pulumi create the rest, you should tell it to import these resources by passing in a ResourceOptions object at resource creation time. Here's an example of our S3 bucket: bootcamps_storage_bucket_name = f\"ol-bootcamps-app-{stack_info.env_suffix}\" bootcamps_storage_bucket = s3.Bucket( f\"ol-bootcamps-app-{stack_info.env_suffix}\", ### ****THIS LINE IS THE IMPORT CLAUSE**** opts=ResourceOptions(import_=bootcamps_storage_bucket_name,ignore_changes=[]), bucket=bootcamps_storage_bucket_name, # ... Note that obviously the object you're creating will differ if it's not an S3 bucket, for example a Vault mount point.","title":"Bring On The Special (Import) Sauce"},{"location":"how_to/import_existing_resources_with_pulumi/#tuning","text":"After having added the above code, go ahead and run pulumi up on your stack. At this stage DO NOT SAY YES to finalizing these changes if there are any diagnostic warnigns displayed. Here's an example of something like what we'd expect: # cpatti @ rocinante in ~/src/mit/ol-infrastructure/src/ol_infrastructure/applications/bootcamps on git:cpatti_pulumi_bootcamp x ol-infrastructure-yqmQEgvq-py3.11 [14:31:28] $ pulumi up -s applications.bootcamps_ecommerce.Production Previewing update (applications.bootcamps_ecommerce.Production): Type Name Plan Info + pulumi:pulumi:Stack ol-infrastructure-bootcamps-ecommerce-application-applications.bootcamps_ecommerce.Production create + \u251c\u2500 ol:infrastructure:aws:database:OLAmazonDB bootcamps-db-applications-production create + \u2502 \u251c\u2500 aws:rds:ParameterGroup bootcamps-db-applications-production-postgres-parameter-group create + \u2502 \u251c\u2500 aws:rds:Instance bootcamps-db-applications-production-postgres-instance create + \u2502 \u2514\u2500 aws:rds:Instance bootcamps-db-applications-production-postgres-replica create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-CPUUtilization-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-CPUUtilization-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-WriteLatency-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-WriteLatency-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-FreeStorageSpace-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-FreeStorageSpace-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-EBSIOBlance-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-EBSIOBalance%-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-DiskQueueDepth-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-DiskQueueDepth-simple-rds-alarm create + \u251c\u2500 ol:infrastructure.aws.cloudwatch.OLCloudWatchAlarmRDS bootcamps-db-applications-production-ReadLatency-OLCloudWatchAlarmSimpleRDSConfig create + \u2502 \u2514\u2500 aws:cloudwatch:MetricAlarm bootcamps-db-applications-production-ReadLatency-simple-rds-alarm create + \u251c\u2500 ol:services:Vault:DatabaseBackend:postgresql bootcamps create + \u2502 \u2514\u2500 vault:index:Mount bootcamps-mount-point create + \u2502 \u2514\u2500 vault:database:SecretBackendConnection bootcamps-database-connection create + \u2502 \u251c\u2500 vault:database:SecretBackendRole bootcamps-database-role-approle create + \u2502 \u251c\u2500 vault:database:SecretBackendRole bootcamps-database-role-admin create + \u2502 \u251c\u2500 vault:database:SecretBackendRole bootcamps-database-role-readonly create + \u2502 \u2514\u2500 vault:database:SecretBackendRole bootcamps-database-role-app create + \u251c\u2500 pulumi:providers:vault vault-provider create + \u251c\u2500 aws:iam:Policy bootcamps-production-policy create = \u251c\u2500 aws:s3:Bucket ol-bootcamps-app-production import [diff: -tagsAll~tags]; 1 warning = \u251c\u2500 vault:index:Mount bootcamps-vault-secrets-storage import + \u251c\u2500 aws:ec2:SecurityGroup bootcamps-db-access-production create + \u2514\u2500 vault:aws:SecretBackendRole bootcamps-app-production create Diagnostics: aws:s3:Bucket (ol-bootcamps-app-production): warning: inputs to import do not match the existing resource; importing this resource will fail Outputs: bootcamps_app: { rds_host: output<string> } The two key bits of output to focus on here are the diagnostic warning towards the end: Diagnostics: aws:s3:Bucket (ol-bootcamps-app-production): warning: inputs to import do not match the existing resource; importing this resource will fail This tells us that Pulumi has detected a critical difference between the resource's state in the real world and Pulumi's model of what's there and what needs to change to arrive at the desired state. The next most important bit is nestled amongst Pulumi telling us what changes it plans to make: = \u251c\u2500 aws:s3:Bucket ol-bootcamps-app-production import [diff: -tagsAll~tags]; 1 warning This tells us that an attribute, in this case the tags that we're specifying the S3 bucket should have in our Pulumi source disagrees with what's actually sitting out there on EC2. In order to fix this discrepancy, we go back to our ResourceOptions line we added, adding the tags attribute into it to tell Pulumi to leave the current tags alone and not complain that they differ: opts=ResourceOptions(import_=bootcamps_storage_bucket_name,ignore_changes=[\"policy\",\"tags\"]), You'll also note that \"policy\" is in that attributes list. That's needed because Pulumi signalled a mismatch on a previous run. After these additions, your pulumi up should succeed and all the resources should be created with no further warnings.","title":"Tuning"},{"location":"how_to/import_existing_resources_with_pulumi/#cleaning-up","text":"After you've successfully built your environment with Pulumi and imported the existing resources, you'll want to remove all those ResourceOptions lines from your Pulumi model source as the import should only be done once.","title":"Cleaning Up"},{"location":"how_to/mitol_deploy_app_with_kubernetes/","text":"Deploying an Application at MIT Open Learning with Kubernetes Assumptions You'll be deploying an app that includes a helm chart. You've installed helm and kubectl . You plan to manage your Kubernetes app's infra and deployment with Pulumi . Questions Will you need to create a new Kubernetes cluster along with this new application you're deploying? Generally we'd like the answer to be \"no\". Structure Application All the files and configuration directly pertaining to the app itself. These usually live here . For example our Open Metadata application is here . We'll use Open Metadata QA's example to help guide us on a tour of how the architecture hangs together. This will generally be your home base for this project. Infrastructure All the component infrastructure required to support your application. Network General Rules: There will only be one EKS cluster in any given VPC Pods and EKS nodes share the same address spaces Pod + node address spaces reside in different availability zones Pod + node address spaces should be positioned \"in the middle of the VPC\" Pod + node address spaces are at least /21 -> ~2048 addresses per space There are at least 4 pod and node address spaces -> at least ~8192 addresses per cluster Service Address spaces are arbitrary but CANNOT be a subnet of the VPC. Service address spaces should not overlap from cluster to cluster Service address spaces jump +60 on the third octet for CI->QA->Production Service address spaces are at least /23 -> ~512 addresses per space VPC CI QA Production Applications Services: 10.110.24.0/23 Pods: 172.18.128.0/21 172.18.136.0/21 172.18.144.0/21 172.18.152.0/21 Services: 10.110.84.0/23 Pods: 10.12.128.0/21 10.12.136.0/21 10.12.144.0/21 10.12.152.0/21 Services: 10.110.144.0/23 Pods: 10.13.128.0/21 10.13.136.0/21 10.13.144.0/21 10.13.152.0/21 Data Services: 10.110.22.0/23 Pods: 172.23.128.0/21 172.23.136.0/21 172.23.144.0/21 172.23.152.0/21 Services: 10.110.82.0/23 Pods: 10.2.128.0/21 10.2.136.0/21 10.2.144.0/21 10.2.152.0/21 Services: 10.110.142.0/23 Pods: 10.3.128.0/21 10.3.136.0/21 10.3.144.0/21 10.3.152.0/21 Operations Services: 10.110.20.0/23 Pods: 172.16.128.0/21 172.16.136.0/21 172.16.144.0/21 172.16.152.0/21 Services: 10.110.80.0/23 Pods: 10.1.128.0/21 10.1.136.0/21 10.1.144.0/21 10.1.152.0/21 Services: 10.110.140.0/23 Pods: 10.0.128.0/21 10.0.136.0/21 10.0.144.0/21 10.0.152.0/21 ... ... ... ... Networking Configuration YAML src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.CI.yaml src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.QA.yaml src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.Production.yaml EKS Cluster This is the beating heart of where your kubernetes cluster is defined in Pulumi. It contains a multitude of configuration optioons including namespaces defined in this cluster, what operating environment (e.g. CI, QA, or Production) the cluster will operate in. It also contains such details as how Vault ties in, and the instance type (size, etc) for its workers. Substructure There's not much to configure here , but the code that this section embodies is critical. It builds critical path components like SSL certs and the components (like Traefik and Vault!) that manage them as well as authentication and other secrets. Building the Foundation - EKS Cluster If you'll need to build a new EKS cluster as we did the data cluster used for the Open metadata application, you'll need to create the necessary configuration in the infrastructure and substructure sections. Networking You'll need to choose pod and service subnets for your cluster. For now, use the example of the data-qa cluster's definitions and pay attention to the relationship between the subnets we chose and the VPC subnets they live in. Namespace You'll likely need to add a new namespace for your application whether you're creating a new cluster or not to the cluster's configuration . Application When we deploy our Kubernetes applications with Pulumi we use the Pulumi Kubernetes Provider . Helm charts are deployed by Pulumi by translating the helm chart into a kubernetes.helm.v3.release object. Click the link about for an example of how we translated Open Metadata's helm chart. Pay particular attention to the Values dictionary. Make It So We use CI to prove things out. So you'll want to start with that environment. If your application will require its own EKS cluster, you'll want to build that first. So change directories to ol-infrastructure/src/ol_infrastructure/infrastructure/aws/eks and run pulumi up. For instance, were you building the data CI cluster, you'd run: pulumi up -s infrastructure.aws.eks.data.CI You'll almost certainly need to fix issues as you go. There's lots of complex configuration here that can't be covered in a simple doc. Then you'll want to build the resources in substructure for your project, so once again for the data CI cluster we'd want to change directory to ol-infrastructure/src/ol_infrastructure/substructure/aws/eks and run pulumi up. pulumi up -s substructure.aws.eks.data.CI Then you'll want to deploy your application. So for OMD as an example, cd to ol-infrastucture/src/ol_infrastructure/applications/open_metadata and run pulumi up. pulumi up -s applications.open_metadata.CI","title":"Deploying an Application at MIT Open Learning with Kubernetes"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#deploying-an-application-at-mit-open-learning-with-kubernetes","text":"","title":"Deploying an Application at MIT Open Learning with Kubernetes"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#assumptions","text":"You'll be deploying an app that includes a helm chart. You've installed helm and kubectl . You plan to manage your Kubernetes app's infra and deployment with Pulumi .","title":"Assumptions"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#questions","text":"Will you need to create a new Kubernetes cluster along with this new application you're deploying? Generally we'd like the answer to be \"no\".","title":"Questions"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#structure","text":"","title":"Structure"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#application","text":"All the files and configuration directly pertaining to the app itself. These usually live here . For example our Open Metadata application is here . We'll use Open Metadata QA's example to help guide us on a tour of how the architecture hangs together. This will generally be your home base for this project.","title":"Application"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#infrastructure","text":"All the component infrastructure required to support your application.","title":"Infrastructure"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#network","text":"General Rules: There will only be one EKS cluster in any given VPC Pods and EKS nodes share the same address spaces Pod + node address spaces reside in different availability zones Pod + node address spaces should be positioned \"in the middle of the VPC\" Pod + node address spaces are at least /21 -> ~2048 addresses per space There are at least 4 pod and node address spaces -> at least ~8192 addresses per cluster Service Address spaces are arbitrary but CANNOT be a subnet of the VPC. Service address spaces should not overlap from cluster to cluster Service address spaces jump +60 on the third octet for CI->QA->Production Service address spaces are at least /23 -> ~512 addresses per space VPC CI QA Production Applications Services: 10.110.24.0/23 Pods: 172.18.128.0/21 172.18.136.0/21 172.18.144.0/21 172.18.152.0/21 Services: 10.110.84.0/23 Pods: 10.12.128.0/21 10.12.136.0/21 10.12.144.0/21 10.12.152.0/21 Services: 10.110.144.0/23 Pods: 10.13.128.0/21 10.13.136.0/21 10.13.144.0/21 10.13.152.0/21 Data Services: 10.110.22.0/23 Pods: 172.23.128.0/21 172.23.136.0/21 172.23.144.0/21 172.23.152.0/21 Services: 10.110.82.0/23 Pods: 10.2.128.0/21 10.2.136.0/21 10.2.144.0/21 10.2.152.0/21 Services: 10.110.142.0/23 Pods: 10.3.128.0/21 10.3.136.0/21 10.3.144.0/21 10.3.152.0/21 Operations Services: 10.110.20.0/23 Pods: 172.16.128.0/21 172.16.136.0/21 172.16.144.0/21 172.16.152.0/21 Services: 10.110.80.0/23 Pods: 10.1.128.0/21 10.1.136.0/21 10.1.144.0/21 10.1.152.0/21 Services: 10.110.140.0/23 Pods: 10.0.128.0/21 10.0.136.0/21 10.0.144.0/21 10.0.152.0/21 ... ... ... ... Networking Configuration YAML src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.CI.yaml src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.QA.yaml src/ol_infrastructure/infrastructure/aws/network/Pulumi.infrastructure.aws.network.Production.yaml","title":"Network"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#eks-cluster","text":"This is the beating heart of where your kubernetes cluster is defined in Pulumi. It contains a multitude of configuration optioons including namespaces defined in this cluster, what operating environment (e.g. CI, QA, or Production) the cluster will operate in. It also contains such details as how Vault ties in, and the instance type (size, etc) for its workers.","title":"EKS Cluster"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#substructure","text":"There's not much to configure here , but the code that this section embodies is critical. It builds critical path components like SSL certs and the components (like Traefik and Vault!) that manage them as well as authentication and other secrets.","title":"Substructure"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#building-the-foundation-eks-cluster","text":"If you'll need to build a new EKS cluster as we did the data cluster used for the Open metadata application, you'll need to create the necessary configuration in the infrastructure and substructure sections.","title":"Building the Foundation - EKS Cluster"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#networking","text":"You'll need to choose pod and service subnets for your cluster. For now, use the example of the data-qa cluster's definitions and pay attention to the relationship between the subnets we chose and the VPC subnets they live in.","title":"Networking"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#namespace","text":"You'll likely need to add a new namespace for your application whether you're creating a new cluster or not to the cluster's configuration .","title":"Namespace"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#application_1","text":"When we deploy our Kubernetes applications with Pulumi we use the Pulumi Kubernetes Provider . Helm charts are deployed by Pulumi by translating the helm chart into a kubernetes.helm.v3.release object. Click the link about for an example of how we translated Open Metadata's helm chart. Pay particular attention to the Values dictionary.","title":"Application"},{"location":"how_to/mitol_deploy_app_with_kubernetes/#make-it-so","text":"We use CI to prove things out. So you'll want to start with that environment. If your application will require its own EKS cluster, you'll want to build that first. So change directories to ol-infrastructure/src/ol_infrastructure/infrastructure/aws/eks and run pulumi up. For instance, were you building the data CI cluster, you'd run: pulumi up -s infrastructure.aws.eks.data.CI You'll almost certainly need to fix issues as you go. There's lots of complex configuration here that can't be covered in a simple doc. Then you'll want to build the resources in substructure for your project, so once again for the data CI cluster we'd want to change directory to ol-infrastructure/src/ol_infrastructure/substructure/aws/eks and run pulumi up. pulumi up -s substructure.aws.eks.data.CI Then you'll want to deploy your application. So for OMD as an example, cd to ol-infrastucture/src/ol_infrastructure/applications/open_metadata and run pulumi up. pulumi up -s applications.open_metadata.CI","title":"Make It So"},{"location":"how_to/mitol_dev_access_to_vault/","text":"OL Secrets Management The Engineering group in Open Learning hosts its own Hashicorp Vault clusters to handle secrets management. Part of our work is to provide OL developers access to Vault QA to perform the following tasks: 1. Securely share secrets 2. Generate local .env file for app development Requirements A Keycloak account in the ol-platform-engineering realm. If you currently do not have one, please ping someone in Platform Engineering to set you up. 1. Securely share secrets Log in to Vault QA Method: OIDC Role: local-dev Note: Make sure to enable popups You should see a popup asking for your Keycloak username and password. Once successfully authenticated, you should see the Vault UI where we have configured a separate Vault mount secret-sandbox that should be used for securely sharing senisitive data with the group. WARNING - DO NOT store anything permanent in that mount as there are no guarantees that they will not be deleted or overwritten. Any values stored in this mount are visible and accessible by all users of the `ol-platform-engineering` Keycloak realm which as of this writing is and should be restricted to OL Engineering staff. 2. Generate local .env file for app development Install Hashicorp Vault Download the relevant app vars shell script from the repo (Ex. app_vars.sh) In your terminal do the following: Set Vault's url as an environment variable export VAULT_ADDR=https://vault-qa.odl.mit.edu Login to Vault from the CLI: vault login -method=oidc role=\"local-dev\" Run the following to generate vault client config: vault agent generate-config -type=\"env-template\" \\ -exec=\"./app_vars.sh\" \\ -path=\"secret-dev/*\" \\ -path=\"secret-operations/mailgun\" \\ -path=\"secret-operations/global/embedly\" \\ -path=\"secret-operations/global/odlbot-github-access-token\" \\ -path=\"secret-operations/global/mit-smtp\" \\ -path=\"secret-operations/global/update-search-data-webhook-key\" \\ -path=\"secret-operations/sso/mitlearn\" \\ -path=\"secret-operations/tika/access-token\" \\ agent-config.hcl Start the vault agent: vault agent -config=agent-config.hcl -log-level=error You should now have a .env file containing the secrets for the relevant app from vault. References https://developer.hashicorp.com/vault/docs/agent-and-proxy/agent/generate-config","title":"OL Secrets Management"},{"location":"how_to/mitol_dev_access_to_vault/#ol-secrets-management","text":"The Engineering group in Open Learning hosts its own Hashicorp Vault clusters to handle secrets management. Part of our work is to provide OL developers access to Vault QA to perform the following tasks: 1. Securely share secrets 2. Generate local .env file for app development","title":"OL Secrets Management"},{"location":"how_to/mitol_dev_access_to_vault/#requirements","text":"A Keycloak account in the ol-platform-engineering realm. If you currently do not have one, please ping someone in Platform Engineering to set you up.","title":"Requirements"},{"location":"how_to/mitol_dev_access_to_vault/#1-securely-share-secrets","text":"Log in to Vault QA Method: OIDC Role: local-dev Note: Make sure to enable popups You should see a popup asking for your Keycloak username and password. Once successfully authenticated, you should see the Vault UI where we have configured a separate Vault mount secret-sandbox that should be used for securely sharing senisitive data with the group. WARNING - DO NOT store anything permanent in that mount as there are no guarantees that they will not be deleted or overwritten. Any values stored in this mount are visible and accessible by all users of the `ol-platform-engineering` Keycloak realm which as of this writing is and should be restricted to OL Engineering staff.","title":"1. Securely share secrets"},{"location":"how_to/mitol_dev_access_to_vault/#2-generate-local-env-file-for-app-development","text":"Install Hashicorp Vault Download the relevant app vars shell script from the repo (Ex. app_vars.sh) In your terminal do the following: Set Vault's url as an environment variable export VAULT_ADDR=https://vault-qa.odl.mit.edu Login to Vault from the CLI: vault login -method=oidc role=\"local-dev\" Run the following to generate vault client config: vault agent generate-config -type=\"env-template\" \\ -exec=\"./app_vars.sh\" \\ -path=\"secret-dev/*\" \\ -path=\"secret-operations/mailgun\" \\ -path=\"secret-operations/global/embedly\" \\ -path=\"secret-operations/global/odlbot-github-access-token\" \\ -path=\"secret-operations/global/mit-smtp\" \\ -path=\"secret-operations/global/update-search-data-webhook-key\" \\ -path=\"secret-operations/sso/mitlearn\" \\ -path=\"secret-operations/tika/access-token\" \\ agent-config.hcl Start the vault agent: vault agent -config=agent-config.hcl -log-level=error You should now have a .env file containing the secrets for the relevant app from vault.","title":"2. Generate local .env file for app development"},{"location":"how_to/mitol_dev_access_to_vault/#references","text":"https://developer.hashicorp.com/vault/docs/agent-and-proxy/agent/generate-config","title":"References"},{"location":"how_to/mitol_devops_patterns_cookbook/","text":"MIT OL Devops Patterns Cookbook Summary This document will serve as a place to put patterns and best practices. The goal is to ease the path for both new builders and experienced builders by helping narrow down the bevvy of choices and present a workable best practice solution to a given problem. There are many potential ways to organize a document like this, but for now I intend to start with the recipes broken down by infrastructure components. Example of what I mean are Docker, Traefik, Pyinfra and the like. Please write your recipes in the following form with What I Want and How To Build It as bold subsections. Recipes Traefik Token Based Authentication What I Want I want Traefik to allow requests only from clients that pass a particular token in the HTTP headers of the request. Here's an example curl from Tika with the actual token: curl --header 'X-Access-Token: <crazy hex digits>' https://tika-qa.odl.mit.edu How To Build It Traefik does not contain this functionality by default, so we must leverage the checkheaders Traefik middleware plugin. You will need to add a blob to your Traefik static configuration like this: experimental: plugins: checkheadersplugin: moduleName: \"github.com/dkijkuit/checkheadersplugin\" version: \"v0.2.6\" Since we use pyinfra to automate our image builds, you'll need to add code like this to your deploy.py and a line like this to your docker-compose.yaml file.","title":"MIT OL Devops Patterns Cookbook"},{"location":"how_to/mitol_devops_patterns_cookbook/#mit-ol-devops-patterns-cookbook","text":"","title":"MIT OL Devops Patterns Cookbook"},{"location":"how_to/mitol_devops_patterns_cookbook/#summary","text":"This document will serve as a place to put patterns and best practices. The goal is to ease the path for both new builders and experienced builders by helping narrow down the bevvy of choices and present a workable best practice solution to a given problem. There are many potential ways to organize a document like this, but for now I intend to start with the recipes broken down by infrastructure components. Example of what I mean are Docker, Traefik, Pyinfra and the like. Please write your recipes in the following form with What I Want and How To Build It as bold subsections.","title":"Summary"},{"location":"how_to/mitol_devops_patterns_cookbook/#recipes","text":"","title":"Recipes"},{"location":"how_to/mitol_devops_patterns_cookbook/#traefik","text":"","title":"Traefik"},{"location":"how_to/mitol_devops_patterns_cookbook/#token-based-authentication","text":"What I Want I want Traefik to allow requests only from clients that pass a particular token in the HTTP headers of the request. Here's an example curl from Tika with the actual token: curl --header 'X-Access-Token: <crazy hex digits>' https://tika-qa.odl.mit.edu How To Build It Traefik does not contain this functionality by default, so we must leverage the checkheaders Traefik middleware plugin. You will need to add a blob to your Traefik static configuration like this: experimental: plugins: checkheadersplugin: moduleName: \"github.com/dkijkuit/checkheadersplugin\" version: \"v0.2.6\" Since we use pyinfra to automate our image builds, you'll need to add code like this to your deploy.py and a line like this to your docker-compose.yaml file.","title":"Token Based Authentication"},{"location":"how_to/mitol_kubernetes_cookbook/","text":"MIT OL Kubernetes Cookbook kubectl Recipes Port-forward to a pod https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ kubectl port-forward <podname> <local port>:<remote port> -n <namespace> kubectl port-forward grafana-alloy-dnqj2 12345:12345 -n operations Get a pgsql Prompt kubectl run -i --tty postgres --image=postgres --restart=Never -n airbyte -- sh Deleting k8s Namespaces With Stuck Vault Finalizers kubectl patch -n airbyte vaultauth airbyte-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultconnection airbyte-vault-connection -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultstaticsecret airbyte-basic-auth-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultstaticsecret airbyte-forward-auth-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultdynamicsecret airbyte-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultdynamicsecret open-metadata-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultdynamicsecret openmetadata-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultdynamicsecret openmetadata-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultstaticsecret openmetadata-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultstaticsecret openmetadata-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultauth open-metadata-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultauth open-metadata-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultconnection open-metadata-vault-connection -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n operations vaultstaticsecret vault-kv-global-odl-wildcard -p '{\"metadata\":{\"finalizers\":null}}' --type=merge Get Overview Of a Namespace Shows things like open ports, pod status and the like. kubectl get all -n open-metadata Get Information / Status On A Particular Resource kubectl describe <resource> <optional-resource-name> -n <namespace> e.g. kubectl describe pod -n open-metadata openmetadata-5f78b769d4-4wgs9 feoh@prometheus Pulumi Server Side Complaints Sometimes pulumi will complain about being unable to manage a field or something on k8s resources. Something like this: Diagnostics: pulumi:pulumi:Stack (ol-infrastructure-open_metadata-application-applications.open_metadata.CI): error: preview failed kubernetes:core/v1:ServiceAccount (open-metadata-vault-service-account): error: Preview failed: 1 error occurred: * the Kubernetes API server reported that \"open-metadata/open-metadata-vault\" failed to fully initialize or become live: Server-Side Apply field conflict detected. See https://www.pulumi.com/registry/packages/kubernetes/how-to-guides/managing-resources-with-server-side-apply/#handle-field-conflicts-on-existing-resources for troubleshooting help. The resource managed by field manager \"pulumi-kubernetes-51b738f0\" had an apply conflict: Apply failed with 1 conflict: conflict with \"pulumi-kubernetes-cef7f602\": .metadata.labels.pulumi_stack kubernetes:rbac.authorization.k8s.io/v1:ClusterRoleBinding (open-metadata-vault-cluster-role-binding): error: Preview failed: 1 error occurred: * the Kubernetes API server reported that \"open-metadata-vault:cluster-auth\" failed to fully initialize or become live: Server-Side Apply field conflict detected. See https://www.pulumi.com/registry/packages/kubernetes/how-to-guides/managing-resources-with-server-side-apply/#handle-field-conflicts-on-existing-resources for troubleshooting help. The resource managed by field manager \"pulumi-kubernetes-0e168a03\" had an apply conflict: Apply failed with 2 conflicts: conflicts with \"pulumi-kubernetes-0754bbed\": - .metadata.labels.pulumi_stack conflicts with \"pulumi-kubernetes-f4f83ba0\": - .metadata.labels.pulumi_stack Easiest thing to do is set an env var on execution which will bring the questionable fields back into pulumi management and keep you moving. There is still probably a bigger issue at play, though. PULUMI_K8S_ENABLE_PATCH_FORCE=\"true\" pr pulumi up -s applications.open_metadata.CI","title":"MIT OL Kubernetes Cookbook"},{"location":"how_to/mitol_kubernetes_cookbook/#mit-ol-kubernetes-cookbook","text":"","title":"MIT OL Kubernetes Cookbook"},{"location":"how_to/mitol_kubernetes_cookbook/#kubectl-recipes","text":"","title":"kubectl Recipes"},{"location":"how_to/mitol_kubernetes_cookbook/#port-forward-to-a-pod","text":"https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ kubectl port-forward <podname> <local port>:<remote port> -n <namespace> kubectl port-forward grafana-alloy-dnqj2 12345:12345 -n operations","title":"Port-forward to a pod"},{"location":"how_to/mitol_kubernetes_cookbook/#get-a-pgsql-prompt","text":"kubectl run -i --tty postgres --image=postgres --restart=Never -n airbyte -- sh","title":"Get a pgsql Prompt"},{"location":"how_to/mitol_kubernetes_cookbook/#deleting-k8s-namespaces-with-stuck-vault-finalizers","text":"kubectl patch -n airbyte vaultauth airbyte-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultconnection airbyte-vault-connection -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultstaticsecret airbyte-basic-auth-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultstaticsecret airbyte-forward-auth-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n airbyte vaultdynamicsecret airbyte-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultdynamicsecret open-metadata-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultdynamicsecret openmetadata-app-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultdynamicsecret openmetadata-db-creds -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultstaticsecret openmetadata-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultstaticsecret openmetadata-oidc-config -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultauth open-metadata-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultauth open-metadata-auth -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n open-metadata vaultconnection open-metadata-vault-connection -p '{\"metadata\":{\"finalizers\":null}}' --type=merge kubectl patch -n operations vaultstaticsecret vault-kv-global-odl-wildcard -p '{\"metadata\":{\"finalizers\":null}}' --type=merge","title":"Deleting k8s Namespaces With Stuck Vault Finalizers"},{"location":"how_to/mitol_kubernetes_cookbook/#get-overview-of-a-namespace","text":"Shows things like open ports, pod status and the like. kubectl get all -n open-metadata","title":"Get Overview Of a Namespace"},{"location":"how_to/mitol_kubernetes_cookbook/#get-information-status-on-a-particular-resource","text":"kubectl describe <resource> <optional-resource-name> -n <namespace> e.g. kubectl describe pod -n open-metadata openmetadata-5f78b769d4-4wgs9 feoh@prometheus","title":"Get Information / Status On A Particular Resource"},{"location":"how_to/mitol_kubernetes_cookbook/#pulumi-server-side-complaints","text":"Sometimes pulumi will complain about being unable to manage a field or something on k8s resources. Something like this: Diagnostics: pulumi:pulumi:Stack (ol-infrastructure-open_metadata-application-applications.open_metadata.CI): error: preview failed kubernetes:core/v1:ServiceAccount (open-metadata-vault-service-account): error: Preview failed: 1 error occurred: * the Kubernetes API server reported that \"open-metadata/open-metadata-vault\" failed to fully initialize or become live: Server-Side Apply field conflict detected. See https://www.pulumi.com/registry/packages/kubernetes/how-to-guides/managing-resources-with-server-side-apply/#handle-field-conflicts-on-existing-resources for troubleshooting help. The resource managed by field manager \"pulumi-kubernetes-51b738f0\" had an apply conflict: Apply failed with 1 conflict: conflict with \"pulumi-kubernetes-cef7f602\": .metadata.labels.pulumi_stack kubernetes:rbac.authorization.k8s.io/v1:ClusterRoleBinding (open-metadata-vault-cluster-role-binding): error: Preview failed: 1 error occurred: * the Kubernetes API server reported that \"open-metadata-vault:cluster-auth\" failed to fully initialize or become live: Server-Side Apply field conflict detected. See https://www.pulumi.com/registry/packages/kubernetes/how-to-guides/managing-resources-with-server-side-apply/#handle-field-conflicts-on-existing-resources for troubleshooting help. The resource managed by field manager \"pulumi-kubernetes-0e168a03\" had an apply conflict: Apply failed with 2 conflicts: conflicts with \"pulumi-kubernetes-0754bbed\": - .metadata.labels.pulumi_stack conflicts with \"pulumi-kubernetes-f4f83ba0\": - .metadata.labels.pulumi_stack Easiest thing to do is set an env var on execution which will bring the questionable fields back into pulumi management and keep you moving. There is still probably a bigger issue at play, though. PULUMI_K8S_ENABLE_PATCH_FORCE=\"true\" pr pulumi up -s applications.open_metadata.CI","title":"Pulumi Server Side Complaints"},{"location":"how_to/mitol_openedx_mysql_access/","text":"Querying An MIT OL Open EdX Mysql Database This is a quick and dirty guide to connecting to the OpenEdX MySQL database for one of our products. Connect to an EC2 server that hosts containers for the product in question For instance, in the case of XPro CI, connect to a server in instance group edxapp-[web/worker]-mitxpro-ci. You can find detailed instructions on how to do this here . Install a mysql or mariadb client if one's not there already (If you do this in production environments, and you probably shouldn't, be sure to remove it when done.) sudo apt install mariadb-client OR sudo apt install mysql-client Get the Database Credentials Use docker compose to connect to a container like the LMS or CMS: docker compose exec -it lms bash and then, once insde the container, you can find the database connection information in /openedx/edx-platform/lms/envs/lms.yaml in the DATABASES section. Be sure to choose the correct database for your environment. To The Prompt! Now, Ctrl-D out of the container bash you were logged into and get back to the host you sshed into. You can now use the mysql client to connect to the database. e.g. mysql -h <hostname> -u <username> -p You'll need to enter or paste in the password you found in the previous section when prompted. That should be all you need to get a mysql prompt to run queries against! Be careful, there are no guardrails here!","title":"Querying An MIT OL Open EdX Mysql Database"},{"location":"how_to/mitol_openedx_mysql_access/#querying-an-mit-ol-open-edx-mysql-database","text":"This is a quick and dirty guide to connecting to the OpenEdX MySQL database for one of our products.","title":"Querying An MIT OL Open EdX Mysql Database"},{"location":"how_to/mitol_openedx_mysql_access/#connect-to-an-ec2-server-that-hosts-containers-for-the-product-in-question","text":"For instance, in the case of XPro CI, connect to a server in instance group edxapp-[web/worker]-mitxpro-ci. You can find detailed instructions on how to do this here .","title":"Connect to an EC2 server that hosts containers for the product in question"},{"location":"how_to/mitol_openedx_mysql_access/#install-a-mysql-or-mariadb-client-if-ones-not-there-already","text":"(If you do this in production environments, and you probably shouldn't, be sure to remove it when done.) sudo apt install mariadb-client OR sudo apt install mysql-client","title":"Install a mysql or mariadb client if one's not there already"},{"location":"how_to/mitol_openedx_mysql_access/#get-the-database-credentials","text":"Use docker compose to connect to a container like the LMS or CMS: docker compose exec -it lms bash and then, once insde the container, you can find the database connection information in /openedx/edx-platform/lms/envs/lms.yaml in the DATABASES section. Be sure to choose the correct database for your environment.","title":"Get the Database Credentials"},{"location":"how_to/mitol_openedx_mysql_access/#to-the-prompt","text":"Now, Ctrl-D out of the container bash you were logged into and get back to the host you sshed into. You can now use the mysql client to connect to the database. e.g. mysql -h <hostname> -u <username> -p You'll need to enter or paste in the password you found in the previous section when prompted. That should be all you need to get a mysql prompt to run queries against! Be careful, there are no guardrails here!","title":"To The Prompt!"},{"location":"how_to/mitol_openedx_retirement_pipeline/","text":"Setting Up The Retirement Pipeline For A New MIT OL OpenEdX Application Getting Set Up First, follow the instructions in the Setting Up User Retirement In The LMS document. You'll need to ssh into an edx-worker for the appropriate environment you're working in. Run sudo su - edxapp -s /bin/bash and then source edxapp_env to get to where you need to be to run the Django manage application. You can follow the doc verbatim with the exception that: * The manage.py executable is located in the edx-platform folder, so you'd start your invocations with python edx-platform/manage.py * Ignore the --settings <your settings> option as we already have our settings defined in the LMS configuration. Be sure that the retirement tates, user and application creation invocations exit without throwing any fatal errors. ALL these scripts unfortunately print a number of warnings, which can be ignored. Here's what the output of each section should look approxiately like: Retirement States Creation States have been synchronized. Differences: Added: {'COMPLETE', 'FORUMS_COMPLETE', 'ENROLLMENTS_COMPLETE', 'RETIRING_LMS_MISC', 'RETIRING_LMS', 'NOTES_COMPLETE', 'RETIRING_ENROLLMENTS', 'PENDING', 'RETIRING_NOTES', 'PROCTORING_COMPLETE', 'RETIRING_FORUMS', 'ABORTED', 'ERRORED', 'LMS_MISC_COMPLETE', 'RETIRING_PROCTORING', 'LMS_COMPLETE'} Removed: set() Remaining: set() States updated successfully. Current states: PENDING (step 1) RETIRING_FORUMS (step 11) FORUMS_COMPLETE (step 21) RETIRING_ENROLLMENTS (step 31) ENROLLMENTS_COMPLETE (step 41) RETIRING_NOTES (step 51) NOTES_COMPLETE (step 61) RETIRING_PROCTORING (step 71) PROCTORING_COMPLETE (step 81) RETIRING_LMS_MISC (step 91) LMS_MISC_COMPLETE (step 101) RETIRING_LMS (step 111) LMS_COMPLETE (step 121) ERRORED (step 131) ABORTED (step 141) COMPLETE (step 151) edxapp@ip-10-22-2-49:~$ Retirement User Creation Created new user: \"retirement_service_worker\" Setting is_staff for user \"retirement_service_worker\" to \"True\" Setting is_superuser for user \"retirement_service_worker\" to \"True\" Adding user \"retirement_service_worker\" to groups [] Removing user \"retirement_service_worker\" from groups [] 2023-04-28 21:10:00,691 INFO 69935 [common.djangoapps.student.models.user] [user None] [ip None] user.py:782 - Created new profile for user: retirement_service_worker Retirement DOT Application Creation None] [ip None] create_dot_application.py:82 - Created retirement application with id: 10, client_id: XXXXXXXXXXXXXXXXXXXXXXXX, and client_secret: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX You'll need to save this client_id and secret somewhere because you'll need to use it in the next step! We Keep Secrets In The Vault, Duh :) Next, we'll need to make these secrets accessible to the pipeline so they can be consumed by the various retirement worker scripts. In order to do that, we'll use a utility called sops to encrypt the secrets so we can safely store them in Github and conveyed to Vault. Unfortunately setting yourself up to use sops is a bit of a process in and of itself and it outside the scope of this document. Once you've got sops squared away, use it on the appropriate operational configuration file. Here's the one for QA . So you'd invoke: sops operations.qa.yml . At that point, sops will decrypt the file and open your editor of choice with its contents. Now, we want to add the necessary secrets we copied in the previous step, as well as the LMS host for this environment. You can find the LMS hosts for the various environments listed in the App Links Wiki page. If you're unsure, ask an old hand for help. Disambiguating the various environments can be tricky, and better safe than sorry! Here's an example of what I added for mitxonline qa: mitxonline/tubular_oauth_client: id: CLIENTIDUNENCRYPTEDSECRETSAREFUNYESTHEYARE secret: EVENMORESLIGHTLYLONGERGIBBERISHTHATISYOURUNENCRYPTEDSECRET host: https://courses-qa.mitxonline.mit.edu Once you've made your additions, save the file and quit your editor. sops will now do its magic and re-encrypt those values. BE SURE NOT TO COMMIT UNENDCRYPTED SECRETS TO GITHUB . Now create a pull request and get these changes merged. If this is for production, you'll also need to kick off this pipeline manually. At this point, your secrets should be in vault and available to the pipeline we're about to build. START THE RUBE GOLDBERG DEVICE! Building The Pipeline Itself In order to get this done, you'll need to have the Concourse fly CLI command installed and an appropriate target for your Concourse server and team defined. I like to keep my target names reasonably short, so for the target I created for mitxonline QA, I ran: fly login --target mo-qa --team-name=mitxonline --concourse-url https://cicd-qa.odl.mit.edu Once we have a suitable target defined, we can use it to actually create our pipeline for real: From a checked out mitodl/ol-infrastructure repository, change directory to the src/ol_concourse/pipelines/open_edx/tubular folder. Run poetry run python tubular.py - This should print a reasonable looking blob of JSON to the screen with a fly command to run at the bottom. Go ahead and run that command! For example, given the mo-qa target I defined above, I ran: fly -t mo-qa sp -p misc-cloud-tubular -c definition.json . At this point fly should show you the pipeline definition and will ask you if you want to actually make the change. Answer yes. Assuming there were no errors, your pipeline definition should be in place and we're ready to get the ball rolling! You can either use the fly unpause-pipeline or else, in the Concourse web UI, click the little Play icon at the very top right of the screen. The build is set to run once a week, so once you unpause the pipeline, you may noeed to click the + icon in order to actually kick off a build. That's it! Obviously keep an eye out on the pipeline for any failures.","title":"Setting Up The Retirement Pipeline For A New MIT OL OpenEdX Application"},{"location":"how_to/mitol_openedx_retirement_pipeline/#setting-up-the-retirement-pipeline-for-a-new-mit-ol-openedx-application","text":"","title":"Setting Up The Retirement Pipeline For A New MIT OL OpenEdX Application"},{"location":"how_to/mitol_openedx_retirement_pipeline/#getting-set-up","text":"First, follow the instructions in the Setting Up User Retirement In The LMS document. You'll need to ssh into an edx-worker for the appropriate environment you're working in. Run sudo su - edxapp -s /bin/bash and then source edxapp_env to get to where you need to be to run the Django manage application. You can follow the doc verbatim with the exception that: * The manage.py executable is located in the edx-platform folder, so you'd start your invocations with python edx-platform/manage.py * Ignore the --settings <your settings> option as we already have our settings defined in the LMS configuration. Be sure that the retirement tates, user and application creation invocations exit without throwing any fatal errors. ALL these scripts unfortunately print a number of warnings, which can be ignored. Here's what the output of each section should look approxiately like:","title":"Getting Set Up"},{"location":"how_to/mitol_openedx_retirement_pipeline/#retirement-states-creation","text":"States have been synchronized. Differences: Added: {'COMPLETE', 'FORUMS_COMPLETE', 'ENROLLMENTS_COMPLETE', 'RETIRING_LMS_MISC', 'RETIRING_LMS', 'NOTES_COMPLETE', 'RETIRING_ENROLLMENTS', 'PENDING', 'RETIRING_NOTES', 'PROCTORING_COMPLETE', 'RETIRING_FORUMS', 'ABORTED', 'ERRORED', 'LMS_MISC_COMPLETE', 'RETIRING_PROCTORING', 'LMS_COMPLETE'} Removed: set() Remaining: set() States updated successfully. Current states: PENDING (step 1) RETIRING_FORUMS (step 11) FORUMS_COMPLETE (step 21) RETIRING_ENROLLMENTS (step 31) ENROLLMENTS_COMPLETE (step 41) RETIRING_NOTES (step 51) NOTES_COMPLETE (step 61) RETIRING_PROCTORING (step 71) PROCTORING_COMPLETE (step 81) RETIRING_LMS_MISC (step 91) LMS_MISC_COMPLETE (step 101) RETIRING_LMS (step 111) LMS_COMPLETE (step 121) ERRORED (step 131) ABORTED (step 141) COMPLETE (step 151) edxapp@ip-10-22-2-49:~$","title":"Retirement States Creation"},{"location":"how_to/mitol_openedx_retirement_pipeline/#retirement-user-creation","text":"Created new user: \"retirement_service_worker\" Setting is_staff for user \"retirement_service_worker\" to \"True\" Setting is_superuser for user \"retirement_service_worker\" to \"True\" Adding user \"retirement_service_worker\" to groups [] Removing user \"retirement_service_worker\" from groups [] 2023-04-28 21:10:00,691 INFO 69935 [common.djangoapps.student.models.user] [user None] [ip None] user.py:782 - Created new profile for user: retirement_service_worker","title":"Retirement User Creation"},{"location":"how_to/mitol_openedx_retirement_pipeline/#retirement-dot-application-creation","text":"None] [ip None] create_dot_application.py:82 - Created retirement application with id: 10, client_id: XXXXXXXXXXXXXXXXXXXXXXXX, and client_secret: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX You'll need to save this client_id and secret somewhere because you'll need to use it in the next step!","title":"Retirement DOT Application Creation"},{"location":"how_to/mitol_openedx_retirement_pipeline/#we-keep-secrets-in-the-vault-duh","text":"Next, we'll need to make these secrets accessible to the pipeline so they can be consumed by the various retirement worker scripts. In order to do that, we'll use a utility called sops to encrypt the secrets so we can safely store them in Github and conveyed to Vault. Unfortunately setting yourself up to use sops is a bit of a process in and of itself and it outside the scope of this document. Once you've got sops squared away, use it on the appropriate operational configuration file. Here's the one for QA . So you'd invoke: sops operations.qa.yml . At that point, sops will decrypt the file and open your editor of choice with its contents. Now, we want to add the necessary secrets we copied in the previous step, as well as the LMS host for this environment. You can find the LMS hosts for the various environments listed in the App Links Wiki page. If you're unsure, ask an old hand for help. Disambiguating the various environments can be tricky, and better safe than sorry! Here's an example of what I added for mitxonline qa: mitxonline/tubular_oauth_client: id: CLIENTIDUNENCRYPTEDSECRETSAREFUNYESTHEYARE secret: EVENMORESLIGHTLYLONGERGIBBERISHTHATISYOURUNENCRYPTEDSECRET host: https://courses-qa.mitxonline.mit.edu Once you've made your additions, save the file and quit your editor. sops will now do its magic and re-encrypt those values. BE SURE NOT TO COMMIT UNENDCRYPTED SECRETS TO GITHUB . Now create a pull request and get these changes merged. If this is for production, you'll also need to kick off this pipeline manually. At this point, your secrets should be in vault and available to the pipeline we're about to build.","title":"We Keep Secrets In The Vault, Duh :)"},{"location":"how_to/mitol_openedx_retirement_pipeline/#start-the-rube-goldberg-device-building-the-pipeline-itself","text":"In order to get this done, you'll need to have the Concourse fly CLI command installed and an appropriate target for your Concourse server and team defined. I like to keep my target names reasonably short, so for the target I created for mitxonline QA, I ran: fly login --target mo-qa --team-name=mitxonline --concourse-url https://cicd-qa.odl.mit.edu Once we have a suitable target defined, we can use it to actually create our pipeline for real: From a checked out mitodl/ol-infrastructure repository, change directory to the src/ol_concourse/pipelines/open_edx/tubular folder. Run poetry run python tubular.py - This should print a reasonable looking blob of JSON to the screen with a fly command to run at the bottom. Go ahead and run that command! For example, given the mo-qa target I defined above, I ran: fly -t mo-qa sp -p misc-cloud-tubular -c definition.json . At this point fly should show you the pipeline definition and will ask you if you want to actually make the change. Answer yes. Assuming there were no errors, your pipeline definition should be in place and we're ready to get the ball rolling! You can either use the fly unpause-pipeline or else, in the Concourse web UI, click the little Play icon at the very top right of the screen. The build is set to run once a week, so once you unpause the pipeline, you may noeed to click the + icon in order to actually kick off a build. That's it! Obviously keep an eye out on the pipeline for any failures.","title":"START THE RUBE GOLDBERG DEVICE! Building The Pipeline Itself"},{"location":"how_to/moira_certificates/","text":"Moira Certificates There are two services in our porfolio that interact with Moira . MIT Open / OpenDiscussions ODL-Video-Service Both of these services authenticate against Moira with certificates issued by ca.mit.edu which is also the provider of MIT Personal Certificates. There is no web interface for requesting an application certificate from ca.mit.edu, so you need to email mitcert@mit.edu with the CSR in the body of the email and a clear request that you're asking for a certificate issued from ca.mit.edu and NOT InCommon/Internet2 which is where most MIT certificates now come from. Both of these applications utilize the same two environment variables for storing and accessing this key/cert pair. MIT_WS_CERTIFICATE MIT_WS_PRIVATE_KEY MITOpen Vault Locations In all vault environments: secret-mit-open/global/mit-application-certificate Maintained by hand. ODL Video Service Vault Location In all vault environments: secret-odl-video-service/ovs/secrets Inside a single JSON structure at misc.mit_ws_certificate and misc.mit_ws_private_key Maintained automatically by pulumi. sr/bridge/secrets/odl_video_service or here Certificate Usage and Expiration Tracking Action Date Application Description Who 20230625 Open Replaced MIT Open certificate, expires 20240625 MD 20230928 OVS OVS Cert expired, replaced with Open certificate above, expires 20240625 MD 20240201 Both No action. Verified certificates currently in use. Updated reminder in team calendar. MD 20240613 Both Replaced both certificates with 2024-2025 versions. Reminder sent to team calendar. MD","title":"Moira Certificates"},{"location":"how_to/moira_certificates/#moira-certificates","text":"There are two services in our porfolio that interact with Moira . MIT Open / OpenDiscussions ODL-Video-Service Both of these services authenticate against Moira with certificates issued by ca.mit.edu which is also the provider of MIT Personal Certificates. There is no web interface for requesting an application certificate from ca.mit.edu, so you need to email mitcert@mit.edu with the CSR in the body of the email and a clear request that you're asking for a certificate issued from ca.mit.edu and NOT InCommon/Internet2 which is where most MIT certificates now come from. Both of these applications utilize the same two environment variables for storing and accessing this key/cert pair. MIT_WS_CERTIFICATE MIT_WS_PRIVATE_KEY","title":"Moira Certificates"},{"location":"how_to/moira_certificates/#mitopen-vault-locations","text":"In all vault environments: secret-mit-open/global/mit-application-certificate Maintained by hand.","title":"MITOpen Vault Locations"},{"location":"how_to/moira_certificates/#odl-video-service-vault-location","text":"In all vault environments: secret-odl-video-service/ovs/secrets Inside a single JSON structure at misc.mit_ws_certificate and misc.mit_ws_private_key Maintained automatically by pulumi. sr/bridge/secrets/odl_video_service or here","title":"ODL Video Service Vault Location"},{"location":"how_to/moira_certificates/#certificate-usage-and-expiration-tracking","text":"Action Date Application Description Who 20230625 Open Replaced MIT Open certificate, expires 20240625 MD 20230928 OVS OVS Cert expired, replaced with Open certificate above, expires 20240625 MD 20240201 Both No action. Verified certificates currently in use. Updated reminder in team calendar. MD 20240613 Both Replaced both certificates with 2024-2025 versions. Reminder sent to team calendar. MD","title":"Certificate Usage and Expiration Tracking"},{"location":"how_to/openedx_release_upgrades/","text":"How To Update The Release Version Of An Open edX Deployment Open edX Releases Open edX cuts new named releases approximately every 6 months . Each of our deployments needs to be able to independently upgrade the respective version on their own schedule. MITx Residential upgrades twice per year, roughly aligned with the release timing of the upstream Open edX platform, whereas xPro has typically upgraded on an annual basis in the December timeframe. Our MITx Online deployment of Open edX is a special case in that it deploys directly from the master branches of the different applications and has a weekly release schedule. Open Learning's Open edX Deployments At Open Learning we have four distinct installations of Open edX, each with their own release cadence and configuration requirements. For each of those deployments we have multiple environment stages (e.g. CI, QA, Production). Each of those deployment * stages combinations needs to be able to have their versions managed independently, with newer versions being progressively promoted to higher environments. This is further complicated by the long testing cycles for new releases in a given deployment. We need to be able to deploy a newer release to a lower environment stage, while maintaining the ability to deploy patches of the current release in a given time period to subsequent environments. For example, as we start testing the Palm release for the MITx Residential deployment we need to get it installed in the CI environment. Meanwhile, we may have a bug-fix or security patch that needs to be tested in the QA environment and propagated to Production. In addition to the cartesian product of (versions * deployments * env stages) we also have to manage these values across the different components of the Open edX ecosystem. For example, the core of the platform is called edx-platform, which in turn relies on: - a forum service - the edx-notes API service - a codejail REST API service - and myriad MFEs (Micro-FrontEnds). For any of these different components we may need to be able to override the repository and/or branch that we are building/deploying from. Versioning of Build and Deploy In order to support this sea of complexity we have a module of bridge.settings.openedx that has a combination of data structures and helper functions to control what applications and versions get deployed where and when. These values are then used in the build stages of the different components, as well as driving the deployment pipelines in Concourse that are responsible for orchestrating all of this. Managing Supported Releases We only want to support the build and deployment of a minimal subset of Open edX releases. This is controlled by the Enum OpenEdxSupportedRelease in the bridge.settings.openedx.types module. When there is a new release created it needs to be added to this Enum. For example, to add support for the Palm release a new line is added of the format palm = (\"palm\", \"open-release/palm.master\") When we have upgraded all deployments past a given release we remove it from that Enum so that we no longer need to worry about maintaining configuration/code/etc. for that release. Performing An Upgrade There are two data structures that control the applications and versions that get included in a given deployment and which version to use in the respective environment stage. The OpenLearningOpenEdxDeployment Enum is the top level that sets the release name for the environment stage of a given deployment. There are situations where we need to customize a component that is being deployed. In those cases we typically create a fork of the upstream repository where we manage the patches that we require. The ReleaseMap dictionary is used to manage any overrides of repository and branch for a given component of the platform, as well as which components are included in that deployment. The OpenEdxApplicationVersion structure will map to the default repositories and branches for a given component, but supplies a branch_override and origin_override parameter to manage these customizations. For example, to upgrade our MITx Residential deployments to start testing the Palm release we change the CI stages of the mitx and mitx-staging deployments to use the palm value for the OpenEdxSupportedRelease @@ -13,7 +13,7 @@ class OpenLearningOpenEdxDeployment(Enum): mitx = DeploymentEnvRelease( deployment_name=\"mitx\", env_release_map=[ - EnvRelease(\"CI\", OpenEdxSupportedRelease[\"olive\"]), + EnvRelease(\"CI\", OpenEdxSupportedRelease[\"palm\"]), EnvRelease(\"QA\", OpenEdxSupportedRelease[\"olive\"]), EnvRelease(\"Production\", OpenEdxSupportedRelease[\"olive\"]), ], @@ -21,7 +21,7 @@ class OpenLearningOpenEdxDeployment(Enum): mitx_staging = DeploymentEnvRelease( deployment_name=\"mitx-staging\", env_release_map=[ - EnvRelease(\"CI\", OpenEdxSupportedRelease[\"olive\"]), + EnvRelease(\"CI\", OpenEdxSupportedRelease[\"palm\"]), EnvRelease(\"QA\", OpenEdxSupportedRelease[\"olive\"]), EnvRelease(\"Production\", OpenEdxSupportedRelease[\"olive\"]), ], Because Palm is a new release for these deployments we also need to add a palm key to the ReleaseMap dictionary that contains the applications that are associated with those deployments and the appropriate OpenEdxApplicationVersion records for that deployment. @@ -61,6 +61,122 @@ ReleaseMap: dict[ OpenEdxSupportedRelease, dict[OpenEdxDeploymentName, list[OpenEdxApplicationVersion]], ] = { + \"palm\": { + \"mitx\": [ + OpenEdxApplicationVersion( + application=\"edx-platform\", # type: ignore + application_type=\"IDA\", + release=\"palm\", + branch_override=\"mitx/palm\", + origin_override=\"https://github.com/mitodl/edx-platform\", + ), ... + ], + \"mitx-staging\": [ + OpenEdxApplicationVersion( + application=\"edx-platform\", # type: ignore + application_type=\"IDA\", + release=\"palm\", + branch_override=\"mitx/palm\", + origin_override=\"https://github.com/mitodl/edx-platform\", + ), ... + ], + }, \"olive\": { \"mitx\": [ OpenEdxApplicationVersion( All of the deployment pipelines for these application components are managed by a corresponding meta pipeline that will automatically update the build and pipeline configuration based on the changed version information as soon as it is merged into the master branch of ol-infrastructure . Supporting New Applications There are two categories of applications that comprise the overall Open edX platform. These are \"IDA\"s (Independently Deployable Applications), and \"MFE\"s (Micro Front-Ends). An IDA is a Django application that can be deployed as a backend service and typically integrates with the edx-platform (LMS and CMS) via OAuth. An MFE is a ReactJS application that is deployed as a standalone site which then interacts with LMS and/or CMS via REST APIs. In order to ensure that we have visibility into which of these components we are deploying there is an Enum of OpenEdxApplication and OpenEdxMicroFrontend respectively which captures the details of which elements of the overall edX ecosystem we are supporting in our deployments. These Enums are then used as an attribute of the OpenEdxApplicationVersion model which captures the details of a specific instance of one of those components. These are then used in the ReleaseMap dict to show which versions and deployments include the given instance of that application and version. To add support for a new IDA or MFE you need to add a new entry to the appropriate Enum, and then include an instance of an OpenEdxApplicationVersion that points to that application in the ReleaseMap at the appropriate location. Note - TMM 2023-04-14 The current configuration of our meta pipelines means that before the updates to the bridge.settings.openedx.version_matrix module can be picked up by the meta pipelines the ol-infrastructure docker image needs to be built and pushed to our registry so that it can be loaded. This means that once the ol-infrastructure image pipeline completes it might be necessary to manually trigger the meta pipelines again. Once a new release is deployed to a given environment stage for the first time it may be necessary to manually ensure that all database migrations are run properly. It will be attempted automatically on deployment, but there are often conflicts between the existing database state and the migration logic that require intervention. Updating An MITOL OpenEdX Branch With The Latest This involves updating our edx-platform fork and rebasing the correct branch. For instance, to update our forked OpenEdX Sumac branch for MITxOnline, you would: Check out our fork of the edx-platform repo - gh repo clone mitodl/edx-platform Change to the right branch - git checkout mitx/sumac Update all the things - git fetch --all Rebase from upstream - git pull --rebase upstream open-release/sumac.master Check the results, and if correct push the branch - git push origin mitx/sumac --force Troubleshooting OpenEdX Redwood With each new OpenEdX release, inevitably there will be problems. For instance, in the Redwood release, we noticed that login for MITX CI was failing with a 500 response code. So we ran the following Grafana query using the query builder: {environment=\"mitx-ci\", application=\"edxapp\"} |= `` | json | line_format `{{.message}} That yielded a problem with JWT signing keys as the exception cited a signing failure. So we had to log onto an EC2 instance appropriate to MITX CI and run the following command from the lms container shell: manage.py generate_jwt_signing_key That produced public and private signing key JSON blobs, which we could then add to SOPS and then through the pipeline into Vault where they'll be picked up as Redwood deploys to the various apps and environments.","title":"How To Update The Release Version Of An Open edX Deployment"},{"location":"how_to/openedx_release_upgrades/#how-to-update-the-release-version-of-an-open-edx-deployment","text":"","title":"How To Update The Release Version Of An Open edX Deployment"},{"location":"how_to/openedx_release_upgrades/#open-edx-releases","text":"Open edX cuts new named releases approximately every 6 months . Each of our deployments needs to be able to independently upgrade the respective version on their own schedule. MITx Residential upgrades twice per year, roughly aligned with the release timing of the upstream Open edX platform, whereas xPro has typically upgraded on an annual basis in the December timeframe. Our MITx Online deployment of Open edX is a special case in that it deploys directly from the master branches of the different applications and has a weekly release schedule.","title":"Open edX Releases"},{"location":"how_to/openedx_release_upgrades/#open-learnings-open-edx-deployments","text":"At Open Learning we have four distinct installations of Open edX, each with their own release cadence and configuration requirements. For each of those deployments we have multiple environment stages (e.g. CI, QA, Production). Each of those deployment * stages combinations needs to be able to have their versions managed independently, with newer versions being progressively promoted to higher environments. This is further complicated by the long testing cycles for new releases in a given deployment. We need to be able to deploy a newer release to a lower environment stage, while maintaining the ability to deploy patches of the current release in a given time period to subsequent environments. For example, as we start testing the Palm release for the MITx Residential deployment we need to get it installed in the CI environment. Meanwhile, we may have a bug-fix or security patch that needs to be tested in the QA environment and propagated to Production. In addition to the cartesian product of (versions * deployments * env stages) we also have to manage these values across the different components of the Open edX ecosystem. For example, the core of the platform is called edx-platform, which in turn relies on: - a forum service - the edx-notes API service - a codejail REST API service - and myriad MFEs (Micro-FrontEnds). For any of these different components we may need to be able to override the repository and/or branch that we are building/deploying from.","title":"Open Learning's Open edX Deployments"},{"location":"how_to/openedx_release_upgrades/#versioning-of-build-and-deploy","text":"In order to support this sea of complexity we have a module of bridge.settings.openedx that has a combination of data structures and helper functions to control what applications and versions get deployed where and when. These values are then used in the build stages of the different components, as well as driving the deployment pipelines in Concourse that are responsible for orchestrating all of this.","title":"Versioning of Build and Deploy"},{"location":"how_to/openedx_release_upgrades/#managing-supported-releases","text":"We only want to support the build and deployment of a minimal subset of Open edX releases. This is controlled by the Enum OpenEdxSupportedRelease in the bridge.settings.openedx.types module. When there is a new release created it needs to be added to this Enum. For example, to add support for the Palm release a new line is added of the format palm = (\"palm\", \"open-release/palm.master\") When we have upgraded all deployments past a given release we remove it from that Enum so that we no longer need to worry about maintaining configuration/code/etc. for that release.","title":"Managing Supported Releases"},{"location":"how_to/openedx_release_upgrades/#performing-an-upgrade","text":"There are two data structures that control the applications and versions that get included in a given deployment and which version to use in the respective environment stage. The OpenLearningOpenEdxDeployment Enum is the top level that sets the release name for the environment stage of a given deployment. There are situations where we need to customize a component that is being deployed. In those cases we typically create a fork of the upstream repository where we manage the patches that we require. The ReleaseMap dictionary is used to manage any overrides of repository and branch for a given component of the platform, as well as which components are included in that deployment. The OpenEdxApplicationVersion structure will map to the default repositories and branches for a given component, but supplies a branch_override and origin_override parameter to manage these customizations. For example, to upgrade our MITx Residential deployments to start testing the Palm release we change the CI stages of the mitx and mitx-staging deployments to use the palm value for the OpenEdxSupportedRelease @@ -13,7 +13,7 @@ class OpenLearningOpenEdxDeployment(Enum): mitx = DeploymentEnvRelease( deployment_name=\"mitx\", env_release_map=[ - EnvRelease(\"CI\", OpenEdxSupportedRelease[\"olive\"]), + EnvRelease(\"CI\", OpenEdxSupportedRelease[\"palm\"]), EnvRelease(\"QA\", OpenEdxSupportedRelease[\"olive\"]), EnvRelease(\"Production\", OpenEdxSupportedRelease[\"olive\"]), ], @@ -21,7 +21,7 @@ class OpenLearningOpenEdxDeployment(Enum): mitx_staging = DeploymentEnvRelease( deployment_name=\"mitx-staging\", env_release_map=[ - EnvRelease(\"CI\", OpenEdxSupportedRelease[\"olive\"]), + EnvRelease(\"CI\", OpenEdxSupportedRelease[\"palm\"]), EnvRelease(\"QA\", OpenEdxSupportedRelease[\"olive\"]), EnvRelease(\"Production\", OpenEdxSupportedRelease[\"olive\"]), ], Because Palm is a new release for these deployments we also need to add a palm key to the ReleaseMap dictionary that contains the applications that are associated with those deployments and the appropriate OpenEdxApplicationVersion records for that deployment. @@ -61,6 +61,122 @@ ReleaseMap: dict[ OpenEdxSupportedRelease, dict[OpenEdxDeploymentName, list[OpenEdxApplicationVersion]], ] = { + \"palm\": { + \"mitx\": [ + OpenEdxApplicationVersion( + application=\"edx-platform\", # type: ignore + application_type=\"IDA\", + release=\"palm\", + branch_override=\"mitx/palm\", + origin_override=\"https://github.com/mitodl/edx-platform\", + ), ... + ], + \"mitx-staging\": [ + OpenEdxApplicationVersion( + application=\"edx-platform\", # type: ignore + application_type=\"IDA\", + release=\"palm\", + branch_override=\"mitx/palm\", + origin_override=\"https://github.com/mitodl/edx-platform\", + ), ... + ], + }, \"olive\": { \"mitx\": [ OpenEdxApplicationVersion( All of the deployment pipelines for these application components are managed by a corresponding meta pipeline that will automatically update the build and pipeline configuration based on the changed version information as soon as it is merged into the master branch of ol-infrastructure .","title":"Performing An Upgrade"},{"location":"how_to/openedx_release_upgrades/#supporting-new-applications","text":"There are two categories of applications that comprise the overall Open edX platform. These are \"IDA\"s (Independently Deployable Applications), and \"MFE\"s (Micro Front-Ends). An IDA is a Django application that can be deployed as a backend service and typically integrates with the edx-platform (LMS and CMS) via OAuth. An MFE is a ReactJS application that is deployed as a standalone site which then interacts with LMS and/or CMS via REST APIs. In order to ensure that we have visibility into which of these components we are deploying there is an Enum of OpenEdxApplication and OpenEdxMicroFrontend respectively which captures the details of which elements of the overall edX ecosystem we are supporting in our deployments. These Enums are then used as an attribute of the OpenEdxApplicationVersion model which captures the details of a specific instance of one of those components. These are then used in the ReleaseMap dict to show which versions and deployments include the given instance of that application and version. To add support for a new IDA or MFE you need to add a new entry to the appropriate Enum, and then include an instance of an OpenEdxApplicationVersion that points to that application in the ReleaseMap at the appropriate location. Note - TMM 2023-04-14 The current configuration of our meta pipelines means that before the updates to the bridge.settings.openedx.version_matrix module can be picked up by the meta pipelines the ol-infrastructure docker image needs to be built and pushed to our registry so that it can be loaded. This means that once the ol-infrastructure image pipeline completes it might be necessary to manually trigger the meta pipelines again. Once a new release is deployed to a given environment stage for the first time it may be necessary to manually ensure that all database migrations are run properly. It will be attempted automatically on deployment, but there are often conflicts between the existing database state and the migration logic that require intervention.","title":"Supporting New Applications"},{"location":"how_to/openedx_release_upgrades/#updating-an-mitol-openedx-branch-with-the-latest","text":"This involves updating our edx-platform fork and rebasing the correct branch. For instance, to update our forked OpenEdX Sumac branch for MITxOnline, you would: Check out our fork of the edx-platform repo - gh repo clone mitodl/edx-platform Change to the right branch - git checkout mitx/sumac Update all the things - git fetch --all Rebase from upstream - git pull --rebase upstream open-release/sumac.master Check the results, and if correct push the branch - git push origin mitx/sumac --force","title":"Updating An MITOL OpenEdX Branch With The Latest"},{"location":"how_to/openedx_release_upgrades/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"how_to/openedx_release_upgrades/#openedx-redwood","text":"With each new OpenEdX release, inevitably there will be problems. For instance, in the Redwood release, we noticed that login for MITX CI was failing with a 500 response code. So we ran the following Grafana query using the query builder: {environment=\"mitx-ci\", application=\"edxapp\"} |= `` | json | line_format `{{.message}} That yielded a problem with JWT signing keys as the exception cited a signing failure. So we had to log onto an EC2 instance appropriate to MITX CI and run the following command from the lms container shell: manage.py generate_jwt_signing_key That produced public and private signing key JSON blobs, which we could then add to SOPS and then through the pipeline into Vault where they'll be picked up as Redwood deploys to the various apps and environments.","title":"OpenEdX Redwood"},{"location":"how_to/publish_to_npm/","text":"How to Publish an MIT OL Package to NPM Note : This is a VERY rough document. We hate the current process, but I'm a big believer in not allowing the perfect to be the enemy of the good :) Pre-Requisites You will need Vault access for production. You'll need to know which Github repository needs publhsing Setup Git Check out the Git repository that equates to the NPM module that needs publishing. Usually the dev asking for help can give you this if it's not obvious. npm login Fun with Vault's Web UI Now login to npm from the command line. You'll need to have npm and node installed in order for this to work. If you're on a Mac, you can use homebrew brew install npm but YMMV. You'll need the username and TOTP code we use for this. Get from the production Vault instance . Navigate to \"platform-secrets\" and look for the 'npmjs' entry. This will get you the username and password. Now you'll need the TOTP code. Click the eye icon to reveal the contents of the 'totp-path-mitx-devops' entry. Now click the icon to the left of the eye to copy the command you'll need to run to your local clipboard. Open the CLI by clicking on the little black box with the '>' in it in the very upper right of the screen. Now paste the contents if your clipboard into the bottom section of the screen where you can enter commands. This will get you the TOTP code you'll need. To the CLI! To login to npm on the command line and complete setup, run: npm login mitx-devops and hit return. This will prompt to open a web page. You'll need to supply the password and TOTP code you got in the previous step. If it's been too long since you gathered the TOTP code, you may need to hit up-arrow in the Vault web UI CLI and get a fresh TOTP code. If you're successful you should see something like: Logged in on https://registry.npmjs.org/. Doing The Actual Publishing Now change directory to the Git repo you checked our earlier and type: npm publish . If it blows up, check the packages.json file and ensure the the organization is set correctly. It should be 'mitodl'. See this commit for an example fix.","title":"How to Publish an MIT OL Package to NPM"},{"location":"how_to/publish_to_npm/#how-to-publish-an-mit-ol-package-to-npm","text":"Note : This is a VERY rough document. We hate the current process, but I'm a big believer in not allowing the perfect to be the enemy of the good :)","title":"How to Publish an MIT OL Package to NPM"},{"location":"how_to/publish_to_npm/#pre-requisites","text":"You will need Vault access for production. You'll need to know which Github repository needs publhsing","title":"Pre-Requisites"},{"location":"how_to/publish_to_npm/#setup","text":"","title":"Setup"},{"location":"how_to/publish_to_npm/#git","text":"Check out the Git repository that equates to the NPM module that needs publishing. Usually the dev asking for help can give you this if it's not obvious.","title":"Git"},{"location":"how_to/publish_to_npm/#npm-login","text":"","title":"npm login"},{"location":"how_to/publish_to_npm/#fun-with-vaults-web-ui","text":"Now login to npm from the command line. You'll need to have npm and node installed in order for this to work. If you're on a Mac, you can use homebrew brew install npm but YMMV. You'll need the username and TOTP code we use for this. Get from the production Vault instance . Navigate to \"platform-secrets\" and look for the 'npmjs' entry. This will get you the username and password. Now you'll need the TOTP code. Click the eye icon to reveal the contents of the 'totp-path-mitx-devops' entry. Now click the icon to the left of the eye to copy the command you'll need to run to your local clipboard. Open the CLI by clicking on the little black box with the '>' in it in the very upper right of the screen. Now paste the contents if your clipboard into the bottom section of the screen where you can enter commands. This will get you the TOTP code you'll need.","title":"Fun with Vault's Web UI"},{"location":"how_to/publish_to_npm/#to-the-cli","text":"To login to npm on the command line and complete setup, run: npm login mitx-devops and hit return. This will prompt to open a web page. You'll need to supply the password and TOTP code you got in the previous step. If it's been too long since you gathered the TOTP code, you may need to hit up-arrow in the Vault web UI CLI and get a fresh TOTP code. If you're successful you should see something like: Logged in on https://registry.npmjs.org/.","title":"To the CLI!"},{"location":"how_to/publish_to_npm/#doing-the-actual-publishing","text":"Now change directory to the Git repo you checked our earlier and type: npm publish . If it blows up, check the packages.json file and ensure the the organization is set correctly. It should be 'mitodl'. See this commit for an example fix.","title":"Doing The Actual Publishing"},{"location":"how_to/pulumi_aws_classic_5_to_6_upgrade/","text":"Pulumi AWS Classic 4 To 5 Upgrade So, this is roughly what I believe is happening. So you guys can know too. I updated / tested a bunch of stacks, mostly CI or sometimes QA, yesterday with the pulumi-aws 6.5.0 upgrade. That is a two part thing. Python package but also provider. Part of that upgrade, pulumi changed some of the state/stack structures and they are no longer backwards compatible, in particular those around RDS instances. So I, by hand, upgraded a bunch of stacks to the new provider, they migrate nicely everything is good. Concourse comes around and something triggers it and it applies the stacks again but with the old provider. It works okay the first time, but when the second time comes around it decides it has lost track of the RDS instance and it tries to recreate it. If it DOES try to recreate it, thankfully it fails, because the RDS instance is still there, just pulumi has lost it, and a duplicate error gets thrown and the apply/up fails. But, now the stack is broken and requires manual intervention to revive it. So what I do is I go into the history/checkpoint area in S3 and find the most recent checkpoint that references the 6.5.0 provider (the one I upgraded to). I pull that down. Checkpoints can\u2019t be imported directly they need to be fixed. Fix it with this: https://gist.github.com/clstokes/977b7bd00b37e0a564f707f0ebe36e08 pr pulumi stack import -s <stackname> --file <fixed stack checkpoint file> poetry install using an environment pre-6.5.0 upgrade if necessary (I keep multiple ol-inf envs for exactly this kind of thing so I just switch between my 5.4 and 6.5 envs). pulumi plugin rm resource aws 6.5.0 uninstall the PROVIDER pr pulumi up --refresh -s <stackname> x2 \u2014 Shouldn\u2019t be trying to create a database anymore. Many of the stacks were fine because they either: didn\u2019t have RDS resources didn\u2019t get up\u2019d by concourse inbetween. BUT! BUT! When the PR making this upgrade was merged to main , it trigged nearly everything. Very sad. :disappointed: But the code has been merged so that is fine right? Wrong. The resources that concourse uses to do the needful are not upgraded yet. Specifically these: https://hub.docker.com/r/mitodl/concourse-pulumi-resource-provisioner https://hub.docker.com/r/mitodl/concourse-pulumi-resource https://hub.docker.com/r/mitodl/ol-infrastructure These actually have a complicated silent depedency between them, but basically mitodl/ol-infrastructure is a base layer for the other two. These builds also kicked off (maybe? I did run them by hand too\u2026) when the PR was merged but they didn\u2019t publish to dockerhub before concourse started trying to update CI/QA stacks automatically. EVEN IF they had published to dockerhub before, it wouldn\u2019t matter because servers-be-cachin\u2019. The concourse workers were holding on to their old versions and efficiently re-using them, ignorant of the new versions available out on dockerhub. Solve this by doing and instance refresh on concourse workers. New servers do new docker pull on the resources needed. Addendum: Made a neat script to pause concourse pipelines en-mass. Should have run this before #8 :disappointed: for pl in $(fly -t pr-inf ps --json | jq -r '.[].name'); do fly -t pr-inf pp -p $pl done","title":"Pulumi AWS Classic 4 To 5 Upgrade"},{"location":"how_to/pulumi_aws_classic_5_to_6_upgrade/#pulumi-aws-classic-4-to-5-upgrade","text":"So, this is roughly what I believe is happening. So you guys can know too. I updated / tested a bunch of stacks, mostly CI or sometimes QA, yesterday with the pulumi-aws 6.5.0 upgrade. That is a two part thing. Python package but also provider. Part of that upgrade, pulumi changed some of the state/stack structures and they are no longer backwards compatible, in particular those around RDS instances. So I, by hand, upgraded a bunch of stacks to the new provider, they migrate nicely everything is good. Concourse comes around and something triggers it and it applies the stacks again but with the old provider. It works okay the first time, but when the second time comes around it decides it has lost track of the RDS instance and it tries to recreate it. If it DOES try to recreate it, thankfully it fails, because the RDS instance is still there, just pulumi has lost it, and a duplicate error gets thrown and the apply/up fails. But, now the stack is broken and requires manual intervention to revive it. So what I do is I go into the history/checkpoint area in S3 and find the most recent checkpoint that references the 6.5.0 provider (the one I upgraded to). I pull that down. Checkpoints can\u2019t be imported directly they need to be fixed. Fix it with this: https://gist.github.com/clstokes/977b7bd00b37e0a564f707f0ebe36e08 pr pulumi stack import -s <stackname> --file <fixed stack checkpoint file> poetry install using an environment pre-6.5.0 upgrade if necessary (I keep multiple ol-inf envs for exactly this kind of thing so I just switch between my 5.4 and 6.5 envs). pulumi plugin rm resource aws 6.5.0 uninstall the PROVIDER pr pulumi up --refresh -s <stackname> x2 \u2014 Shouldn\u2019t be trying to create a database anymore. Many of the stacks were fine because they either: didn\u2019t have RDS resources didn\u2019t get up\u2019d by concourse inbetween. BUT! BUT! When the PR making this upgrade was merged to main , it trigged nearly everything. Very sad. :disappointed: But the code has been merged so that is fine right? Wrong. The resources that concourse uses to do the needful are not upgraded yet. Specifically these: https://hub.docker.com/r/mitodl/concourse-pulumi-resource-provisioner https://hub.docker.com/r/mitodl/concourse-pulumi-resource https://hub.docker.com/r/mitodl/ol-infrastructure These actually have a complicated silent depedency between them, but basically mitodl/ol-infrastructure is a base layer for the other two. These builds also kicked off (maybe? I did run them by hand too\u2026) when the PR was merged but they didn\u2019t publish to dockerhub before concourse started trying to update CI/QA stacks automatically. EVEN IF they had published to dockerhub before, it wouldn\u2019t matter because servers-be-cachin\u2019. The concourse workers were holding on to their old versions and efficiently re-using them, ignorant of the new versions available out on dockerhub. Solve this by doing and instance refresh on concourse workers. New servers do new docker pull on the resources needed. Addendum: Made a neat script to pause concourse pipelines en-mass. Should have run this before #8 :disappointed: for pl in $(fly -t pr-inf ps --json | jq -r '.[].name'); do fly -t pr-inf pp -p $pl done","title":"Pulumi AWS Classic 4 To 5 Upgrade"},{"location":"how_to/recaptcha/","text":"reCAPTCHA Errors ERROR for site owner: Invalid domain for site key. Need to login to google with the mitx devops username and password (vault-production -> platform-secrets/google ). It will need to do an email verification code to one of our lists if you don't have an active session. Make your way to the reCAPTCHA console located here . There is a dropdown on the top left that lets you see which site/application configuration that you're working with. Once you're on the site that you care about, there is a gear icon on the top right. Click that for the settings. Three things to verify: Does the site key match what is listed in app configuration + vault? Does the secret key match what is listed in app configuration + vault? Is the list of domains correct? NOTE: When checking keys, look at the end of the string rather than the start. They all seem to start the same.","title":"reCAPTCHA Errors"},{"location":"how_to/recaptcha/#recaptcha-errors","text":"ERROR for site owner: Invalid domain for site key. Need to login to google with the mitx devops username and password (vault-production -> platform-secrets/google ). It will need to do an email verification code to one of our lists if you don't have an active session. Make your way to the reCAPTCHA console located here . There is a dropdown on the top left that lets you see which site/application configuration that you're working with. Once you're on the site that you care about, there is a gear icon on the top right. Click that for the settings. Three things to verify: Does the site key match what is listed in app configuration + vault? Does the secret key match what is listed in app configuration + vault? Is the list of domains correct? NOTE: When checking keys, look at the end of the string rather than the start. They all seem to start the same.","title":"reCAPTCHA Errors"},{"location":"how_to/restore_vault_backups/","text":"So you broke vault? Symptoms The vault UI or CLI reports that the cluster is sealed and there is no amount of coaxing to get it out of that state. There may be other symptoms. Add to this list as needed. root@ip-172-16-0-82:/etc/vault# VAULT_SKIP_VERIFY=true vault status Key Value --- ----- Recovery Seal Type awskms Initialized false <<<<< Ahh! Bad! Sealed true <<<<< Sealed! Total Recovery Shares 0 Threshold 0 Unseal Progress 0/0 <<<<< This node knowns nothing about any other nodes Unseal Nonce n/a Version 1.14.1 Build Date 2023-07-21T10:15:14Z Storage Type raft HA Enabled true <<<<< And it is supposed too! What to do!?! Go to the apporpriate S3 bucket and find the most recent backup. Production backs up every six hours. QA + CI backup once a day. Bucket names are: ol-infra-ci-vault-backups , ol-infra-qa-vault-backups , ol-infra-production-vault-backups . Download the most recent backup locally to your machine and stage it to the vault node you're going to run the restore on. You only run this procedure ONCE! You do not need to run this on every vault node. scp -i <path to your oldevops.pem ssh private key> <path to downloaded .snapshot file> admin@<IP address of the vault node you're restoring to>:/tmp On the node that you've copied the .snapshot file to, verify that the vault status outputs as above. Export a vault setting to make life less annoying export VAULT_SKIP_VERIFY=true Initialize the vault cluster: vault operator init Output will look like this: Recovery Key 1: bX5A***********************************ExhBC Recovery Key 2: XHo3***********************************LZxmZ Recovery Key 3: EEOE***********************************XWC8p Recovery Key 4: FyTq***********************************EY0ij Recovery Key 5: oXaW***********************************Wr74k Initial Root Token: hvs.**********************wf Success! Vault is initialized Recovery key initialized with 5 key shares and a key threshold of 3. Please securely distribute the key shares printed above. The message says those recovery keys are important but they aren't in this case. You don't need to save them. What you do need is the initial root token. Export that into env var: export VAULT_TOKEN=<token value including hvs. from prev command output> Do the restore: vault operator raft snapshot restore <path to .snapshot file> . It doesn't actually output anything. Unset VAULT_TOKEN with unset VAULT_TOKEN (because that token was tied the shortlived cluster that existed before running the restore). Then do a vault status: vault status and it should look something like this: Key Value --- ----- Recovery Seal Type shamir Initialized true Sealed false <<<< Victory! Total Recovery Shares 2 Threshold 2 Version 1.14.1 Build Date 2023-07-21T10:15:14Z Storage Type raft Cluster Name vault-cluster-ab3e7a1b Cluster ID ef172e45-fake-uuid-here-aeb8da8a7179 HA Enabled true HA Cluster https://256.256.256.256:8201 HA Mode standby Active Node Address https://active.vault.service.consul:8200 Raft Committed Index 815551 Raft Applied Index 815551 Loop through the other nodes in the cluster and verify they have a similar vault unseal status. If they don't, try systemctl restart vault . They should join backup and restore the raft on their own provided they were broken to begin with. Verify that the vault UI works as excpected again and your secrets are there. Some, possibly serious, complications after doing this: it is possible that there are credential secrets out in the wild being used that are no longer tracked via leases in vault. For instance if they were issued between when the backup took place and when the cluster stopped being viable. This could be the case for PKI secrets as well and of course if any static secrets in vaults were changed between the backup and beginning of the outage.","title":"So you broke vault?"},{"location":"how_to/restore_vault_backups/#so-you-broke-vault","text":"","title":"So you broke vault?"},{"location":"how_to/restore_vault_backups/#symptoms","text":"The vault UI or CLI reports that the cluster is sealed and there is no amount of coaxing to get it out of that state. There may be other symptoms. Add to this list as needed. root@ip-172-16-0-82:/etc/vault# VAULT_SKIP_VERIFY=true vault status Key Value --- ----- Recovery Seal Type awskms Initialized false <<<<< Ahh! Bad! Sealed true <<<<< Sealed! Total Recovery Shares 0 Threshold 0 Unseal Progress 0/0 <<<<< This node knowns nothing about any other nodes Unseal Nonce n/a Version 1.14.1 Build Date 2023-07-21T10:15:14Z Storage Type raft HA Enabled true <<<<< And it is supposed too!","title":"Symptoms"},{"location":"how_to/restore_vault_backups/#what-to-do","text":"Go to the apporpriate S3 bucket and find the most recent backup. Production backs up every six hours. QA + CI backup once a day. Bucket names are: ol-infra-ci-vault-backups , ol-infra-qa-vault-backups , ol-infra-production-vault-backups . Download the most recent backup locally to your machine and stage it to the vault node you're going to run the restore on. You only run this procedure ONCE! You do not need to run this on every vault node. scp -i <path to your oldevops.pem ssh private key> <path to downloaded .snapshot file> admin@<IP address of the vault node you're restoring to>:/tmp On the node that you've copied the .snapshot file to, verify that the vault status outputs as above. Export a vault setting to make life less annoying export VAULT_SKIP_VERIFY=true Initialize the vault cluster: vault operator init Output will look like this: Recovery Key 1: bX5A***********************************ExhBC Recovery Key 2: XHo3***********************************LZxmZ Recovery Key 3: EEOE***********************************XWC8p Recovery Key 4: FyTq***********************************EY0ij Recovery Key 5: oXaW***********************************Wr74k Initial Root Token: hvs.**********************wf Success! Vault is initialized Recovery key initialized with 5 key shares and a key threshold of 3. Please securely distribute the key shares printed above. The message says those recovery keys are important but they aren't in this case. You don't need to save them. What you do need is the initial root token. Export that into env var: export VAULT_TOKEN=<token value including hvs. from prev command output> Do the restore: vault operator raft snapshot restore <path to .snapshot file> . It doesn't actually output anything. Unset VAULT_TOKEN with unset VAULT_TOKEN (because that token was tied the shortlived cluster that existed before running the restore). Then do a vault status: vault status and it should look something like this: Key Value --- ----- Recovery Seal Type shamir Initialized true Sealed false <<<< Victory! Total Recovery Shares 2 Threshold 2 Version 1.14.1 Build Date 2023-07-21T10:15:14Z Storage Type raft Cluster Name vault-cluster-ab3e7a1b Cluster ID ef172e45-fake-uuid-here-aeb8da8a7179 HA Enabled true HA Cluster https://256.256.256.256:8201 HA Mode standby Active Node Address https://active.vault.service.consul:8200 Raft Committed Index 815551 Raft Applied Index 815551 Loop through the other nodes in the cluster and verify they have a similar vault unseal status. If they don't, try systemctl restart vault . They should join backup and restore the raft on their own provided they were broken to begin with. Verify that the vault UI works as excpected again and your secrets are there. Some, possibly serious, complications after doing this: it is possible that there are credential secrets out in the wild being used that are no longer tracked via leases in vault. For instance if they were issued between when the backup took place and when the cluster stopped being viable. This could be the case for PKI secrets as well and of course if any static secrets in vaults were changed between the backup and beginning of the outage.","title":"What to do!?!"},{"location":"how_to/retire_an_openedx_user/","text":"How To Retire An MIT Online Learning OpenEdX User Pre-Requisites You'll need the email address the user registered under. e.g. cpatti@mit.edu You'll need the pre-requisites defined in How To Access An MIT OL OpenEdX Django Admin manage.py . Look Up This User's username You'll need to have Django admin and superuser access to the product you're looking to retire users for. Anyone who has the requisite access already can help. pdpinch@ and the Devops team are good folks to ask. Next, you'll need to login to the admin web UI for the application we'll be working with. You can find that URL from the Application Links Page . Find the product in question (e.g. mitxonline) and choose the LMS url for the environment you want (e.g. production) as a base. Then add /admin on to the end. Ensure there's only one slash before /admin or things will go awry. In our case, since we are looking to retire users from mitxonline, we'll use https://courses.mitxonline.mit.edu/admin Find the Users link on that page, click it. Now type the email address into the search box and click Search. This should yield the user's username. Copy that off into a safe place as we'll need it for the next section. Get Yourself Connected First, follow the step by step instructions defined in How To Access An MIT OL OpenEdX Django Admin manage.py . This will land you at a shell prompt of the OpenEdX worker for the product in question. It should look something like: ubuntu@ip-10-22-3-162:~$ Prepare Your Environment Next you'll need to prepare your UNIX shell's environment to be able to run the manage.py command. Type: sudo su - edxapp -s /bin/bash Then type source edxapp_env . At this point you should be ready to run the user retirement command. Get To The Retiring Already! The Invocation Here's the command you'll use to retire a user. python edx-platform/manage.py lms retire_user --user_email <user email> --username '<username>' The single quotes around username are important in case there are any spaces in there. Otherwise the shell will mis-parse the command and throw an error. So for example if I wanted to retire myself from mitxonline production, I'd use: python edx-platform/manage.py lms retire_user --user_email cpatti@mit.edu --username 'ChrisPatti' You should see a bunch of very voluminous output. Most of it is honestly garbage for our purposes. We'll focus on the bits we care about at the end: 2023-06-21 16:07:20,714 INFO 186970 [openedx.core.djangoapps.user_api.management.commands.retire_user] [user None] [ip None] retire_user.py:173 - User succesfully moved to the retirment pipeline The None here isn't anything to worry about. It's the system trying to prevent us from leaking PII (personally identifiable information) into the logs. At this point if all went well, we're done with our edx worker shell prompt for now so we can log out. Always be super careful to not leave production shells open unnecessarily. You'd be surprised how many systems have been brought down by someone not realizing they're in the wrong terminal :) Priming The Pump (Well, Pipeline In This Case) Now that we've successfully staged our user for retirement, we need to tell the retirement pipeline to actually retire the user. Surf to the appropriate concourse URL for the entironment you're working with: CI QA Production and search for 'tubular'. You'll want the tubular pipeline in the group associated with whichever product you're working with. In our case, it'd be the misc-cloud-tubular pipeline in the mitxonline group . - Click the green + icon with a circle around it in the upper right of your screen. This will trigger a run of this pipeline. - If all goes well, you should see each stage go green one by one. You can click on any stage to see more detail around what that stage is doing. You can see an example of a successful pipeline run here At this point, if the pipeline is green, congratulations are in order! This user had been retired!","title":"How To Retire An MIT Online Learning OpenEdX User"},{"location":"how_to/retire_an_openedx_user/#how-to-retire-an-mit-online-learning-openedx-user","text":"","title":"How To Retire An MIT Online Learning OpenEdX User"},{"location":"how_to/retire_an_openedx_user/#pre-requisites","text":"You'll need the email address the user registered under. e.g. cpatti@mit.edu You'll need the pre-requisites defined in How To Access An MIT OL OpenEdX Django Admin manage.py .","title":"Pre-Requisites"},{"location":"how_to/retire_an_openedx_user/#look-up-this-users-username","text":"You'll need to have Django admin and superuser access to the product you're looking to retire users for. Anyone who has the requisite access already can help. pdpinch@ and the Devops team are good folks to ask. Next, you'll need to login to the admin web UI for the application we'll be working with. You can find that URL from the Application Links Page . Find the product in question (e.g. mitxonline) and choose the LMS url for the environment you want (e.g. production) as a base. Then add /admin on to the end. Ensure there's only one slash before /admin or things will go awry. In our case, since we are looking to retire users from mitxonline, we'll use https://courses.mitxonline.mit.edu/admin Find the Users link on that page, click it. Now type the email address into the search box and click Search. This should yield the user's username. Copy that off into a safe place as we'll need it for the next section.","title":"Look Up This User's username"},{"location":"how_to/retire_an_openedx_user/#get-yourself-connected","text":"First, follow the step by step instructions defined in How To Access An MIT OL OpenEdX Django Admin manage.py . This will land you at a shell prompt of the OpenEdX worker for the product in question. It should look something like: ubuntu@ip-10-22-3-162:~$","title":"Get Yourself Connected"},{"location":"how_to/retire_an_openedx_user/#prepare-your-environment","text":"Next you'll need to prepare your UNIX shell's environment to be able to run the manage.py command. Type: sudo su - edxapp -s /bin/bash Then type source edxapp_env . At this point you should be ready to run the user retirement command.","title":"Prepare Your Environment"},{"location":"how_to/retire_an_openedx_user/#get-to-the-retiring-already","text":"","title":"Get To The Retiring Already!"},{"location":"how_to/retire_an_openedx_user/#the-invocation","text":"Here's the command you'll use to retire a user. python edx-platform/manage.py lms retire_user --user_email <user email> --username '<username>' The single quotes around username are important in case there are any spaces in there. Otherwise the shell will mis-parse the command and throw an error. So for example if I wanted to retire myself from mitxonline production, I'd use: python edx-platform/manage.py lms retire_user --user_email cpatti@mit.edu --username 'ChrisPatti' You should see a bunch of very voluminous output. Most of it is honestly garbage for our purposes. We'll focus on the bits we care about at the end: 2023-06-21 16:07:20,714 INFO 186970 [openedx.core.djangoapps.user_api.management.commands.retire_user] [user None] [ip None] retire_user.py:173 - User succesfully moved to the retirment pipeline The None here isn't anything to worry about. It's the system trying to prevent us from leaking PII (personally identifiable information) into the logs. At this point if all went well, we're done with our edx worker shell prompt for now so we can log out. Always be super careful to not leave production shells open unnecessarily. You'd be surprised how many systems have been brought down by someone not realizing they're in the wrong terminal :)","title":"The Invocation"},{"location":"how_to/retire_an_openedx_user/#priming-the-pump-well-pipeline-in-this-case","text":"Now that we've successfully staged our user for retirement, we need to tell the retirement pipeline to actually retire the user. Surf to the appropriate concourse URL for the entironment you're working with: CI QA Production and search for 'tubular'. You'll want the tubular pipeline in the group associated with whichever product you're working with. In our case, it'd be the misc-cloud-tubular pipeline in the mitxonline group . - Click the green + icon with a circle around it in the upper right of your screen. This will trigger a run of this pipeline. - If all goes well, you should see each stage go green one by one. You can click on any stage to see more detail around what that stage is doing. You can see an example of a successful pipeline run here At this point, if the pipeline is green, congratulations are in order! This user had been retired!","title":"Priming The Pump (Well, Pipeline In This Case)"},{"location":"how_to/tagging_amis_with_installed_software_metadata/","text":"Tagging AMIs with Installed Software Metadata Background We are looking to start a new pattern where each AMI we produce is tagged with metadata about the software installed on it. This included 3rd party software such as Hashicorp products as well as information our applications. Recently we've been asked 'What version of X is running in production right now?\" and it was a surprisingly difficult question to answer. The idea behind this new pattern is to change that, by providing the required information simply by inspecting the AMI behind any running EC2 instance. Implementation We use pyinfra to do most of the operations involved with creating a new AMI and this is no exception. Firstly, during the new build we will create a file on the build instance at /etc/ami_tags.json which contains our tag keys and values. from bilder.lib.ami_helpers import build_tags_document tags_json = json.dumps( build_tags_document( source_tags={ \"consul_version\": VERSIONS[\"consul\"], \"consul_template_version\": VERSIONS[\"consul-template\"], \"vault_version\": VERSIONS[\"vault\"], \"docker_repo\": DOCKER_REPO_NAME, \"docker_digest\": DOCKER_IMAGE_DIGEST, \"edxapp_repo\": edx_platform.git_origin, \"edxapp_branch\": edx_platform.release_branch, \"edxapp_sha\": edx_platform_sha, \"theme_repo\": theme.git_origin, \"theme_branch\": theme.release_branch, \"theme_sha\": theme_sha, } ) ) files.put( name=\"Place the tags document at /etc/ami_tags.json\", src=io.StringIO(tags_json), dest=\"/etc/ami_tags.json\", mode=\"0644\", user=\"root\", ) This file persists as part of the AMI and will exist on any instances spawned from the image. Next, we need to add three steps to our packer build stanza. First, we need to retrieve the file we just created remotely in the pyinfra code. We use the same SSH information that we utilized when we ran pyinfra . This needs to be a provisioner step because the build instance still needs to be running in order to copy a file from it. provisioner \"shell-local\" { inline = [\"scp -o StrictHostKeyChecking=no -i /tmp/packer-${build.ID}.pem ${build.User}@${build.Host}:/etc/ami_tags.json /tmp/ami_tags-${build.ID}.json\"] } Second, we create a post-processor that generates a packer manifest for the build. This is just a json file local to the machine running the packer build (not the remote ec2 build instance as before). Because this is the first post-processor and follows the last provisioner step, the remote EC2 instance has been terminated and an AMI has been generated. The manifest will contain the AMI ID which is needed for the next step. post-processor \"manifest\" { output = \"/tmp/packer-build-manifest-${build.ID}.json\" } Finally, we will take the AMI ID out of the packer manifest and combined with the ami_tags.json file we will make a create-tags call on the newly created AMI to add our metadata to it. post-processor \"shell-local\" { inline = [\"AMI_ID=$(jq -r '.builds[-1].artifact_id' /tmp/packer-build-manifest-${build.ID}.json | cut -d \\\":\\\" -f2)\", \"export AWS_DEFAULT_REGION=us-east-1\", \"aws ec2 create-tags --resource $AMI_ID --cli-input-json \\\"$(cat /tmp/ami_tags-${build.ID}.json)\\\"\", \"aws --no-cli-pager ec2 describe-images --image-ids $AMI_ID\"] }","title":"Tagging AMIs with Installed Software Metadata"},{"location":"how_to/tagging_amis_with_installed_software_metadata/#tagging-amis-with-installed-software-metadata","text":"","title":"Tagging AMIs with Installed Software Metadata"},{"location":"how_to/tagging_amis_with_installed_software_metadata/#background","text":"We are looking to start a new pattern where each AMI we produce is tagged with metadata about the software installed on it. This included 3rd party software such as Hashicorp products as well as information our applications. Recently we've been asked 'What version of X is running in production right now?\" and it was a surprisingly difficult question to answer. The idea behind this new pattern is to change that, by providing the required information simply by inspecting the AMI behind any running EC2 instance.","title":"Background"},{"location":"how_to/tagging_amis_with_installed_software_metadata/#implementation","text":"We use pyinfra to do most of the operations involved with creating a new AMI and this is no exception. Firstly, during the new build we will create a file on the build instance at /etc/ami_tags.json which contains our tag keys and values. from bilder.lib.ami_helpers import build_tags_document tags_json = json.dumps( build_tags_document( source_tags={ \"consul_version\": VERSIONS[\"consul\"], \"consul_template_version\": VERSIONS[\"consul-template\"], \"vault_version\": VERSIONS[\"vault\"], \"docker_repo\": DOCKER_REPO_NAME, \"docker_digest\": DOCKER_IMAGE_DIGEST, \"edxapp_repo\": edx_platform.git_origin, \"edxapp_branch\": edx_platform.release_branch, \"edxapp_sha\": edx_platform_sha, \"theme_repo\": theme.git_origin, \"theme_branch\": theme.release_branch, \"theme_sha\": theme_sha, } ) ) files.put( name=\"Place the tags document at /etc/ami_tags.json\", src=io.StringIO(tags_json), dest=\"/etc/ami_tags.json\", mode=\"0644\", user=\"root\", ) This file persists as part of the AMI and will exist on any instances spawned from the image. Next, we need to add three steps to our packer build stanza. First, we need to retrieve the file we just created remotely in the pyinfra code. We use the same SSH information that we utilized when we ran pyinfra . This needs to be a provisioner step because the build instance still needs to be running in order to copy a file from it. provisioner \"shell-local\" { inline = [\"scp -o StrictHostKeyChecking=no -i /tmp/packer-${build.ID}.pem ${build.User}@${build.Host}:/etc/ami_tags.json /tmp/ami_tags-${build.ID}.json\"] } Second, we create a post-processor that generates a packer manifest for the build. This is just a json file local to the machine running the packer build (not the remote ec2 build instance as before). Because this is the first post-processor and follows the last provisioner step, the remote EC2 instance has been terminated and an AMI has been generated. The manifest will contain the AMI ID which is needed for the next step. post-processor \"manifest\" { output = \"/tmp/packer-build-manifest-${build.ID}.json\" } Finally, we will take the AMI ID out of the packer manifest and combined with the ami_tags.json file we will make a create-tags call on the newly created AMI to add our metadata to it. post-processor \"shell-local\" { inline = [\"AMI_ID=$(jq -r '.builds[-1].artifact_id' /tmp/packer-build-manifest-${build.ID}.json | cut -d \\\":\\\" -f2)\", \"export AWS_DEFAULT_REGION=us-east-1\", \"aws ec2 create-tags --resource $AMI_ID --cli-input-json \\\"$(cat /tmp/ami_tags-${build.ID}.json)\\\"\", \"aws --no-cli-pager ec2 describe-images --image-ids $AMI_ID\"] }","title":"Implementation"},{"location":"how_to/test_and_deploy_pulumi_packer_projects/","text":"How To Test And Deploy MIT OL Pulumi/Packer Projects The way we build images here at MIT OL is complicated and involves a number of components with various moving parts, so it can be difficult to understand where to start, what to change, and moreover how to safely test your changes without breaking production. This document will address these issues. The Pipeline Each MIT OL project that uses this technique has an accompanying Concourse pipeline that builds the Packer image, then deploys that image to the appropriate stage's AWS EC2 launch profile where new instances will be launched by instance refresh to deploy the new code / configuration into the wild. One such project is Tika . It's a data transformation service used in our data platform. Here is its pipeline . Packer Stage The first couple stages are reasonable self explanatory: Validate the packer template for any syntax errors and the like Actually run packer build on the template, invoking pyinfra and packer, producing an AMI. You can find the pyinfra sources in ol-infrastructure src/bilder/images/ . Here is the pyinfra folder for Tika . Testing If you want to test the pyinfra portion locally, you can use the following invocation: # cpatti @ rocinante in ~/src/mit/ol-infrastructure/src/bilder/images/tika on git:main o [17:11:31] C:1 $ pr pyinfra @docker/debian:latest deploy.py You should see output similar to the following: TODO: Is there a base image we can use that won't whine about curl/wget and say no hosts remaining? --> Loading config... --> Loading inventory... --> Connecting to hosts... [@docker/debian:latest] Connected --> Preparing Operations... Loading: deploy.py [@docker/debian:latest] Ready: deploy.py --> Proposed changes: Groups: @docker [@docker/debian:latest] Operations: 32 Change: 32 No change: 0 --> Beginning operation run... --> Starting operation: Install Hashicorp Products | Ensure unzip is installed [@docker/debian:latest] Success --> Starting operation: Install Hashicorp Products | Create system user for vault [@docker/debian:latest] Success --> Starting operation: Install Hashicorp Products | Download vault archive [@docker/debian:latest] sh: 1: curl: not found [@docker/debian:latest] sh: 1: wget: not found [@docker/debian:latest] Error: executed 0/3 commands [@docker/debian:latest] docker build complete, image ID: cc0b00b971bd --> pyinfra error: No hosts remaining! When you're satisfied that your pyinfra build will at least build correctly, you can trigger a local packer image build that the CI deployment stage can consume to get your changes into CI. Note that this can take a while, so be prepared for it to chug for 15-20 minutes. Great time to go get a beverage :) You can kick this off with an invocation like the following: poetry run packer build src/bilder/images/tika/tika.pkr.hcl Note that obviously your path will change if the project you're building is different. Note also that you may require a slightly different invocation for different projects. In Tika's case we require a custom Packer template as specified above, but in the case of other projects which use the default Packer template, you might use an invocation like the one we use to build Concourse's image: poetry run packer build -on-error=ask -var node_type=web -var app_name=concourse -only amazon-ebs.third-party src/bilder/images Note the node_type and app_name variable declarations above. For Concourse, we can build either a web image or a worker image, so it's important we include node_type in our invocation. Deploy Stage The remaining boxes in the pipeline are deployment stages. One per environment stage e.g. CI, QA and Production. Each of these deployment stages basically runs the equivalent of a pulumi up on the Pulumi stack associated with the application in question. Here's the one for Tika . Let's take a look at the output from a build of this stage: INFO:root:@ updating.... 8< snip for brevity 8< INFO:root: aws:ec2:LaunchTemplate tika-server-tika-ci-launch-template [diff: ~blockDeviceMappings] INFO:root: aws:autoscaling:Group tika-server-tika-ci-auto-scale-group [diff: ~instanceRefresh,tags,vpcZoneIdentifiers] INFO:root:@ updating.... INFO:root: pulumi:pulumi:Stack ol-infrastructure-tika-server-applications.tika.CI INFO:root: ol:infrastructure:aws:auto_scale_group:OLAutoScaleGroup tika-server-tika-ci INFO:root: INFO:root:Resources: INFO:root: 17 unchanged From this rather verbose blurb we can see that Pulumi updated the ASG and all associated resources like the Launch template. This is the mechanism which seeds the updated image into our EC2 environment where instance refresh safely cycles the old instances out and the ones with our updated code in.","title":"How To Test And Deploy MIT OL Pulumi/Packer Projects"},{"location":"how_to/test_and_deploy_pulumi_packer_projects/#how-to-test-and-deploy-mit-ol-pulumipacker-projects","text":"The way we build images here at MIT OL is complicated and involves a number of components with various moving parts, so it can be difficult to understand where to start, what to change, and moreover how to safely test your changes without breaking production. This document will address these issues.","title":"How To Test And Deploy MIT OL Pulumi/Packer Projects"},{"location":"how_to/test_and_deploy_pulumi_packer_projects/#the-pipeline","text":"Each MIT OL project that uses this technique has an accompanying Concourse pipeline that builds the Packer image, then deploys that image to the appropriate stage's AWS EC2 launch profile where new instances will be launched by instance refresh to deploy the new code / configuration into the wild. One such project is Tika . It's a data transformation service used in our data platform. Here is its pipeline .","title":"The Pipeline"},{"location":"how_to/test_and_deploy_pulumi_packer_projects/#packer-stage","text":"The first couple stages are reasonable self explanatory: Validate the packer template for any syntax errors and the like Actually run packer build on the template, invoking pyinfra and packer, producing an AMI. You can find the pyinfra sources in ol-infrastructure src/bilder/images/ . Here is the pyinfra folder for Tika .","title":"Packer Stage"},{"location":"how_to/test_and_deploy_pulumi_packer_projects/#testing","text":"If you want to test the pyinfra portion locally, you can use the following invocation: # cpatti @ rocinante in ~/src/mit/ol-infrastructure/src/bilder/images/tika on git:main o [17:11:31] C:1 $ pr pyinfra @docker/debian:latest deploy.py You should see output similar to the following: TODO: Is there a base image we can use that won't whine about curl/wget and say no hosts remaining? --> Loading config... --> Loading inventory... --> Connecting to hosts... [@docker/debian:latest] Connected --> Preparing Operations... Loading: deploy.py [@docker/debian:latest] Ready: deploy.py --> Proposed changes: Groups: @docker [@docker/debian:latest] Operations: 32 Change: 32 No change: 0 --> Beginning operation run... --> Starting operation: Install Hashicorp Products | Ensure unzip is installed [@docker/debian:latest] Success --> Starting operation: Install Hashicorp Products | Create system user for vault [@docker/debian:latest] Success --> Starting operation: Install Hashicorp Products | Download vault archive [@docker/debian:latest] sh: 1: curl: not found [@docker/debian:latest] sh: 1: wget: not found [@docker/debian:latest] Error: executed 0/3 commands [@docker/debian:latest] docker build complete, image ID: cc0b00b971bd --> pyinfra error: No hosts remaining! When you're satisfied that your pyinfra build will at least build correctly, you can trigger a local packer image build that the CI deployment stage can consume to get your changes into CI. Note that this can take a while, so be prepared for it to chug for 15-20 minutes. Great time to go get a beverage :) You can kick this off with an invocation like the following: poetry run packer build src/bilder/images/tika/tika.pkr.hcl Note that obviously your path will change if the project you're building is different. Note also that you may require a slightly different invocation for different projects. In Tika's case we require a custom Packer template as specified above, but in the case of other projects which use the default Packer template, you might use an invocation like the one we use to build Concourse's image: poetry run packer build -on-error=ask -var node_type=web -var app_name=concourse -only amazon-ebs.third-party src/bilder/images Note the node_type and app_name variable declarations above. For Concourse, we can build either a web image or a worker image, so it's important we include node_type in our invocation.","title":"Testing"},{"location":"how_to/test_and_deploy_pulumi_packer_projects/#deploy-stage","text":"The remaining boxes in the pipeline are deployment stages. One per environment stage e.g. CI, QA and Production. Each of these deployment stages basically runs the equivalent of a pulumi up on the Pulumi stack associated with the application in question. Here's the one for Tika . Let's take a look at the output from a build of this stage: INFO:root:@ updating.... 8< snip for brevity 8< INFO:root: aws:ec2:LaunchTemplate tika-server-tika-ci-launch-template [diff: ~blockDeviceMappings] INFO:root: aws:autoscaling:Group tika-server-tika-ci-auto-scale-group [diff: ~instanceRefresh,tags,vpcZoneIdentifiers] INFO:root:@ updating.... INFO:root: pulumi:pulumi:Stack ol-infrastructure-tika-server-applications.tika.CI INFO:root: ol:infrastructure:aws:auto_scale_group:OLAutoScaleGroup tika-server-tika-ci INFO:root: INFO:root:Resources: INFO:root: 17 unchanged From this rather verbose blurb we can see that Pulumi updated the ASG and all associated resources like the Launch template. This is the mechanism which seeds the updated image into our EC2 environment where instance refresh safely cycles the old instances out and the ones with our updated code in.","title":"Deploy Stage"},{"location":"how_to/transition_heroku_postgres_db_to_rds/","text":"Moving a Heroku Managed Postgres DB to Pulumi Managed AWS RDS Preparation You will need to gather some information about the Heroku managed application and its database before you start. You'll need the Heroku CLI with auehenticated access to the application in question to continue. You'll also need the postgresql tools, specifically the pg_dump and psql tools. Record the following information somewhere persistent that you'll be able to refer back to through this process. I like using my notes . The Heroku application's name. Our convention is generally - so for the CI environment of the micromasters application, you'd use micromasters-ci . The applications's DATABASE_URL . You can obtain this with the following invocation: heroku config:get DATABASE_URL -a <your app> The currently attached Heroku database name as well as the alias they have assigned to it by default. You can get this with: heroku addons -a <application> | grep -i postgres for example: \u2570 \u27a4 heroku addons -a micromasters-ci | grep -i postgres heroku-postgresql (postgresql-rigid-71273) mini $5/month created \u2514\u2500 as HEROKU_POSTGRES_YELLOW so in this case we see that postgresql-rigid-71273 is attached as HEROKU_POSTGRES_YELLOW - A set of URLs to browse when the transition is done to ensure everything is working properly. You should also browse them before the transition to note how everything looks. Building the Infrastructure with Pulumi Describing in detail how to code the necessary resources to build the AWS RDS Postgres instance and associated S3 bucket, IAM rules, VPC peerings etc. is beyond the scope of this ducument. You can see what I did in my Github branch . Once the infrastructure is properly built, you'll need to record the new AWS RDS Postgres instance's endpoint. You can do this from within the directory for the application you're working on. For my current project, that's ol-infrastructure/src/ol_infrastructure/applications/micromasters with this: 'poetry run pulumi stack export -s applications.micromasters.CI | grep -i endpoint' but obviously sub your app in for micromasters. You'll also need to retrieve the database password from Vault using Pulumi. You can do that with the following invocation: poetry run pulumi config get \"micromasters:db_password\" Construct A New DATABASE_URL I suggest doing this in a text file you can source easily since you'll be working with this database a bit for this project. I keep such things in an 'envsnips' folder in my home directory. The file should look something like: export DATABASE_URL=postgresql://oldevops:<password you pulled from Pulumi config>@micromasters-ci-app-db.cbnm7ajau6mi.us-east-1.rds.amazonaws.com:5432/micromasters Make sure the URL has the following components: postgresql:// is the protocol identifier followed by a :. 'oldevops' is the database user, then another :. the database password we pulled from Pulumi above, followed by an @ sign. The endpoint hostname we retrieved from Pulumi earlier, followed by a :. The port number. We usually use 5432. Then a /. The database name. If your URL is missing any of these it will not work. Once you've finished write out your file and source it in your shell. Now, test that you can connect using the URL you just built with: psql $DATABASE_URL If you get an access denied message, make sure you got the correct password for the app and environment (e.g. CI, QA or production) and check the other components. We'll assume $DATABASE_URL is set to to the new RDS database we've created for the rest of the runbook. Put the Heroku App Into Maintenance Mode In order to ensure database consistency during the transition we need to put the Heroku application into maintenance mode. heroku maintenance:on -a <app> CAUTION customers will see a notice about ongoing maintenance until you take the app out of maintenance mode, so be mindful of the wall clock time! Dump Heroku Managed DB Use something like the following invocation to dump the contents of the current application database. pg_dump -x -O $(heroku config:get DATABASE_URL -a micromasters-rc) > micromasters_qa_db_dump.sql Obviously, substitute your app for micromasters and your environment for rc/qa. (Aside: We use rc and qa interchangably here). Examine the dump in your editor (read-only to be safe) and ensure that all the necessary components are present: Schema, data, foreign keys, and the like. Restore Dump Into AWS RDS DB Using the DATABASE_URL we just created and tested, we can now restore the data we dumped in the prior step into the new DB: psql $DATABASE_URL < micromasters_qa_db_dump.sql You will see a lot of output representing each statement as it's processed by the DB. You shouldn't see any errors here. Take Heroku application out of maintenance mode heroku maintenance:off -a <app> At this point, the application will resume normal operation and be available to customers. Coordinate Transition In the process of changing the database out from under a running application, there will be some small period of down time, so it's important to coordinate with all the appropriate stakeholders and leadership before you do. Perform The Final Transition At the time, it's important that you perform the following steps quickly in succession, because once you detach the current DB, the application will be down. Keep this as brief as possible. You may wish to cue up the commands you want to run in a text file somewhere you can eaily review them, and then cut and paste them into your shell when the time comes. Create An Additional DB Attachment You'll need to create an additional attachment for the current DB: heroku addons:attach postgresql-amorphous-36035 --as HEROKU_POSTGRES_DB Substitute your db instance you gathered above. HEROKU_POSTGRES_DB is just an alias we can use if we should need to roll back. Detach The Current Database This is where you'll need to use the Heroku managed database instance above, along with the Heroku application name we collected. Substitute accordingly into the following invocation: heroku addons:detach postgresql-amorphous-36035 -a micromasters-rc Change the DATABASE_URL to the New RDS Instance Ensuring that your DATABASE_URL environment variable is properly set to your new RDS from the above steps, use it to set DATABASE_URL in the heroku app: heroku config:set -a micromasters-rc DATABASE_URL=$DATABASE_URL Now immediately print out the value you just set to ensure that all looks good: heroku config:get -a micromasters-rc DATABASE_URL Test Your Work You should carefully test the application you just transitioned to ensure everything works using the set of URLs you gathered at the beginning. - Do the pages have all the elements they should? - Are images loading? How To Roll Back If something goes wrong and you need to roll back, don't panic! All you need to do is promote the old Heroku managed DB back into use: heroku pg:promote --app micromasters-ci postgresql-rigid-71273 Obviously substitute your db and application for the ones above. Re-run your tests as defined above to make sure everything's working right post-rollback. S3 Buckets Our applications use S3 buckets for CMS asset storage and backup among other things. You will need to either continue using the existing buckets by using pulumi import or creating new onnes. You should create new ones if the old ones don't conform to naming conventions. You'll also need to ensure that IAM permissions are properly set in your Pulumi code. To sync the bucket contents, use the AWS CLI aws s3 sync command. For example, the command we used to sync the old micromasters S3 bucket to the new one which conforms to our desired naming conventions is: aws s3 sync s3://odl-micromasters-production s3://ol-micromasters-app-production Saltstack & Cloudfront Currently, we are using Saltstack to configure some aspects of our Heroku applications, including which S3 bucket to use and which CloudFront distribution we're fronting the app with. TODO : This section needs to be way less of a hand wave and include actual detail as to how to operate the Saltstack side or whatever we replace it with.","title":"Moving a Heroku Managed Postgres DB to Pulumi Managed AWS RDS"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#moving-a-heroku-managed-postgres-db-to-pulumi-managed-aws-rds","text":"","title":"Moving a Heroku Managed Postgres DB to Pulumi Managed AWS RDS"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#preparation","text":"You will need to gather some information about the Heroku managed application and its database before you start. You'll need the Heroku CLI with auehenticated access to the application in question to continue. You'll also need the postgresql tools, specifically the pg_dump and psql tools. Record the following information somewhere persistent that you'll be able to refer back to through this process. I like using my notes . The Heroku application's name. Our convention is generally - so for the CI environment of the micromasters application, you'd use micromasters-ci . The applications's DATABASE_URL . You can obtain this with the following invocation: heroku config:get DATABASE_URL -a <your app> The currently attached Heroku database name as well as the alias they have assigned to it by default. You can get this with: heroku addons -a <application> | grep -i postgres for example: \u2570 \u27a4 heroku addons -a micromasters-ci | grep -i postgres heroku-postgresql (postgresql-rigid-71273) mini $5/month created \u2514\u2500 as HEROKU_POSTGRES_YELLOW so in this case we see that postgresql-rigid-71273 is attached as HEROKU_POSTGRES_YELLOW - A set of URLs to browse when the transition is done to ensure everything is working properly. You should also browse them before the transition to note how everything looks.","title":"Preparation"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#building-the-infrastructure-with-pulumi","text":"Describing in detail how to code the necessary resources to build the AWS RDS Postgres instance and associated S3 bucket, IAM rules, VPC peerings etc. is beyond the scope of this ducument. You can see what I did in my Github branch . Once the infrastructure is properly built, you'll need to record the new AWS RDS Postgres instance's endpoint. You can do this from within the directory for the application you're working on. For my current project, that's ol-infrastructure/src/ol_infrastructure/applications/micromasters with this: 'poetry run pulumi stack export -s applications.micromasters.CI | grep -i endpoint' but obviously sub your app in for micromasters. You'll also need to retrieve the database password from Vault using Pulumi. You can do that with the following invocation: poetry run pulumi config get \"micromasters:db_password\"","title":"Building the Infrastructure with Pulumi"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#construct-a-new-database_url","text":"I suggest doing this in a text file you can source easily since you'll be working with this database a bit for this project. I keep such things in an 'envsnips' folder in my home directory. The file should look something like: export DATABASE_URL=postgresql://oldevops:<password you pulled from Pulumi config>@micromasters-ci-app-db.cbnm7ajau6mi.us-east-1.rds.amazonaws.com:5432/micromasters Make sure the URL has the following components: postgresql:// is the protocol identifier followed by a :. 'oldevops' is the database user, then another :. the database password we pulled from Pulumi above, followed by an @ sign. The endpoint hostname we retrieved from Pulumi earlier, followed by a :. The port number. We usually use 5432. Then a /. The database name. If your URL is missing any of these it will not work. Once you've finished write out your file and source it in your shell. Now, test that you can connect using the URL you just built with: psql $DATABASE_URL If you get an access denied message, make sure you got the correct password for the app and environment (e.g. CI, QA or production) and check the other components. We'll assume $DATABASE_URL is set to to the new RDS database we've created for the rest of the runbook.","title":"Construct A New DATABASE_URL"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#put-the-heroku-app-into-maintenance-mode","text":"In order to ensure database consistency during the transition we need to put the Heroku application into maintenance mode. heroku maintenance:on -a <app> CAUTION customers will see a notice about ongoing maintenance until you take the app out of maintenance mode, so be mindful of the wall clock time!","title":"Put the Heroku App Into Maintenance Mode"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#dump-heroku-managed-db","text":"Use something like the following invocation to dump the contents of the current application database. pg_dump -x -O $(heroku config:get DATABASE_URL -a micromasters-rc) > micromasters_qa_db_dump.sql Obviously, substitute your app for micromasters and your environment for rc/qa. (Aside: We use rc and qa interchangably here). Examine the dump in your editor (read-only to be safe) and ensure that all the necessary components are present: Schema, data, foreign keys, and the like.","title":"Dump Heroku Managed DB"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#restore-dump-into-aws-rds-db","text":"Using the DATABASE_URL we just created and tested, we can now restore the data we dumped in the prior step into the new DB: psql $DATABASE_URL < micromasters_qa_db_dump.sql You will see a lot of output representing each statement as it's processed by the DB. You shouldn't see any errors here.","title":"Restore Dump Into AWS RDS DB"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#take-heroku-application-out-of-maintenance-mode","text":"heroku maintenance:off -a <app> At this point, the application will resume normal operation and be available to customers.","title":"Take Heroku application out of maintenance mode"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#coordinate-transition","text":"In the process of changing the database out from under a running application, there will be some small period of down time, so it's important to coordinate with all the appropriate stakeholders and leadership before you do.","title":"Coordinate Transition"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#perform-the-final-transition","text":"At the time, it's important that you perform the following steps quickly in succession, because once you detach the current DB, the application will be down. Keep this as brief as possible. You may wish to cue up the commands you want to run in a text file somewhere you can eaily review them, and then cut and paste them into your shell when the time comes.","title":"Perform The Final Transition"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#create-an-additional-db-attachment","text":"You'll need to create an additional attachment for the current DB: heroku addons:attach postgresql-amorphous-36035 --as HEROKU_POSTGRES_DB Substitute your db instance you gathered above. HEROKU_POSTGRES_DB is just an alias we can use if we should need to roll back.","title":"Create An Additional DB Attachment"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#detach-the-current-database","text":"This is where you'll need to use the Heroku managed database instance above, along with the Heroku application name we collected. Substitute accordingly into the following invocation: heroku addons:detach postgresql-amorphous-36035 -a micromasters-rc","title":"Detach The Current Database"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#change-the-database_url-to-the-new-rds-instance","text":"Ensuring that your DATABASE_URL environment variable is properly set to your new RDS from the above steps, use it to set DATABASE_URL in the heroku app: heroku config:set -a micromasters-rc DATABASE_URL=$DATABASE_URL Now immediately print out the value you just set to ensure that all looks good: heroku config:get -a micromasters-rc DATABASE_URL","title":"Change the DATABASE_URL to the New RDS Instance"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#test-your-work","text":"You should carefully test the application you just transitioned to ensure everything works using the set of URLs you gathered at the beginning. - Do the pages have all the elements they should? - Are images loading?","title":"Test Your Work"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#how-to-roll-back","text":"If something goes wrong and you need to roll back, don't panic! All you need to do is promote the old Heroku managed DB back into use: heroku pg:promote --app micromasters-ci postgresql-rigid-71273 Obviously substitute your db and application for the ones above. Re-run your tests as defined above to make sure everything's working right post-rollback.","title":"How To Roll Back"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#s3-buckets","text":"Our applications use S3 buckets for CMS asset storage and backup among other things. You will need to either continue using the existing buckets by using pulumi import or creating new onnes. You should create new ones if the old ones don't conform to naming conventions. You'll also need to ensure that IAM permissions are properly set in your Pulumi code. To sync the bucket contents, use the AWS CLI aws s3 sync command. For example, the command we used to sync the old micromasters S3 bucket to the new one which conforms to our desired naming conventions is: aws s3 sync s3://odl-micromasters-production s3://ol-micromasters-app-production","title":"S3 Buckets"},{"location":"how_to/transition_heroku_postgres_db_to_rds/#saltstack-cloudfront","text":"Currently, we are using Saltstack to configure some aspects of our Heroku applications, including which S3 bucket to use and which CloudFront distribution we're fronting the app with. TODO : This section needs to be way less of a hand wave and include actual detail as to how to operate the Saltstack side or whatever we replace it with.","title":"Saltstack &amp; Cloudfront"},{"location":"how_to/update_this_site/","text":"Install mkdocs if not already. Make your changes to the Markdown files or add new ones. Run mkdocs build to build the site. Run mkdocs serve to preview the site locally. When you're happy, run mkdocs gh-deploy to push the changes to the GitHub Pages site.","title":"Update this site"},{"location":"monitoring/inventory/","text":"Tools Sentry Purpose: Detailed application monitoring and error logging. Healthchecks.io Purpose: Absence of alerting detection. For determining when other alerting tools may have failed or become unresponsive. GrafanaCloud Purpose: Collecting and storing metric and log data from applications and infrastructure. Subcomponents: Grafana: Visualization and alerting on metric and log data. Cortex: Backend for storing metric data. Loki: Backend for storing log data Pingdom Purpose: External synthetic and HTTP monitoring. Usage Matrix ODL App / Component / Process Sentry HealthChecks.io Grafana Metrics Grafana Logs OCW Studio (webapp) Yes Yes Yes Yes OCW Site (static content) N/A N/A No No OCW Site Backup (process) N/A Yes N/A No MITx (webapp) Yes No No Yes MITx (openEdx) Yes No No Yes MITx (infrastructure) N/A N/A Yes Yes xPro (webapp) Yes No No Yes xPro (openEdx) Yes No No Yes xPro (infrastructure) N/A N/A Yes Yes Residential (openEdx) Yes No No Yes Residential (infrastructure) N/A N/A Yes Yes MIT Open (webapp) Yes No No Yes MIT Open Discussions (webapp) Yes No No Yes MIT Open Reddit (webapp) Yes No No Yes odl-video (webapp) Yes No No Yes Bootcamps (webapp) Yes No No Yes MicroMasters (webapp) Yes No No Yes Synthetic Monitoring ODL App / Component / Process URL Pingdom Grafana Bootcamp production bootcamp.odl.mit.edu yes no MITx CAS cas.mitx.mit.edu yes no MITx Online Production Application nitxonline.mit.edu yes no MITx Online Production edX courses.mitxonline.mit.edu yes no MITx Online QA edX Application courses-qa.mitxonline.mit.edu yes no MITx Online RC Application rc.mitxonline.mit.edu yes no MITx QA CMS studio-mitx-qa.mitx.mit.edu yes no MITx QA LMS mitx-qa.mitx.mit.edu yes no MITx current QA preview preview-mitx-qa.mitx.mit.edu yes no MITx production CMS studio.mitx.mit.edu yes no MITx production CMS draft studio-staging.mitx.mit.edu yes no MITx production LMS lms.mitx.mit.edu yes no MITx production LMS draft staging.mitx.mit.edu yes no MITx production preview preview.mitx.mit.edu yes no MITx production preview draft preview.mitx.mit.edu yes no Micromasters CI micromasters-ci.odl.mit.edu yes no Micromasters RC micromasters-rc.odl.mit.edu yes no Micromasters production micromasters.mit.edu yes no OCW Production (Fastly) ocw.mit.edu yes no OCW production CMS 1 ocwcms.mit.edu yes no OCW production CMS 2 ocw-production-cms-2.odl.mit.edu yes no OCW production origin server ocw-origin.odl.mit.edu yes no ODL Video RC video-rc.odl.mit.edu yes no ODL Video production video.odl.mit.edu yes no Open Discussions production open.mit.edu yes no xPro CMS RC studio-rc.xpro.mit.edu/heartbeat yes no xPro CMS production studio.xpro.mit.edu/heartbeat yes no xPro LMS RC courses-rc.xpro.mit.edu/heartbeat yes no xPro LMS production courses.xpro.mit.edu/heartbeat yes no xPro RC xpro-rc.odl.mit.edu yes no xPro preview RC preview-rc.xpro.mit.edu/heartbeat yes no xPro preview production preview.xpro.mit.edu/heartbeat yes no xPro production xpro.mit.edu yes no","title":"Tools"},{"location":"monitoring/inventory/#tools","text":"Sentry Purpose: Detailed application monitoring and error logging. Healthchecks.io Purpose: Absence of alerting detection. For determining when other alerting tools may have failed or become unresponsive. GrafanaCloud Purpose: Collecting and storing metric and log data from applications and infrastructure. Subcomponents: Grafana: Visualization and alerting on metric and log data. Cortex: Backend for storing metric data. Loki: Backend for storing log data Pingdom Purpose: External synthetic and HTTP monitoring.","title":"Tools"},{"location":"monitoring/inventory/#usage-matrix","text":"ODL App / Component / Process Sentry HealthChecks.io Grafana Metrics Grafana Logs OCW Studio (webapp) Yes Yes Yes Yes OCW Site (static content) N/A N/A No No OCW Site Backup (process) N/A Yes N/A No MITx (webapp) Yes No No Yes MITx (openEdx) Yes No No Yes MITx (infrastructure) N/A N/A Yes Yes xPro (webapp) Yes No No Yes xPro (openEdx) Yes No No Yes xPro (infrastructure) N/A N/A Yes Yes Residential (openEdx) Yes No No Yes Residential (infrastructure) N/A N/A Yes Yes MIT Open (webapp) Yes No No Yes MIT Open Discussions (webapp) Yes No No Yes MIT Open Reddit (webapp) Yes No No Yes odl-video (webapp) Yes No No Yes Bootcamps (webapp) Yes No No Yes MicroMasters (webapp) Yes No No Yes","title":"Usage Matrix"},{"location":"monitoring/inventory/#synthetic-monitoring","text":"ODL App / Component / Process URL Pingdom Grafana Bootcamp production bootcamp.odl.mit.edu yes no MITx CAS cas.mitx.mit.edu yes no MITx Online Production Application nitxonline.mit.edu yes no MITx Online Production edX courses.mitxonline.mit.edu yes no MITx Online QA edX Application courses-qa.mitxonline.mit.edu yes no MITx Online RC Application rc.mitxonline.mit.edu yes no MITx QA CMS studio-mitx-qa.mitx.mit.edu yes no MITx QA LMS mitx-qa.mitx.mit.edu yes no MITx current QA preview preview-mitx-qa.mitx.mit.edu yes no MITx production CMS studio.mitx.mit.edu yes no MITx production CMS draft studio-staging.mitx.mit.edu yes no MITx production LMS lms.mitx.mit.edu yes no MITx production LMS draft staging.mitx.mit.edu yes no MITx production preview preview.mitx.mit.edu yes no MITx production preview draft preview.mitx.mit.edu yes no Micromasters CI micromasters-ci.odl.mit.edu yes no Micromasters RC micromasters-rc.odl.mit.edu yes no Micromasters production micromasters.mit.edu yes no OCW Production (Fastly) ocw.mit.edu yes no OCW production CMS 1 ocwcms.mit.edu yes no OCW production CMS 2 ocw-production-cms-2.odl.mit.edu yes no OCW production origin server ocw-origin.odl.mit.edu yes no ODL Video RC video-rc.odl.mit.edu yes no ODL Video production video.odl.mit.edu yes no Open Discussions production open.mit.edu yes no xPro CMS RC studio-rc.xpro.mit.edu/heartbeat yes no xPro CMS production studio.xpro.mit.edu/heartbeat yes no xPro LMS RC courses-rc.xpro.mit.edu/heartbeat yes no xPro LMS production courses.xpro.mit.edu/heartbeat yes no xPro RC xpro-rc.odl.mit.edu yes no xPro preview RC preview-rc.xpro.mit.edu/heartbeat yes no xPro preview production preview.xpro.mit.edu/heartbeat yes no xPro production xpro.mit.edu yes no","title":"Synthetic Monitoring"},{"location":"monitoring/key_labels/","text":"Key Labels used by ODL environment : The 'tier'. For example, dev, QA, Production. service : The product provided. FOr example, 'MITx', 'Residential', 'OCW' application : The application within the service, for example, edxapp, django, consul There may be multiple instances (environments) of a service. A service is comprised of applications. Environments ci production qa Services bootcamps micromasters mitx-online mitx-residential ocw open xpro Applications concourse consul django edxapp elasticsearch heroku-app nginx redis vault","title":"Key Labels used by ODL"},{"location":"monitoring/key_labels/#key-labels-used-by-odl","text":"environment : The 'tier'. For example, dev, QA, Production. service : The product provided. FOr example, 'MITx', 'Residential', 'OCW' application : The application within the service, for example, edxapp, django, consul There may be multiple instances (environments) of a service. A service is comprised of applications.","title":"Key Labels used by ODL"},{"location":"monitoring/key_labels/#environments","text":"ci production qa","title":"Environments"},{"location":"monitoring/key_labels/#services","text":"bootcamps micromasters mitx-online mitx-residential ocw open xpro","title":"Services"},{"location":"monitoring/key_labels/#applications","text":"concourse consul django edxapp elasticsearch heroku-app nginx redis vault","title":"Applications"},{"location":"monitoring/overview/","text":"Monitoring Overview We primarily use Vector for collecting both metrics and logs from our applications. Vector is pretty flexible and offers sources and sinks to/from many different systems. Metrics Ultimately we utilize Prometheus , but our stack is a little non-traditional. For our applications in EC2, we use Vector as a middleman to perform the 'prometheus-scrape' action, rather than performing the scrape with an actual prometheus instance that we would have to run. Right at scrape time, on the node performing the scrape, we can perform transforms on the data to drop timeseries that we are not interested in or add labels + metadata that would be helpful. This is good, because we are charged by the timeseries in GrafanaCloud and many prometheus endpoints produce lots of not-interesting data. The more we can drop early, the better. Once it is in GrafanaCloud, we pay for it. After the transforms, Vector ships the data straight to GrafanaCloud where it is stored in Cortex, which is actually not prometheus but it is API compatible and the distinction isn't important to us. Logs For log collection and aggregation, we again are using Vector and GrafanaCloud. Vector is also very good at collecting and parsing logfiles and again we can utilize its ability to transform the data to drop not-interesting data and save a little money. After any transforms on the log data are completed, Vector ships the data straight to GrafanaCloud again and it is stored in Loki . Loki is like a hybrid of ElasticSearch and Prometheus and it has its own query language . Vector Patterns There are a couple different patterns at play in our environment. Applications deployed into EC2 The statements below do not necessarily apply to EC2 instances managed with salt-stack For applications deployed in EC2, we install and configure Vector at AMI build time. We pull in global secrets (values that are the same regardless of where the AMI is running) at build time. Other secrets are interpolated runtime secrets using environment variables populated via /etc/default/ files and vault-template. Examples: - Config files laid down at AMI build time here - /etc/default secrets from runtime here Applications deployed in Heroku Specifics of collecting logs out of Heroku are covered here Applications deployed into ECS Coming soon!","title":"Monitoring Overview"},{"location":"monitoring/overview/#monitoring-overview","text":"We primarily use Vector for collecting both metrics and logs from our applications. Vector is pretty flexible and offers sources and sinks to/from many different systems.","title":"Monitoring Overview"},{"location":"monitoring/overview/#metrics","text":"Ultimately we utilize Prometheus , but our stack is a little non-traditional. For our applications in EC2, we use Vector as a middleman to perform the 'prometheus-scrape' action, rather than performing the scrape with an actual prometheus instance that we would have to run. Right at scrape time, on the node performing the scrape, we can perform transforms on the data to drop timeseries that we are not interested in or add labels + metadata that would be helpful. This is good, because we are charged by the timeseries in GrafanaCloud and many prometheus endpoints produce lots of not-interesting data. The more we can drop early, the better. Once it is in GrafanaCloud, we pay for it. After the transforms, Vector ships the data straight to GrafanaCloud where it is stored in Cortex, which is actually not prometheus but it is API compatible and the distinction isn't important to us.","title":"Metrics"},{"location":"monitoring/overview/#logs","text":"For log collection and aggregation, we again are using Vector and GrafanaCloud. Vector is also very good at collecting and parsing logfiles and again we can utilize its ability to transform the data to drop not-interesting data and save a little money. After any transforms on the log data are completed, Vector ships the data straight to GrafanaCloud again and it is stored in Loki . Loki is like a hybrid of ElasticSearch and Prometheus and it has its own query language .","title":"Logs"},{"location":"monitoring/overview/#vector-patterns","text":"There are a couple different patterns at play in our environment.","title":"Vector Patterns"},{"location":"monitoring/overview/#applications-deployed-into-ec2","text":"The statements below do not necessarily apply to EC2 instances managed with salt-stack For applications deployed in EC2, we install and configure Vector at AMI build time. We pull in global secrets (values that are the same regardless of where the AMI is running) at build time. Other secrets are interpolated runtime secrets using environment variables populated via /etc/default/ files and vault-template. Examples: - Config files laid down at AMI build time here - /etc/default secrets from runtime here","title":"Applications deployed into EC2"},{"location":"monitoring/overview/#applications-deployed-in-heroku","text":"Specifics of collecting logs out of Heroku are covered here","title":"Applications deployed in Heroku"},{"location":"monitoring/overview/#applications-deployed-into-ecs","text":"Coming soon!","title":"Applications deployed into ECS"},{"location":"post-mortems/20231030_xpro_outage/","text":"xPro Outage - October 30, 2023 Timeline Undetermined date within the past 6 months: AWS RDS config updated to default new credentials to use SCRAM rather than md5 Monday, October 30th 2023 - 3:45PM - Configuration update deployed xPro Production Heroku stack via Salt. - 3:51PM - Pingdom alerts DevOps on-call . - 3:51PM - Alert Acknowledged by DevOps on-call (Mike Davidson). - 3:52PM - Verified that site was non-responsive. - 3:52PM - Begin detailed investigation. - ~3:53PM - Confirm log messages in Heroku regarding failed database logins: LOG C-0x55cb586d2540: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@127.0.0.1:55038 login attempt: db=db1 user=v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728 tls=no ERROR S-0x55cb586d5580: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@3.209.36.28:5432 cannot do SCRAM authentication: wrong password type LOG C-0x55cb586d2e00: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@127.0.0.1:52474 closing because: server login failed: wrong password type (age=14s) ~3:55PM Determined this matches the fingerprint of a previously encountered issue seen when setting up the new MITOpen environments. 3:56PM Found previous PR that addressed this for MITOpen: 3:56PM Started looking for code location for make the above modification. 3:58PM Tobias finds a possible alternative resolution . 4:00PM Unable to locate code for this environment. Determined it is not managed by pulumi. 4:03PM Started a zoom call. Attendees: Mike, Tobias, Sar 4:05PM Decided making the database parameter group configuration will possibly take longer than is desirable. Opt for trying the alternative resolution. ~4:07PM Role definition in vault is modified and verified to still generate credentials via the Vault UI. 4:09PM Cached credentials are cleared and Heroku configuration is re-applied via Salt. ~4:10PM Verified the issue is not resolved. 4:12PM Cleared cached credentials again and Heroku configuration is re-applied via Salt. 4:14PM Verified the issue is not resolved and that the credentials applied via Salt are truely different. 4:15PM Decided alternative solution did not work. Begin implementing database parameter group fix known to work. ~4:20PM A new database parameter group is created and applied to the running database. ~4:23PM The database is listed as Available in the RDS web console. 4:24PM Cleared cached credentials again and Heroku configuration is re-applied via Salt. 4:25PM Verified the site is now available again and login/other database interactions work. 4:25PM Pingdom automatically closes the OpsGenie alert. Root Cause: At some point in the last 6 months, the default behavior of the underlying RDS instance for xPro production was transitioned to using SCRAM password authentication rather than MD5 password authentication. Further information about this is documented here . The buildpack we use in Heroku for pgbouncer does not support SCRAM. When managing configuration items for applications deployed in Heroku, we invoke Vault via SaltStack to generate database credentials. These credentials are then cached on the Salt Master. Vault tracks the credentials it generates via internal objects known as 'leases'. When Salt checked + applied the configuration at 3:45PM, it saw that the lease was due to expire soon and was not eligible for renewal. This triggered Vault generating new credentials, however these new credentials were generated with the new default SCRAM configuration rather than the MD5 configuration supported by pgbouncer. When attempting to establish a pool of database connections, pgbouncer was rejected by the RDS instance for sending the wrong password type. It sent an MD5 password hash while the database expected SCRAM for the newly generated credential. Being unable to establish a connection to the database is a fatal error for the application and it was not able to finish starting up and serving requests. Action Items Migrate management of XPro resources into Pulumi One of the key factors delaying the resolution of the outage was the fact that none, or nearly none, of the configuration for this stack was managed as code. In particular the RDS instance and its associated resources. Additionally, the RDS instance was using an instance of the default parameter group provided by AWS, which cannot be modified. As such, we were required to duplicate this default parameter group definitions, make the required modifications, and then associate the newly created group with the instance. This is a more time consuming task than modifying a parameter configuration on an already associated parameter group. All of this would have been mitigated if the resources were managed with pulumi, where we do not utilize AWS sourced default parameter groups.","title":"xPro Outage - October 30, 2023"},{"location":"post-mortems/20231030_xpro_outage/#xpro-outage-october-30-2023","text":"","title":"xPro Outage - October 30, 2023"},{"location":"post-mortems/20231030_xpro_outage/#timeline","text":"Undetermined date within the past 6 months: AWS RDS config updated to default new credentials to use SCRAM rather than md5 Monday, October 30th 2023 - 3:45PM - Configuration update deployed xPro Production Heroku stack via Salt. - 3:51PM - Pingdom alerts DevOps on-call . - 3:51PM - Alert Acknowledged by DevOps on-call (Mike Davidson). - 3:52PM - Verified that site was non-responsive. - 3:52PM - Begin detailed investigation. - ~3:53PM - Confirm log messages in Heroku regarding failed database logins: LOG C-0x55cb586d2540: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@127.0.0.1:55038 login attempt: db=db1 user=v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728 tls=no ERROR S-0x55cb586d5580: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@3.209.36.28:5432 cannot do SCRAM authentication: wrong password type LOG C-0x55cb586d2e00: db1/v-aws-mitxpro-OC7crfPS2rL3jSryzvHE-1698696728@127.0.0.1:52474 closing because: server login failed: wrong password type (age=14s) ~3:55PM Determined this matches the fingerprint of a previously encountered issue seen when setting up the new MITOpen environments. 3:56PM Found previous PR that addressed this for MITOpen: 3:56PM Started looking for code location for make the above modification. 3:58PM Tobias finds a possible alternative resolution . 4:00PM Unable to locate code for this environment. Determined it is not managed by pulumi. 4:03PM Started a zoom call. Attendees: Mike, Tobias, Sar 4:05PM Decided making the database parameter group configuration will possibly take longer than is desirable. Opt for trying the alternative resolution. ~4:07PM Role definition in vault is modified and verified to still generate credentials via the Vault UI. 4:09PM Cached credentials are cleared and Heroku configuration is re-applied via Salt. ~4:10PM Verified the issue is not resolved. 4:12PM Cleared cached credentials again and Heroku configuration is re-applied via Salt. 4:14PM Verified the issue is not resolved and that the credentials applied via Salt are truely different. 4:15PM Decided alternative solution did not work. Begin implementing database parameter group fix known to work. ~4:20PM A new database parameter group is created and applied to the running database. ~4:23PM The database is listed as Available in the RDS web console. 4:24PM Cleared cached credentials again and Heroku configuration is re-applied via Salt. 4:25PM Verified the site is now available again and login/other database interactions work. 4:25PM Pingdom automatically closes the OpsGenie alert.","title":"Timeline"},{"location":"post-mortems/20231030_xpro_outage/#root-cause","text":"At some point in the last 6 months, the default behavior of the underlying RDS instance for xPro production was transitioned to using SCRAM password authentication rather than MD5 password authentication. Further information about this is documented here . The buildpack we use in Heroku for pgbouncer does not support SCRAM. When managing configuration items for applications deployed in Heroku, we invoke Vault via SaltStack to generate database credentials. These credentials are then cached on the Salt Master. Vault tracks the credentials it generates via internal objects known as 'leases'. When Salt checked + applied the configuration at 3:45PM, it saw that the lease was due to expire soon and was not eligible for renewal. This triggered Vault generating new credentials, however these new credentials were generated with the new default SCRAM configuration rather than the MD5 configuration supported by pgbouncer. When attempting to establish a pool of database connections, pgbouncer was rejected by the RDS instance for sending the wrong password type. It sent an MD5 password hash while the database expected SCRAM for the newly generated credential. Being unable to establish a connection to the database is a fatal error for the application and it was not able to finish starting up and serving requests.","title":"Root Cause:"},{"location":"post-mortems/20231030_xpro_outage/#action-items","text":"Migrate management of XPro resources into Pulumi One of the key factors delaying the resolution of the outage was the fact that none, or nearly none, of the configuration for this stack was managed as code. In particular the RDS instance and its associated resources. Additionally, the RDS instance was using an instance of the default parameter group provided by AWS, which cannot be modified. As such, we were required to duplicate this default parameter group definitions, make the required modifications, and then associate the newly created group with the instance. This is a more time consuming task than modifying a parameter configuration on an already associated parameter group. All of this would have been mitigated if the resources were managed with pulumi, where we do not utilize AWS sourced default parameter groups.","title":"Action Items"},{"location":"runbooks/oncall/","text":"Style Guide ODL Open Discussions SaltStack XQueueWatcher OVS Bootcamp Ecommerce OpenEdX Residential MITx XPro MITXOnline Reddit Introduction This document is meant to be one stop shopping for your MIT OL Devops oncall needs. Please update this doc as you handle incidents whenever you're oncall. Style Guide There should be a table of contents at the top of the document with links to each product heading. Your editor likely has a plugin to make this automatic. Each product gets its own top level heading. Entries that are keyed to a specific alert should have the relevant text in a second level heading under the product. Boil the alert down to the most relevant searchable text and omit specifics that will vary. For instance: \"[Prometheus]: [FIRING:1] DiskUsageWarning mitx-production (xqwatcher filesystem /dev/root ext4 ip-10-7-0-78 integrations/linux_hos\" would boil down to DiskUsageWarning xqwatcher because the rest will change and make finding the right entry more difficult. Each entry should have at least two sections, Diagnosis and Mitigation. Use bold face for the section title. This will allow the oncall to get only as much Diagnosis in as required to identify the issue and focus on putting out the fire. Products ODL Open Discussions InvalidAccessKeyNonProd qa (odl-open-discussions warning) Diagnosis You get an alert like \"[Prometheus]: [FIRING:1] InvalidAccessKeyNonProd qa (odl-open-discussions warning)\". Mitigation In the mitodl/ol-infrastructure Github repository, change directory to src/mit/ol-infrastructure/src/ol_infrastructure/applications/open_discussions and run `pulumi up'. SaltStack MemoryUsageWarning operations- Diagnosis You get an alert like: [Prometheus]: [FIRING:1] MemoryUsageWarning operations-qa (memory ip-10-1-3-33 integrations/linux_host warning) . You'll need an account and ssh key set up on the saltstack master hosts. This should happen when you join the team. Now, ssh into the salt master appropriate to the environment you received the alert for. The IP address is cited in the alert. So, for the above: (Substitute your username and the appropriate environment if not qa, e.g. production) ssh -l cpatti salt-qa.odl.mit.edu Next, check free memory: mdavidson@ip-10-1-3-33:~$ free -h total used free shared buff/cache available Mem: 7.5G 7.2G 120M 79M 237M 66M Swap: 0B 0B 0B In this case, the machine only has 120M free which isn't great. Mitigation We probably need to restart the Salt master service. Use the systemctl command for that: root@ip-10-1-3-33:~# systemctl restart salt-master Now, wait a minute and then check free memory again. There should be significantly more available: root@ip-10-1-3-33:~# free -h total used free shared buff/cache available Mem: 7.5G 1.9G 5.3G 80M 280M 5.3G Swap: 0B 0B 0B If what you see is something like the above, you're good to go. Problem solved (for now!) XQueueWatcher DiskUsageWarning xqwatcher Diagnosis This happens every few months if the xqueue watcher nodes hang around for that long. Mitigation From salt-pr master: sudo ssh -i /etc/salt/keys/aws/salt-production.pem ubuntu@10.7.0.78 sudo su - root@ip-10-7-0-78:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/root 20G 16G 3.9G 81% / <<<<<<<<<<<<<<<<<<<<<<<<<< offending filesystem devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 1.9G 560K 1.9G 1% /dev/shm tmpfs 389M 836K 389M 1% /run tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/loop1 56M 56M 0 100% /snap/core18/2751 /dev/loop2 25M 25M 0 100% /snap/amazon-ssm-agent/6312 /dev/loop0 25M 25M 0 100% /snap/amazon-ssm-agent/6563 /dev/loop3 54M 54M 0 100% /snap/snapd/19361 /dev/loop4 64M 64M 0 100% /snap/core20/1950 /dev/loop6 56M 56M 0 100% /snap/core18/2785 /dev/loop5 54M 54M 0 100% /snap/snapd/19457 /dev/loop7 92M 92M 0 100% /snap/lxd/24061 /dev/loop8 92M 92M 0 100% /snap/lxd/23991 /dev/loop10 64M 64M 0 100% /snap/core20/1974 tmpfs 389M 0 389M 0% /run/user/1000 root@ip-10-7-0-78:~# cd /edx/var <<<<<<<<<<<<<<<<<<< intuition / memory root@ip-10-7-0-78:/edx/var# du -h | sort -hr | head 8.8G . 8.7G ./log 8.2G ./log/xqwatcher <<<<<<<<<<<< Offender 546M ./log/supervisor 8.0K ./supervisor 4.0K ./xqwatcher 4.0K ./log/aws 4.0K ./aws root@ip-10-7-0-78:/edx/var# cd log/xqwatcher/ root@ip-10-7-0-78:/edx/var/log/xqwatcher# ls -tlrha total 8.2G drwxr-xr-x 2 www-data xqwatcher 4.0K Mar 11 08:35 . drwxr-xr-x 5 syslog syslog 4.0K Jul 14 00:00 .. -rw-r--r-- 1 www-data www-data 8.2G Jul 14 14:12 xqwatcher.log <<<<<<<<< big file root@ip-10-7-0-78:/edx/var/log/xqwatcher# rm xqwatcher.log root@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl restart supervisor.service Job for supervisor.service failed because the control process exited with error code. See \"systemctl status supervisor.service\" and \"journalctl -xe\" for details. root@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl restart supervisor.service <<<<<<<<<<<< Restart it twice because ??? root@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl status supervisor.service \u25cf supervisor.service - supervisord - Supervisor process control system Loaded: loaded (/etc/systemd/system/supervisor.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2023-07-14 14:12:51 UTC; 4min 48s ago Docs: http://supervisord.org Process: 1114385 ExecStart=/edx/app/supervisor/venvs/supervisor/bin/supervisord --configuration /edx/app/supervisor/supervisord.conf (code=exited, status=0/SUCCESS) Main PID: 1114387 (supervisord) Tasks: 12 (limit: 4656) Memory: 485.8M CGroup: /system.slice/supervisor.service \u251c\u25001114387 /edx/app/supervisor/venvs/supervisor/bin/python /edx/app/supervisor/venvs/supervisor/bin/supervisord --configuration /edx/app/supervisor/supervisord.conf \u2514\u25001114388 /edx/app/xqwatcher/venvs/xqwatcher/bin/python -m xqueue_watcher -d /edx/app/xqwatcher root@ip-10-7-0-78:/edx/var/log/xqwatcher# ls -lthra total 644K drwxr-xr-x 5 syslog syslog 4.0K Jul 14 00:00 .. drwxr-xr-x 2 www-data xqwatcher 4.0K Jul 14 14:12 . -rw-r--r-- 1 www-data www-data 636K Jul 14 14:17 xqwatcher.log <<<<<<<< New file being written to root@ip-10-7-0-78:/edx/var/log/xqwatcher# df -h . Filesystem Size Used Avail Use% Mounted on /dev/root 20G 7.4G 12G 38% <<<<<<<<<<< acceptable utilization OVS [Prometheus]: [FIRING:1] InvalidAccessKeyProduction apps-production (odl-video-service critical) Diagnosis This happens sometimes when the applications's instance S3 credentials become out of date. Mitigation Use the AWS EC2 web console and navigate to the EC2 -> Auto Scaling Group pane. Search on: odl-video-service-production Once you have the right ASG, click on the \"Instance Refresh\" tab and then click the \"Start Instance Refresh\" button. Be sure to un-check the \"Enable Skip Matching\" box , or your instance refresh will most likely not do anything at all. Request by deeveloper to add videos Diagnosis N/A - developer request Mitigation Use the AWS EC2 web console and find instances of type odl-video-service-production - detailed instructions for accessing the instance can be found here . The only difference in this case is that the user is admin rather than ubuntu . Stop when you get a shell prompt and rejoin this document. First, run: sudo docker compose ps to see a list of running processes. In our case, we're looking for app . This isn't strictly necessary here as we know what we're looking for, but good to look before you leap anyway. You should see something like: admin@ip-10-13-3-50:/etc/docker/compose$ sudo docker compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS compose-app-1 mitodl/ovs-app:v0.69.0-5-gf76af37 \"/bin/bash -c ' slee\u2026\" app 3 weeks ago Up 3 weeks 0.0.0.0:8087->8087/tcp, :::8087->8087/tcp, 8089/tcp compose-celery-1 mitodl/ovs-app:v0.69.0-5-gf76af37 \"/bin/bash -c ' slee\u2026\" celery 3 weeks ago Up 3 weeks 8089/tcp compose-nginx-1 pennlabs/shibboleth-sp-nginx:latest \"/usr/bin/supervisor\u2026\" nginx 3 weeks ago Up 3 weeks 0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp Now run: sudo docker compose exec -it app /bin/bash which should get you a new, less colorful shell prompt. At this point you can run the manage.py command the developer gave you in slack. In my case, this is what I ran and the output I got: mitodl@486c7fbba98b:/src$ python ./manage.py add_hls_video_to_edx --edx-course-id course-v1:xPRO+DECA_Boeing+SPOC_R0 Attempting to post video(s) to edX... Video successfully added to edX \u2013 VideoFile: CCADE_V11JW_Hybrid_Data_Formats_v1.mp4 (105434), edX url: https://courses.xpro.mit.edu/api/val/v0/videos/ You're all set! Bootcamp Ecommerce [Prometheus]: [FIRING:1] AlternateInvalidAccessKeyProduction production (bootcamp-ecommerce critical) Diagnosis N/A Mitigation You need to refresh the credentials the salt-proxy is using for Heroku to manage this app. ssh to the salt production server: ssh salt-production.odl.mit.edu Run the salt proxy command to refresh creds: salt proxy-bootcamps-production state.sls heroku.update_heroku_config . You should see output similar to the following: cpatti@ip-10-0-2-195:~$ sudo salt proxy-bootcamps-production state.sls heroku.update_heroku_config proxy-bootcamps-production: ---------- ID: update_heroku_bootcamp-ecommerce_config Function: heroku.update_app_config_vars Name: bootcamp-ecommerce Result: True Comment: Started: 14:43:58.916128 Duration: 448.928 ms Changes: ---------- new: ---------- ** 8< snip 8< secret squirrel content elided ** Summary for proxy-bootcamps-production ------------ Succeeded: 1 (changed=1) Failed: 0 ------------ Total states run: 1 Total run time: 448.928 ms cpatti@ip-10-0-2-195:~$ OpenEdX Residential MITx Task handler raised error: \"OperationalError(1045, \"Access denied for user 'v-edxa-fmT0KbL5X'@'10.7.0.237' (using password: YES) Diagnosis If the oncall receives this page, instances credentials to access Vault and the secrets it contains have lapsed. Mitigation Fixing this issue currently requires an instance refresh, as the newly launched instances will have all the necessary credentials. From the EC2 console, on the left hand side, click \"Auto Scaling Groups\", then type 'edxapp-web-mitx- ' e.g. 'edxapp-web-mitx-production'. This should yield 1 result with something like 'edxapp-web-autoscaling-group-XXXX' in the 'Name' column. Click that. Now click the \"Instance Refresh\" tab. Click \"Start instance refresh\". Be sure to un-check the \"Enable Skip Matching\" box , or your instance refresh will most likely not do anything at all. Monitor the instance refresh to ensure it completes successfully. If you have been receiving multiple similar pages, they should stop coming in. If they continue, please escalate this incident as this problem is user visible and thus high impact to customers. XPro ApiException hubspot_xpro.tasks.sync_contact_with_hubspot Diagnosis This error is thrown when the Hubspot API key has expired. You'll see an error similar to this one in Sentry . Mitigation The fix for this is to generate a new API key in Hubspot and then get that key into Vault, triggering the appropriate pipeline deployment afterwards. First, generate a new API key in Hubspot. You can do this by logging into Hubspot, You can do this using the username/password and TOTP token found in [Vault](https://vault-production.odl.mit.edu/ui/vault/secrets/platform-secrets/kv/hubspot/details?version=1. Once you're logged in, click \"Open\" next to \"MIT XPro\" in the Accounts list. Then, click on the gear icon in the upper right corner of the page and select \"Integrations\" -> \"Private Apps\" in the sidebar on the left. You should then see the XPRo private app and beneath that a link for \"View Access Token\". Click that, then click on the \"Manage Token\" link. On this screen, you should see a \"Rotate\" button, click that to generate a new API key. Now that you've generated your new API token, you'll need to get that token into Vault using SOPS. You can find the right secrets file for this in Github here . The process for deploying secrets deserves its own document, so after adding the new API token to the SOPS decrypted secrets file you just generated, commit it to Github, ensure it runs through the appropriate pipelines and ends up in Vault. You can find the ultimate home of the XPro Hubspot API key in Vault here . Once the new API token is in the correct spot, you'll need to ensure that new token gets deployed to production in Heroku by tracking its progress in this pipeline. You will likely need to close Concourse Github workflow issues to make this happen. See its users guide for details. Once that's complete, you should have mitigated this issue. Keep checking that Sentry page to ensure that the Last Seen value reflects something appropriately long ago and you can resolve this ticket. If you are asked to run a sync to Hubspot: - Inform the requester, preferably on the #product-xpro Slack that the process will take quite a long time. If this is time critical they may ask you to run only parts of the sync. You can find documentation on the command you'll run here . Since XPro runs on Heroku, you'll need to get a Heroku console shell to run the management command. You can get to that shell by logging into heroku with the Heroku CLI and running: heroku run /bin/bash -a xpro-production It takes a while but you will eventually get your shell prompt. From there, run the following commands. To sync all variants: ./manage.py sync_db_to_hubspot create If you're asked to run only one, for example deals, you can consult the documentation linked above and see that you should add the --deals flag to the invocation. Be sure to inform the requester of what you see for output and add it to the ticket for this issue if there is one. If you see the command fail with an exception, note the HTTP response code. In particular a 401 means that the API key is likely out of date. A 409 signals a conflict (e.g. dupe email) that will likely be handled by conflict resolution code and thus can probably be ignored. MITXOnline Cybersource credentials potentially out of date Diagnosis Often we will get a report like this indicating that one of our Cybersource credentials is out of date. Mitigation Since we have no access to the Cybersource web UI, we must send E-mail to: sbmit@mit.edu to validate the status of the current credential or request a new one. Grading Celery Task Failed (STUB entry. Needs love) Diagnosis Usually we'll get reports from our users telling us that grading tasks have failed. At that point we should surf to celery monitoring and login with your Keycloak Platform Engineering realm credentials. Then, get the course ID for the failed grading tasks and search for it in Celery Monitoring by entering the course key in the kwargs input, surrounded by { ' and ' }, for example { 'course-v1:MITxT+14.310x+1T2024' }. Mitigation You may well be asked to run the compute_graded management command on the LMS for mitxonline. (TODO: Needs details. How do we get there? etc.) Reddit [Prometheus]: [FIRING:1] DiskUsageWarning production-apps (reddit filesystem /dev/nvme0n1p1 ext4 ip-10-13-1-59 integrations/linux_ [Pingdom] Open Discussions production home page has an alert Diagnosis We often get low disk errors on our reddit nodes, but in this case the low disk alert was paired with a pingdom alert on open-discussions. This may mean that pgbouncer is in trouble on reddit, likely because its credentials are out of date. You can get a view into what's happening by logging into the node cited in the disk usage ticket and typing: salt reddit-production* state.sls reddit.config,pgbouncer Mitigation Once you've determined that pgbouncer is indeed sad, you can try a restart / credential refresh with the following command: salt reddit-production* state.sls reddit.config","title":"Oncall"},{"location":"runbooks/oncall/#introduction","text":"This document is meant to be one stop shopping for your MIT OL Devops oncall needs. Please update this doc as you handle incidents whenever you're oncall.","title":"Introduction"},{"location":"runbooks/oncall/#style-guide","text":"There should be a table of contents at the top of the document with links to each product heading. Your editor likely has a plugin to make this automatic. Each product gets its own top level heading. Entries that are keyed to a specific alert should have the relevant text in a second level heading under the product. Boil the alert down to the most relevant searchable text and omit specifics that will vary. For instance: \"[Prometheus]: [FIRING:1] DiskUsageWarning mitx-production (xqwatcher filesystem /dev/root ext4 ip-10-7-0-78 integrations/linux_hos\" would boil down to DiskUsageWarning xqwatcher because the rest will change and make finding the right entry more difficult. Each entry should have at least two sections, Diagnosis and Mitigation. Use bold face for the section title. This will allow the oncall to get only as much Diagnosis in as required to identify the issue and focus on putting out the fire.","title":"Style Guide"},{"location":"runbooks/oncall/#products","text":"","title":"Products"},{"location":"runbooks/oncall/#odl-open-discussions","text":"","title":"ODL Open Discussions"},{"location":"runbooks/oncall/#invalidaccesskeynonprod-qa-odl-open-discussions-warning","text":"Diagnosis You get an alert like \"[Prometheus]: [FIRING:1] InvalidAccessKeyNonProd qa (odl-open-discussions warning)\". Mitigation In the mitodl/ol-infrastructure Github repository, change directory to src/mit/ol-infrastructure/src/ol_infrastructure/applications/open_discussions and run `pulumi up'.","title":"InvalidAccessKeyNonProd qa (odl-open-discussions warning)"},{"location":"runbooks/oncall/#saltstack","text":"","title":"SaltStack"},{"location":"runbooks/oncall/#memoryusagewarning-operations-","text":"Diagnosis You get an alert like: [Prometheus]: [FIRING:1] MemoryUsageWarning operations-qa (memory ip-10-1-3-33 integrations/linux_host warning) . You'll need an account and ssh key set up on the saltstack master hosts. This should happen when you join the team. Now, ssh into the salt master appropriate to the environment you received the alert for. The IP address is cited in the alert. So, for the above: (Substitute your username and the appropriate environment if not qa, e.g. production) ssh -l cpatti salt-qa.odl.mit.edu Next, check free memory: mdavidson@ip-10-1-3-33:~$ free -h total used free shared buff/cache available Mem: 7.5G 7.2G 120M 79M 237M 66M Swap: 0B 0B 0B In this case, the machine only has 120M free which isn't great. Mitigation We probably need to restart the Salt master service. Use the systemctl command for that: root@ip-10-1-3-33:~# systemctl restart salt-master Now, wait a minute and then check free memory again. There should be significantly more available: root@ip-10-1-3-33:~# free -h total used free shared buff/cache available Mem: 7.5G 1.9G 5.3G 80M 280M 5.3G Swap: 0B 0B 0B If what you see is something like the above, you're good to go. Problem solved (for now!)","title":"MemoryUsageWarning operations-"},{"location":"runbooks/oncall/#xqueuewatcher","text":"","title":"XQueueWatcher"},{"location":"runbooks/oncall/#diskusagewarning-xqwatcher","text":"Diagnosis This happens every few months if the xqueue watcher nodes hang around for that long. Mitigation From salt-pr master: sudo ssh -i /etc/salt/keys/aws/salt-production.pem ubuntu@10.7.0.78 sudo su - root@ip-10-7-0-78:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/root 20G 16G 3.9G 81% / <<<<<<<<<<<<<<<<<<<<<<<<<< offending filesystem devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 1.9G 560K 1.9G 1% /dev/shm tmpfs 389M 836K 389M 1% /run tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/loop1 56M 56M 0 100% /snap/core18/2751 /dev/loop2 25M 25M 0 100% /snap/amazon-ssm-agent/6312 /dev/loop0 25M 25M 0 100% /snap/amazon-ssm-agent/6563 /dev/loop3 54M 54M 0 100% /snap/snapd/19361 /dev/loop4 64M 64M 0 100% /snap/core20/1950 /dev/loop6 56M 56M 0 100% /snap/core18/2785 /dev/loop5 54M 54M 0 100% /snap/snapd/19457 /dev/loop7 92M 92M 0 100% /snap/lxd/24061 /dev/loop8 92M 92M 0 100% /snap/lxd/23991 /dev/loop10 64M 64M 0 100% /snap/core20/1974 tmpfs 389M 0 389M 0% /run/user/1000 root@ip-10-7-0-78:~# cd /edx/var <<<<<<<<<<<<<<<<<<< intuition / memory root@ip-10-7-0-78:/edx/var# du -h | sort -hr | head 8.8G . 8.7G ./log 8.2G ./log/xqwatcher <<<<<<<<<<<< Offender 546M ./log/supervisor 8.0K ./supervisor 4.0K ./xqwatcher 4.0K ./log/aws 4.0K ./aws root@ip-10-7-0-78:/edx/var# cd log/xqwatcher/ root@ip-10-7-0-78:/edx/var/log/xqwatcher# ls -tlrha total 8.2G drwxr-xr-x 2 www-data xqwatcher 4.0K Mar 11 08:35 . drwxr-xr-x 5 syslog syslog 4.0K Jul 14 00:00 .. -rw-r--r-- 1 www-data www-data 8.2G Jul 14 14:12 xqwatcher.log <<<<<<<<< big file root@ip-10-7-0-78:/edx/var/log/xqwatcher# rm xqwatcher.log root@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl restart supervisor.service Job for supervisor.service failed because the control process exited with error code. See \"systemctl status supervisor.service\" and \"journalctl -xe\" for details. root@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl restart supervisor.service <<<<<<<<<<<< Restart it twice because ??? root@ip-10-7-0-78:/edx/var/log/xqwatcher# systemctl status supervisor.service \u25cf supervisor.service - supervisord - Supervisor process control system Loaded: loaded (/etc/systemd/system/supervisor.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2023-07-14 14:12:51 UTC; 4min 48s ago Docs: http://supervisord.org Process: 1114385 ExecStart=/edx/app/supervisor/venvs/supervisor/bin/supervisord --configuration /edx/app/supervisor/supervisord.conf (code=exited, status=0/SUCCESS) Main PID: 1114387 (supervisord) Tasks: 12 (limit: 4656) Memory: 485.8M CGroup: /system.slice/supervisor.service \u251c\u25001114387 /edx/app/supervisor/venvs/supervisor/bin/python /edx/app/supervisor/venvs/supervisor/bin/supervisord --configuration /edx/app/supervisor/supervisord.conf \u2514\u25001114388 /edx/app/xqwatcher/venvs/xqwatcher/bin/python -m xqueue_watcher -d /edx/app/xqwatcher root@ip-10-7-0-78:/edx/var/log/xqwatcher# ls -lthra total 644K drwxr-xr-x 5 syslog syslog 4.0K Jul 14 00:00 .. drwxr-xr-x 2 www-data xqwatcher 4.0K Jul 14 14:12 . -rw-r--r-- 1 www-data www-data 636K Jul 14 14:17 xqwatcher.log <<<<<<<< New file being written to root@ip-10-7-0-78:/edx/var/log/xqwatcher# df -h . Filesystem Size Used Avail Use% Mounted on /dev/root 20G 7.4G 12G 38% <<<<<<<<<<< acceptable utilization","title":"DiskUsageWarning xqwatcher"},{"location":"runbooks/oncall/#ovs","text":"","title":"OVS"},{"location":"runbooks/oncall/#prometheus-firing1-invalidaccesskeyproduction-apps-production-odl-video-service-critical","text":"Diagnosis This happens sometimes when the applications's instance S3 credentials become out of date. Mitigation Use the AWS EC2 web console and navigate to the EC2 -> Auto Scaling Group pane. Search on: odl-video-service-production Once you have the right ASG, click on the \"Instance Refresh\" tab and then click the \"Start Instance Refresh\" button. Be sure to un-check the \"Enable Skip Matching\" box , or your instance refresh will most likely not do anything at all.","title":"[Prometheus]: [FIRING:1] InvalidAccessKeyProduction apps-production (odl-video-service critical)"},{"location":"runbooks/oncall/#request-by-deeveloper-to-add-videos","text":"Diagnosis N/A - developer request Mitigation Use the AWS EC2 web console and find instances of type odl-video-service-production - detailed instructions for accessing the instance can be found here . The only difference in this case is that the user is admin rather than ubuntu . Stop when you get a shell prompt and rejoin this document. First, run: sudo docker compose ps to see a list of running processes. In our case, we're looking for app . This isn't strictly necessary here as we know what we're looking for, but good to look before you leap anyway. You should see something like: admin@ip-10-13-3-50:/etc/docker/compose$ sudo docker compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS compose-app-1 mitodl/ovs-app:v0.69.0-5-gf76af37 \"/bin/bash -c ' slee\u2026\" app 3 weeks ago Up 3 weeks 0.0.0.0:8087->8087/tcp, :::8087->8087/tcp, 8089/tcp compose-celery-1 mitodl/ovs-app:v0.69.0-5-gf76af37 \"/bin/bash -c ' slee\u2026\" celery 3 weeks ago Up 3 weeks 8089/tcp compose-nginx-1 pennlabs/shibboleth-sp-nginx:latest \"/usr/bin/supervisor\u2026\" nginx 3 weeks ago Up 3 weeks 0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp Now run: sudo docker compose exec -it app /bin/bash which should get you a new, less colorful shell prompt. At this point you can run the manage.py command the developer gave you in slack. In my case, this is what I ran and the output I got: mitodl@486c7fbba98b:/src$ python ./manage.py add_hls_video_to_edx --edx-course-id course-v1:xPRO+DECA_Boeing+SPOC_R0 Attempting to post video(s) to edX... Video successfully added to edX \u2013 VideoFile: CCADE_V11JW_Hybrid_Data_Formats_v1.mp4 (105434), edX url: https://courses.xpro.mit.edu/api/val/v0/videos/ You're all set!","title":"Request by deeveloper to add videos"},{"location":"runbooks/oncall/#bootcamp-ecommerce","text":"","title":"Bootcamp Ecommerce"},{"location":"runbooks/oncall/#prometheus-firing1-alternateinvalidaccesskeyproduction-production-bootcamp-ecommerce-critical","text":"Diagnosis N/A Mitigation You need to refresh the credentials the salt-proxy is using for Heroku to manage this app. ssh to the salt production server: ssh salt-production.odl.mit.edu Run the salt proxy command to refresh creds: salt proxy-bootcamps-production state.sls heroku.update_heroku_config . You should see output similar to the following: cpatti@ip-10-0-2-195:~$ sudo salt proxy-bootcamps-production state.sls heroku.update_heroku_config proxy-bootcamps-production: ---------- ID: update_heroku_bootcamp-ecommerce_config Function: heroku.update_app_config_vars Name: bootcamp-ecommerce Result: True Comment: Started: 14:43:58.916128 Duration: 448.928 ms Changes: ---------- new: ---------- ** 8< snip 8< secret squirrel content elided ** Summary for proxy-bootcamps-production ------------ Succeeded: 1 (changed=1) Failed: 0 ------------ Total states run: 1 Total run time: 448.928 ms cpatti@ip-10-0-2-195:~$","title":"[Prometheus]: [FIRING:1] AlternateInvalidAccessKeyProduction production (bootcamp-ecommerce critical)"},{"location":"runbooks/oncall/#openedx-residential-mitx","text":"","title":"OpenEdX Residential MITx"},{"location":"runbooks/oncall/#task-handler-raised-error-operationalerror1045-access-denied-for-user-v-edxa-fmt0kbl5x1070237-using-password-yes","text":"Diagnosis If the oncall receives this page, instances credentials to access Vault and the secrets it contains have lapsed. Mitigation Fixing this issue currently requires an instance refresh, as the newly launched instances will have all the necessary credentials. From the EC2 console, on the left hand side, click \"Auto Scaling Groups\", then type 'edxapp-web-mitx- ' e.g. 'edxapp-web-mitx-production'. This should yield 1 result with something like 'edxapp-web-autoscaling-group-XXXX' in the 'Name' column. Click that. Now click the \"Instance Refresh\" tab. Click \"Start instance refresh\". Be sure to un-check the \"Enable Skip Matching\" box , or your instance refresh will most likely not do anything at all. Monitor the instance refresh to ensure it completes successfully. If you have been receiving multiple similar pages, they should stop coming in. If they continue, please escalate this incident as this problem is user visible and thus high impact to customers.","title":"Task handler raised error: \"OperationalError(1045, \"Access denied for user 'v-edxa-fmT0KbL5X'@'10.7.0.237' (using password: YES)"},{"location":"runbooks/oncall/#xpro","text":"","title":"XPro"},{"location":"runbooks/oncall/#apiexception-hubspot_xprotaskssync_contact_with_hubspot","text":"Diagnosis This error is thrown when the Hubspot API key has expired. You'll see an error similar to this one in Sentry . Mitigation The fix for this is to generate a new API key in Hubspot and then get that key into Vault, triggering the appropriate pipeline deployment afterwards. First, generate a new API key in Hubspot. You can do this by logging into Hubspot, You can do this using the username/password and TOTP token found in [Vault](https://vault-production.odl.mit.edu/ui/vault/secrets/platform-secrets/kv/hubspot/details?version=1. Once you're logged in, click \"Open\" next to \"MIT XPro\" in the Accounts list. Then, click on the gear icon in the upper right corner of the page and select \"Integrations\" -> \"Private Apps\" in the sidebar on the left. You should then see the XPRo private app and beneath that a link for \"View Access Token\". Click that, then click on the \"Manage Token\" link. On this screen, you should see a \"Rotate\" button, click that to generate a new API key. Now that you've generated your new API token, you'll need to get that token into Vault using SOPS. You can find the right secrets file for this in Github here . The process for deploying secrets deserves its own document, so after adding the new API token to the SOPS decrypted secrets file you just generated, commit it to Github, ensure it runs through the appropriate pipelines and ends up in Vault. You can find the ultimate home of the XPro Hubspot API key in Vault here . Once the new API token is in the correct spot, you'll need to ensure that new token gets deployed to production in Heroku by tracking its progress in this pipeline. You will likely need to close Concourse Github workflow issues to make this happen. See its users guide for details. Once that's complete, you should have mitigated this issue. Keep checking that Sentry page to ensure that the Last Seen value reflects something appropriately long ago and you can resolve this ticket. If you are asked to run a sync to Hubspot: - Inform the requester, preferably on the #product-xpro Slack that the process will take quite a long time. If this is time critical they may ask you to run only parts of the sync. You can find documentation on the command you'll run here . Since XPro runs on Heroku, you'll need to get a Heroku console shell to run the management command. You can get to that shell by logging into heroku with the Heroku CLI and running: heroku run /bin/bash -a xpro-production It takes a while but you will eventually get your shell prompt. From there, run the following commands. To sync all variants: ./manage.py sync_db_to_hubspot create If you're asked to run only one, for example deals, you can consult the documentation linked above and see that you should add the --deals flag to the invocation. Be sure to inform the requester of what you see for output and add it to the ticket for this issue if there is one. If you see the command fail with an exception, note the HTTP response code. In particular a 401 means that the API key is likely out of date. A 409 signals a conflict (e.g. dupe email) that will likely be handled by conflict resolution code and thus can probably be ignored.","title":"ApiException hubspot_xpro.tasks.sync_contact_with_hubspot"},{"location":"runbooks/oncall/#mitxonline","text":"","title":"MITXOnline"},{"location":"runbooks/oncall/#cybersource-credentials-potentially-out-of-date","text":"Diagnosis Often we will get a report like this indicating that one of our Cybersource credentials is out of date. Mitigation Since we have no access to the Cybersource web UI, we must send E-mail to: sbmit@mit.edu to validate the status of the current credential or request a new one.","title":"Cybersource credentials potentially out of date"},{"location":"runbooks/oncall/#grading-celery-task-failed-stub-entry-needs-love","text":"Diagnosis Usually we'll get reports from our users telling us that grading tasks have failed. At that point we should surf to celery monitoring and login with your Keycloak Platform Engineering realm credentials. Then, get the course ID for the failed grading tasks and search for it in Celery Monitoring by entering the course key in the kwargs input, surrounded by { ' and ' }, for example { 'course-v1:MITxT+14.310x+1T2024' }. Mitigation You may well be asked to run the compute_graded management command on the LMS for mitxonline. (TODO: Needs details. How do we get there? etc.)","title":"Grading Celery Task Failed (STUB entry. Needs love)"},{"location":"runbooks/oncall/#reddit","text":"","title":"Reddit"},{"location":"runbooks/oncall/#prometheus-firing1-diskusagewarning-production-apps-reddit-filesystem-devnvme0n1p1-ext4-ip-10-13-1-59-integrationslinux_","text":"","title":"[Prometheus]: [FIRING:1] DiskUsageWarning production-apps (reddit filesystem /dev/nvme0n1p1 ext4 ip-10-13-1-59 integrations/linux_"},{"location":"runbooks/oncall/#pingdom-open-discussions-production-home-page-has-an-alert","text":"Diagnosis We often get low disk errors on our reddit nodes, but in this case the low disk alert was paired with a pingdom alert on open-discussions. This may mean that pgbouncer is in trouble on reddit, likely because its credentials are out of date. You can get a view into what's happening by logging into the node cited in the disk usage ticket and typing: salt reddit-production* state.sls reddit.config,pgbouncer Mitigation Once you've determined that pgbouncer is indeed sad, you can try a restart / credential refresh with the following command: salt reddit-production* state.sls reddit.config","title":"[Pingdom] Open Discussions production home page has an alert"}]}